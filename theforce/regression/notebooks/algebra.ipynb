{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "import torch\n",
    "_2pi = torch.tensor(2*pi)\n",
    "\n",
    "\n",
    "# general ---------------------------------------\n",
    "def positive(x):\n",
    "    return torch.log(1. + torch.exp(x))\n",
    "\n",
    "\n",
    "def free_form(x):\n",
    "    return torch.log(torch.exp(x) - 1.)\n",
    "\n",
    "\n",
    "def sum_packed_dim(packed, sizes, dim=-1):\n",
    "    result = torch.stack([piece.sum(dim=dim)\n",
    "                          for piece in torch.split(packed, sizes, dim=dim)], dim=dim)\n",
    "    return result\n",
    "\n",
    "\n",
    "# decompositions ---------------------------------------------\n",
    "def jitcholesky(A, jit=1e-6, jitbase=2):\n",
    "    ridge = 0\n",
    "    try:\n",
    "        L = torch.cholesky(A)\n",
    "    except RuntimeError:\n",
    "        scale = A.diag().mean()\n",
    "        if scale == torch.zeros(1):\n",
    "            scale = torch.finfo().eps\n",
    "        ridge = jit*scale\n",
    "        done = False\n",
    "        while not done:\n",
    "            try:\n",
    "                L = torch.cholesky(A + ridge*torch.eye(\n",
    "                    *A.size(), dtype=A.dtype))\n",
    "                done = True\n",
    "            except RuntimeError:\n",
    "                ridge *= jitbase\n",
    "            if ridge > scale:\n",
    "                raise RuntimeError('cholesky was not successful!')\n",
    "    return L, ridge\n",
    "\n",
    "\n",
    "def low_rank_factor(K, Y, logdet=False, solve=False, jit=1e-6, jitbase=2):\n",
    "    \"\"\"\n",
    "    Inputs: Y, K\n",
    "    K: a symmetric positive definite (covariance) matrix,\n",
    "       this will be factored  as K = L @ L.t()\n",
    "    Y: 1D or 2D\n",
    "    Returns: Q, logdet, ridge\n",
    "    ------------------------------------------------------\n",
    "    The following equality holds:\n",
    "    Q.t() @ Q = Y.t() @ K.inverse() @ Y\n",
    "    \"\"\"\n",
    "    L, ridge = jitcholesky(K, jit=jit, jitbase=jitbase)\n",
    "    if len(Y.size()) == 1:\n",
    "        _1d, _Y = True, Y.view(-1, 1)\n",
    "    else:\n",
    "        _1d, _Y = False, Y\n",
    "    if solve:\n",
    "        Q, _ = torch.triangular_solve(_Y, L, upper=False)\n",
    "    else:\n",
    "        Q = torch.mm(L.inverse(), _Y)\n",
    "    if logdet:\n",
    "        ld = 2*L.diag().log().sum()\n",
    "    else:\n",
    "        ld = None\n",
    "    return Q, ld, ridge\n",
    "\n",
    "\n",
    "def log_normal(Y, K, solve=True):\n",
    "    Q, logdet, ridge = low_rank_factor(K, Y, logdet=True, solve=solve)\n",
    "    return -(torch.mm(Q.t(), Q) + logdet + torch.log(_2pi)*Y.size(0))/2\n",
    "\n",
    "\n",
    "def solve_svd(A, Y):\n",
    "    U, S, V = torch.svd(A)\n",
    "    return V @ ((U.t() @ Y)/S)\n",
    "\n",
    "\n",
    "def projected_process_auxiliary_matrices_I(K, M, Y, sigma):\n",
    "    \"\"\"\n",
    "    If \"x\", \"m\" indicate the data and inducing points, and \"k(.,.)\" is\n",
    "    the kernel function, then\n",
    "    K = k(x, m)\n",
    "    M = k(m, m)\n",
    "    Y are the data values and sigma^2 is the ridge factor that will be \n",
    "    added to the diagonal of the approximated covariance matrix.\n",
    "    In projected process (PP) approximation the predictive distribution \n",
    "    for a given test point \"t\" is:\n",
    "    p(y|t,x,Y,m) = Normal(A @ mu, B - A @ nu @ A.t())\n",
    "    where mu and nu which are independent from the test inputs are \n",
    "    calculated here but A = k(t, m) and B = k(t, t) should be \n",
    "    calculated outside this routine.\n",
    "    \"\"\"\n",
    "    assert type(sigma) == float or sigma.numel() == 1\n",
    "    # mu\n",
    "    L, _ = jitcholesky(M)\n",
    "    A = torch.cat([K, sigma*L.t()], dim=0)\n",
    "    _Y = torch.cat([Y, torch.zeros(L.size(0))], dim=0)\n",
    "    Q, R = torch.qr(A)\n",
    "    mu = R.inverse() @ Q.t() @ _Y\n",
    "    # nu\n",
    "    i = L.inverse()\n",
    "    B = K @ i.t()\n",
    "    I = torch.eye(i.size(0))\n",
    "    T = torch.cholesky(B.t() @ B / sigma**2 + I)\n",
    "    nu = i.t() @ (I - torch.cholesky_inverse(T)) @ i\n",
    "    return mu, nu\n",
    "\n",
    "\n",
    "def inverse_using_low_rank_factor(Q, D):\n",
    "    \"\"\"\n",
    "    returns inverse of Q @ Q.T + D.diag()\n",
    "    algorithm inspired by torch.distributions.LowRankMultivariateNormal\n",
    "    \"\"\"\n",
    "    m = Q.size(-1)\n",
    "    W = Q.t() / D[None]\n",
    "    K = torch.matmul(W, Q).contiguous()\n",
    "    K.view(-1, m * m)[:, ::m + 1] += 1  # add identity matrix to K\n",
    "    C, _ = jitcholesky(K)  # robust\n",
    "    A = torch.triangular_solve(W, C, upper=False)[0]\n",
    "    inv = (torch.diag_embed(D.reciprocal())\n",
    "           - torch.matmul(A.transpose(-1, -2), A))\n",
    "    return inv\n",
    "\n",
    "\n",
    "def projected_process_auxiliary_matrices_D(K, M, Y, D, chol_inverse=False):\n",
    "    \"\"\"\n",
    "    same as projected_process_auxiliary_matrices_I, with a difference\n",
    "    that the scalar input \"sigma\" is replaced by a vector \"D\"\n",
    "    \"\"\"\n",
    "    assert D.numel() == Y.numel()\n",
    "    L, ridge = jitcholesky(M)\n",
    "    i = L.inverse()\n",
    "    B = K@i.t()\n",
    "    J = inverse_using_low_rank_factor(B, D)\n",
    "    mu = i.t()@B.t()@J@Y\n",
    "    nu = i.t()@B.t()@J@B@i\n",
    "    if chol_inverse:\n",
    "        return mu, nu, ridge, i\n",
    "    else:\n",
    "        return mu, nu, ridge\n",
    "\n",
    "\n",
    "# greedy algorithms ------------------------------------------------------------\n",
    "def sparser_projection(K, M, Y, D, alpha=1., indices=None):\n",
    "    mu, _, _ = projected_process_auxiliary_matrices_D(K, M, Y, D)\n",
    "    delta = K@mu-Y\n",
    "    delta_max = delta.abs().max()\n",
    "    var = delta.var()\n",
    "    indices = indices if indices else torch.arange(\n",
    "        M.size(0), dtype=int).tolist()\n",
    "    deleted = []\n",
    "    for _ in range(len(indices)):\n",
    "        i = torch.randint(M.size(0), (1,))\n",
    "        m = torch.ones(M.size(0)).bool()\n",
    "        m[i] = False\n",
    "        _K = K\n",
    "        _M = M\n",
    "        M = M[m][:, m]\n",
    "        K = K[:, m]\n",
    "        mu, _, _ = projected_process_auxiliary_matrices_D(K, M, Y, D)\n",
    "        delta2 = K@mu-Y\n",
    "        delta2_max = delta2.abs().max()\n",
    "        var2 = delta2.var()\n",
    "        if delta2_max <= delta_max and var2 <= alpha*var:\n",
    "            deleted += [indices[i]]\n",
    "            del indices[i]\n",
    "        else:\n",
    "            M = _M\n",
    "            K = _K\n",
    "    return K, M, indices, deleted\n",
    "\n",
    "\n",
    "def select_greedy_simple(T, num, Z=None):\n",
    "    assert T.dim() == 2\n",
    "    X = T\n",
    "    if Z is None:\n",
    "        arg = torch.randint(X.shape[0], (1,))\n",
    "        Z = X[arg]\n",
    "        X = torch.cat([X[:arg], X[arg+1:]])\n",
    "        #selected = [arg]\n",
    "        n = num-1\n",
    "    else:\n",
    "        assert Z.dim() == 2\n",
    "        #selected = []\n",
    "        n = num\n",
    "    for _ in range(n):\n",
    "        val, arg = torch.max(((X[:, None]-Z[None])**2).sum(dim=(1, 2)), 0)\n",
    "        #selected += [arg]\n",
    "        Z = torch.cat([Z, X[arg][None]])\n",
    "        X = torch.cat([X[:arg], X[arg+1:]])\n",
    "    return Z\n",
    "\n",
    "\n",
    "# examples ---------------------------------------------------------------------\n",
    "def example_sum_packed_dim():\n",
    "    sizes = torch.randint(1, 10, (100,))\n",
    "    Y = [torch.ones(7, size) for size in sizes]\n",
    "    Y = torch.cat(Y, dim=1)\n",
    "    P = sum_packed_dim(Y, sizes.tolist())\n",
    "    #print(Y.size(), P.size())\n",
    "    print('sum_packed_dim works: {}'.format(P.size() == torch.Size([7, 100])))\n",
    "\n",
    "\n",
    "# tests ------------------------------------------------------------------------\n",
    "def test(n=1000):\n",
    "\n",
    "    # test cholesky\n",
    "    K = torch.ones(n, n)\n",
    "    L, ridge = jitcholesky(K)\n",
    "    test_cholesky = torch.allclose(torch.mm(L, L.t()), K)\n",
    "    print('Cholesky: {}'.format(test_cholesky))\n",
    "\n",
    "    # test log_normal\n",
    "    Y = torch.rand(n)\n",
    "    dist = torch.distributions.MultivariateNormal(torch.zeros(n), scale_tril=L)\n",
    "    test_log_normal = torch.allclose(dist.log_prob(Y), log_normal(Y, K))\n",
    "    print('log_normal: {}'.format(test_log_normal))\n",
    "\n",
    "    # test select_greedy\n",
    "    X = torch.rand(100, 7)\n",
    "    Z = select_greedy_simple(X, 17)\n",
    "    Z = select_greedy_simple(X, 17, Z=Z)\n",
    "\n",
    "    # test solve SVD\n",
    "    A = torch.diag(torch.ones(10))\n",
    "    Y = torch.linspace(0, 100, 10)\n",
    "    X = solve_svd(A, Y)\n",
    "    test_solve_svd = torch.allclose(X, Y)\n",
    "    print('solve_svd: {}'.format(test_solve_svd))\n",
    "\n",
    "\n",
    "def test_iulrf(n=100, d=7, sigma=1e-4):\n",
    "    torch.set_default_tensor_type(torch.DoubleTensor)  # this is necessary!\n",
    "    \"\"\"keep sigma higher than 1e-4\"\"\"\n",
    "    Q = torch.rand(n, 7)\n",
    "    D = torch.rand(n)*sigma**2\n",
    "    res = inverse_using_low_rank_factor(Q, D) @ (\n",
    "        Q @ Q.t() + D.diag()) - torch.eye(n)\n",
    "    print(\"testing inverse_using_low_rank_factor:\", res.abs().max().allclose(torch.zeros(1)),\n",
    "          res.abs().max())\n",
    "\n",
    "\n",
    "def test_PP(n=100, d=7, sigma=1e-2):\n",
    "    # full Gram matrix is X@X.t()+D.diag(), where:\n",
    "    X = torch.rand(n, d)\n",
    "    D = (torch.ones(n)*sigma**2)\n",
    "    # sparsification:\n",
    "    W = X[::10]\n",
    "    M = (W@W.t())\n",
    "    K = (X@W.t())\n",
    "    # since D = sigma^2*I, the two methods are equivalent\n",
    "    Y = torch.rand(n)\n",
    "    mu1, nu1 = projected_process_auxiliary_matrices_I(K, M, Y, sigma)\n",
    "    mu2, nu2 = projected_process_auxiliary_matrices_D(K, M, Y, D)\n",
    "    a = (mu1-mu2).abs().max()\n",
    "    b = (nu1-nu2).abs().max()\n",
    "    print('test project process (PP): {}, {}'.format(a, b))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    example_sum_packed_dim()\n",
    "    test()\n",
    "    test_iulrf()\n",
    "    test_PP()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
