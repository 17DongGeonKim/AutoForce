{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module, Parameter\n",
    "from theforce.regression.algebra import positive, free_form\n",
    "\n",
    "\n",
    "def atleast2d(t, force2d=False):\n",
    "    _t = torch.as_tensor(t)\n",
    "    if _t.dim() < 2:\n",
    "        return _t.view(-1, 1)\n",
    "    elif _t.dim() >= 2:\n",
    "        if force2d:\n",
    "            return _t.view(_t.size(0), torch.tensor(_t.size()[1:]).prod())\n",
    "        else:\n",
    "            return _t\n",
    "\n",
    "\n",
    "class Kernel(Module):\n",
    "    \"\"\"Kernel is a function that accepts two arguments.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.params = []\n",
    "\n",
    "    def checkout_inputs(self, x, xx=None, diag=False):\n",
    "        t = atleast2d(x, force2d=False)\n",
    "        s = t.size()[1:]  # features original shape\n",
    "        t = atleast2d(t, force2d=True).t().contiguous()\n",
    "        if diag:\n",
    "            assert xx is None\n",
    "            return t, t, s\n",
    "        else:\n",
    "            if xx is None:\n",
    "                tt = t\n",
    "            else:\n",
    "                tt = atleast2d(xx, force2d=True).t().contiguous()\n",
    "            t, tt = torch.broadcast_tensors(t[..., None], tt[:, None])\n",
    "            return t, tt, s\n",
    "\n",
    "    # main mathods\n",
    "    def forward(self, x, xx=None, diag=False, method='func'):\n",
    "        t, tt, s = self.checkout_inputs(x, xx, diag)\n",
    "        k = getattr(self, 'get_'+method)(t, tt)\n",
    "        if k.size(-1) == 1 or (not diag and k.size(-2) == 1):\n",
    "            k = torch.ones_like(t[0])*k  # TODO: use .expand instead\n",
    "        return k\n",
    "\n",
    "    def func(self, x, xx=None, diag=False):\n",
    "        return self.forward(x, xx, diag, method='func')\n",
    "\n",
    "    def leftgrad(self, x, xx=None, diag=False):\n",
    "        return self.forward(x, xx, diag, method='leftgrad')\n",
    "\n",
    "    def rightgrad(self, x, xx=None, diag=False):\n",
    "        return self.forward(x, xx, diag, method='rightgrad')\n",
    "\n",
    "    def gradgrad(self, x, xx=None, diag=False):\n",
    "        return self.forward(x, xx, diag, method='gradgrad')\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.__class__.__name__+'({})'.format(self.state_args)\n",
    "\n",
    "    # Operators\n",
    "    def __add__(self, other):\n",
    "        return Add(self, other)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return Sub(self, other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Mul(self, other)\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        return Pow(self, other)\n",
    "\n",
    "    def pow(self, eta):\n",
    "        return Pow(self, eta)\n",
    "\n",
    "    def exp(self):\n",
    "        return Exp(self)\n",
    "\n",
    "    # overload the following methods\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return ''\n",
    "\n",
    "    def get_func(self, x, xx):\n",
    "        \"\"\"output shape: (m, n)\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'get_func in {}'.format(self.__class__.__name__))\n",
    "\n",
    "    def get_leftgrad(self, x, xx):\n",
    "        \"\"\"output shape: (d, m, n)\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'get_leftgrad in {}'.format(self.__class__.__name__))\n",
    "\n",
    "    def get_rightgrad(self, x, xx):\n",
    "        \"\"\"output shape: (d, m, n)\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'get_rightgrad in {}'.format(self.__class__.__name__))\n",
    "\n",
    "    def get_gradgrad(self, x, xx):\n",
    "        \"\"\"output shape: (d, d, m, n)\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'get_gradgrad in {}'.format(self.__class__.__name__))\n",
    "\n",
    "\n",
    "class BinaryOperator(Kernel):\n",
    "\n",
    "    def __init__(self, a, b):\n",
    "        super().__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.params = a.params + b.params\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return '{}, {}'.format(self.a.state, self.b.state)\n",
    "\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "\n",
    "    def get_func(self, x, xx):\n",
    "        return self.a.get_func(x, xx) + self.b.get_func(x, xx)\n",
    "\n",
    "    def get_leftgrad(self, x, xx):\n",
    "        k = self.a.get_leftgrad(x, xx) + self.b.get_leftgrad(x, xx)\n",
    "        return k\n",
    "\n",
    "    def get_rightgrad(self, x, xx):\n",
    "        k = self.a.get_rightgrad(x, xx) + self.b.get_rightgrad(x, xx)\n",
    "        return k\n",
    "\n",
    "    def get_gradgrad(self, x, xx):\n",
    "        k = self.a.get_gradgrad(x, xx) + self.b.get_gradgrad(x, xx)\n",
    "        return k\n",
    "\n",
    "\n",
    "class Sub(BinaryOperator):\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "\n",
    "    def get_func(self, x, xx):\n",
    "        return self.a.get_func(x, xx) - self.b.get_func(x, xx)\n",
    "\n",
    "    def get_leftgrad(self, x, xx):\n",
    "        k = self.a.get_leftgrad(x, xx) - self.b.get_leftgrad(x, xx)\n",
    "        return k\n",
    "\n",
    "    def get_rightgrad(self, x, xx):\n",
    "        k = self.a.get_rightgrad(x, xx) - self.b.get_rightgrad(x, xx)\n",
    "        return k\n",
    "\n",
    "    def get_gradgrad(self, x, xx):\n",
    "        k = self.a.get_gradgrad(x, xx) - self.b.get_gradgrad(x, xx)\n",
    "        return k\n",
    "\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "\n",
    "    def get_func(self, x, xx):\n",
    "        return self.a.get_func(x, xx)*self.b.get_func(x, xx)\n",
    "\n",
    "    def get_leftgrad(self, x, xx):\n",
    "        k = (self.a.get_func(x, xx)*self.b.get_leftgrad(x, xx) +\n",
    "             self.b.get_func(x, xx)*self.a.get_leftgrad(x, xx))\n",
    "        return k\n",
    "\n",
    "    def get_rightgrad(self, x, xx):\n",
    "        k = (self.a.get_func(x, xx)*self.b.get_rightgrad(x, xx) +\n",
    "             self.b.get_func(x, xx)*self.a.get_rightgrad(x, xx))\n",
    "        return k\n",
    "\n",
    "    def get_gradgrad(self, x, xx):\n",
    "        k = (self.a.get_func(x, xx)*self.b.get_gradgrad(x, xx) +\n",
    "             self.b.get_func(x, xx)*self.a.get_gradgrad(x, xx) +\n",
    "             self.b.get_leftgrad(x, xx)[:, None]*self.a.get_rightgrad(x, xx)[None] +\n",
    "             self.a.get_leftgrad(x, xx)[:, None]*self.b.get_rightgrad(x, xx)[None])\n",
    "        return k\n",
    "\n",
    "\n",
    "class Pow(Kernel):\n",
    "\n",
    "    def __init__(self, kern, eta):\n",
    "        super().__init__()\n",
    "        self.kern = kern\n",
    "        self.eta = eta\n",
    "        self.params = kern.params\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return '{}, {}'.format(self.kern.state, self.eta)\n",
    "\n",
    "    def get_func(self, x, xx):\n",
    "        return self.kern.get_func(x, xx)**self.eta\n",
    "\n",
    "    def get_leftgrad(self, x, xx):\n",
    "        k = (self.eta*self.kern.get_func(x, xx)**(self.eta-1) *\n",
    "             self.kern.get_leftgrad(x, xx))\n",
    "        return k\n",
    "\n",
    "    def get_rightgrad(self, x, xx):\n",
    "        k = (self.eta*self.kern.get_func(x, xx)**(self.eta-1) *\n",
    "             self.kern.get_rightgrad(x, xx))\n",
    "        return k\n",
    "\n",
    "    def get_gradgrad(self, x, xx):\n",
    "        k = (self.eta*self.kern.get_func(x, xx)**(self.eta-1)*self.kern.get_gradgrad(x, xx) +\n",
    "             self.eta*(self.eta-1)*self.kern.get_func(x, xx)**(self.eta-2) *\n",
    "             self.kern.get_leftgrad(x, xx)[:, None]*self.kern.get_rightgrad(x, xx)[None])\n",
    "        return k\n",
    "\n",
    "\n",
    "class Exp(Kernel):\n",
    "    def __init__(self, kern):\n",
    "        super().__init__()\n",
    "        self.kern = kern\n",
    "        self.params = kern.params\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return '{}'.format(self.kern.state)\n",
    "\n",
    "    def get_func(self, x, xx):\n",
    "        return self.kern.get_func(x, xx).exp()\n",
    "\n",
    "    def get_leftgrad(self, x, xx):\n",
    "        k = self.kern.get_leftgrad(x, xx)*self.kern.get_func(x, xx).exp()\n",
    "        return k\n",
    "\n",
    "    def get_rightgrad(self, x, xx):\n",
    "        k = self.kern.get_rightgrad(x, xx)*self.kern.get_func(x, xx).exp()\n",
    "        return k\n",
    "\n",
    "    def get_gradgrad(self, x, xx):\n",
    "        k = (self.kern.get_gradgrad(x, xx) + self.kern.get_leftgrad(x, xx)[:, None] *\n",
    "             self.kern.get_rightgrad(x, xx)[None])*self.kern.get_func(x, xx).exp()\n",
    "        return k\n",
    "\n",
    "\n",
    "class Real(Kernel):\n",
    "\n",
    "    def __init__(self, value):\n",
    "        super().__init__()\n",
    "        self.value = torch.as_tensor(value)\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return '{}'.format(self.value)\n",
    "\n",
    "    def get_func(self, x, xx):\n",
    "        return self.value.view((x.dim()-1)*[1])\n",
    "\n",
    "    def get_leftgrad(self, x, xx):\n",
    "        return torch.zeros(x.dim()*[1], device=x.device)\n",
    "\n",
    "    def get_rightgrad(self, x, xx):\n",
    "        return torch.zeros(x.dim()*[1], device=x.device)\n",
    "\n",
    "    def get_gradgrad(self, x, xx):\n",
    "        return torch.zeros((x.dim()+1)*[1], device=x.device)\n",
    "\n",
    "\n",
    "class Positive(Kernel):\n",
    "\n",
    "    def __init__(self, signal=1.0, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.signal = signal\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "    @property\n",
    "    def signal(self):\n",
    "        return positive(self._signal)\n",
    "\n",
    "    @signal.setter\n",
    "    def signal(self, value):\n",
    "        v = torch.as_tensor(value)\n",
    "        assert v > 0\n",
    "        self._signal = Parameter(free_form(v))\n",
    "        self.params.append(self._signal)\n",
    "\n",
    "    @property\n",
    "    def requires_grad(self):\n",
    "        return self._signal.requires_grad\n",
    "\n",
    "    @requires_grad.setter\n",
    "    def requires_grad(self, value):\n",
    "        self._signal.requires_grad = value\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return 'signal={}, requires_grad={}'.format(self.signal.data, self.requires_grad)\n",
    "\n",
    "    def get_func(self, x, xx):\n",
    "        return self.signal.view((x.dim()-1)*[1])\n",
    "\n",
    "    def get_leftgrad(self, x, xx):\n",
    "        return torch.zeros(x.dim()*[1], device=x.device)\n",
    "\n",
    "    def get_rightgrad(self, x, xx):\n",
    "        return torch.zeros(x.dim()*[1], device=x.device)\n",
    "\n",
    "    def get_gradgrad(self, x, xx):\n",
    "        return torch.zeros((x.dim()+1)*[1], device=x.device)\n",
    "\n",
    "\n",
    "class White(Kernel):\n",
    "\n",
    "    def __init__(self, signal=1e-3, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.signal = signal\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "    @property\n",
    "    def signal(self):\n",
    "        return positive(self._signal)\n",
    "\n",
    "    @signal.setter\n",
    "    def signal(self, value):\n",
    "        v = torch.as_tensor(value)\n",
    "        assert v > 0\n",
    "        self._signal = Parameter(free_form(v))\n",
    "        self.params.append(self._signal)\n",
    "\n",
    "    @property\n",
    "    def requires_grad(self):\n",
    "        return self._signal.requires_grad\n",
    "\n",
    "    @requires_grad.setter\n",
    "    def requires_grad(self, value):\n",
    "        self._signal.requires_grad = value\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return 'signal={}, requires_grad={}'.format(self.signal.data, self.requires_grad)\n",
    "\n",
    "    def get_func(self, x, xx):\n",
    "        return self.signal*x.isclose(xx).all(dim=0).type(x.type())\n",
    "\n",
    "\n",
    "class SqD(Kernel):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return ''\n",
    "\n",
    "    def get_func(self, x, xx):\n",
    "        return (x-xx).pow(2).sum(dim=0)\n",
    "\n",
    "    def get_leftgrad(self, x, xx):\n",
    "        return 2*(x-xx)\n",
    "\n",
    "    def get_rightgrad(self, x, xx):\n",
    "        return -2*(x-xx)\n",
    "\n",
    "    def get_gradgrad(self, x, xx):\n",
    "        # Note: output.expand(d, d, *trail) may be needed\n",
    "        d = x.size(0)\n",
    "        trail = x.size()[1:]\n",
    "        return -2*torch.eye(d, device=x.device).view(d, d, *(len(trail)*[1]))\n",
    "\n",
    "\n",
    "class DotProd(Kernel):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return ''\n",
    "\n",
    "    def get_func(self, x, xx):\n",
    "        return (x*xx).sum(dim=0)\n",
    "\n",
    "    def get_leftgrad(self, x, xx):\n",
    "        return torch.ones_like(x)*xx\n",
    "\n",
    "    def get_rightgrad(self, x, xx):\n",
    "        return x*torch.ones_like(xx)\n",
    "\n",
    "    def get_gradgrad(self, x, xx):\n",
    "        # Note: output.expand(d, d, *trail) may be needed\n",
    "        d = x.size(0)\n",
    "        trail = x.size()[1:]\n",
    "        return torch.eye(d, device=x.device).view(d, d, *(len(trail)*[1]))\n",
    "\n",
    "\n",
    "class Normed(Kernel):\n",
    "\n",
    "    def __init__(self, kern):\n",
    "        super().__init__()\n",
    "        self.kern = kern\n",
    "        self.params = kern.params\n",
    "        self.eps = torch.finfo().eps\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return self.kern.state\n",
    "\n",
    "    def get_func(self, x, xx):\n",
    "        n = x.norm(dim=0).clamp(min=self.eps)\n",
    "        nn = xx.norm(dim=0).clamp(min=self.eps)\n",
    "        return self.kern.get_func(x/n, xx/nn)\n",
    "\n",
    "    def get_leftgrad(self, x, xx):\n",
    "        n = x.norm(dim=0).clamp(min=self.eps)\n",
    "        nn = xx.norm(dim=0).clamp(min=self.eps)\n",
    "        y = x/n\n",
    "        yy = xx/nn\n",
    "        f = self.kern.get_leftgrad(y, yy)\n",
    "        return f/n - (f*x).sum(dim=0)*x/n**3\n",
    "\n",
    "    def get_rightgrad(self, x, xx):\n",
    "        n = x.norm(dim=0).clamp(min=self.eps)\n",
    "        nn = xx.norm(dim=0).clamp(min=self.eps)\n",
    "        y = x/n\n",
    "        yy = xx/nn\n",
    "        f = self.kern.get_rightgrad(y, yy)\n",
    "        return f/nn - (f*xx).sum(dim=0)*xx/nn**3\n",
    "\n",
    "    def get_gradgrad(self, x, xx):\n",
    "        n = x.norm(dim=0).clamp(min=self.eps)\n",
    "        nn = xx.norm(dim=0).clamp(min=self.eps)\n",
    "        y = x/n\n",
    "        yy = xx/nn\n",
    "        f = self.kern.get_gradgrad(y, yy)\n",
    "        gg = (f/(n*nn) - (f*x[:, None]).sum(dim=0)*x[:, None]/(n**3*nn) -\n",
    "              (f*xx[None]).sum(dim=1, keepdim=True)*xx/(n*nn**3) +\n",
    "              (f*x[:, None]*xx[None]).sum(dim=(0, 1)) * x[:, None]*xx[None]/(n**3*nn**3))\n",
    "        return gg\n",
    "\n",
    "\n",
    "class ScaledInput(Kernel):\n",
    "\n",
    "    def __init__(self, kern, scale=1.0):\n",
    "        super().__init__()\n",
    "        self.kern = kern\n",
    "        self.params = kern.params\n",
    "        self.scale = scale\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return '{}, scale={}'.format(self.kern.state, self.scale.data)\n",
    "\n",
    "    @property\n",
    "    def scale(self):\n",
    "        return positive(self._scale)\n",
    "\n",
    "    @scale.setter\n",
    "    def scale(self, value):\n",
    "        v = torch.as_tensor(value).view(-1)\n",
    "        assert (v > 0).all()\n",
    "        self._scale = Parameter(free_form(v))\n",
    "        self.params.append(self._scale)\n",
    "\n",
    "    def get_func(self, x, xx):\n",
    "        scale = self.scale\n",
    "        while scale.dim() < x.dim():\n",
    "            scale = scale[..., None]\n",
    "        return self.kern.get_func(x/scale, xx/scale)\n",
    "\n",
    "    def get_leftgrad(self, x, xx):\n",
    "        scale = self.scale\n",
    "        while scale.dim() < x.dim():\n",
    "            scale = scale[..., None]\n",
    "        return self.kern.get_leftgrad(x/scale, xx/scale)/scale\n",
    "\n",
    "    def get_rightgrad(self, x, xx):\n",
    "        scale = self.scale\n",
    "        while scale.dim() < x.dim():\n",
    "            scale = scale[..., None]\n",
    "        return self.kern.get_rightgrad(x/scale, xx/scale)/scale\n",
    "\n",
    "    def get_gradgrad(self, x, xx):\n",
    "        scale = self.scale\n",
    "        while scale.dim() < x.dim():\n",
    "            scale = scale[..., None]\n",
    "        return self.kern.get_gradgrad(x/scale, xx/scale)/(scale[:, None]*scale[None])\n",
    "\n",
    "\n",
    "def test_kernel_gradients(kern, dim=3):\n",
    "\n",
    "    x = torch.rand(7, dim, requires_grad=True)\n",
    "    xx = torch.rand(11, dim, requires_grad=True)\n",
    "\n",
    "    kern(x, xx).sum().backward()\n",
    "    g = kern.leftgrad(x, xx).sum(dim=(2)).t()\n",
    "    print(g.allclose(x.grad), (g-x.grad).abs().max().data)\n",
    "\n",
    "    g = kern.rightgrad(x, xx).sum(dim=(1)).t()\n",
    "    print(g.allclose(xx.grad), (g-xx.grad).abs().max().data)\n",
    "\n",
    "    x.grad *= 0\n",
    "    kern.rightgrad(x, xx).sum().backward()\n",
    "    g = kern.gradgrad(x, xx).sum(dim=(1, 3)).t()\n",
    "    print(g.allclose(x.grad), (g-x.grad).abs().max().data)\n",
    "\n",
    "    xx.grad *= 0\n",
    "    kern.leftgrad(x, xx).sum().backward()\n",
    "    g = kern.gradgrad(x, xx).sum(dim=(0, 2)).t()\n",
    "    print(g.allclose(xx.grad), (g-xx.grad).abs().max().data)\n",
    "\n",
    "\n",
    "def example():\n",
    "    polynomial = Positive(requires_grad=True) * \\\n",
    "        (DotProd() + Positive(1e-4, requires_grad=True))**2\n",
    "    squaredexp = (SqD()*Real(-0.5)).exp()\n",
    "\n",
    "\n",
    "def test():\n",
    "    from theforce.regression.core import SquaredExp\n",
    "    from theforce.regression.gp import Covariance\n",
    "\n",
    "    x = torch.rand(23, 7)\n",
    "    xx = torch.rand(19, 7)\n",
    "    old = Covariance(SquaredExp(dim=7))\n",
    "    new = (SqD()*Real(-0.5)).exp()\n",
    "    func = old(x, xx).allclose(new(x, xx))\n",
    "    leftgrad = old.leftgrad(x, xx).allclose(new.leftgrad(\n",
    "        x, xx).permute(1, 0, 2).reshape(x.numel(), xx.size(0)))\n",
    "    rightgrad = old.rightgrad(x, xx).allclose(new.rightgrad(\n",
    "        x, xx).permute(1, 2, 0).reshape(x.size(0), xx.numel()))\n",
    "    gradgrad = old.gradgrad(x, xx).allclose(new.gradgrad(\n",
    "        x, xx).permute(2, 0, 3, 1).reshape(x.numel(), xx.numel()))\n",
    "    print('Squared-Exponential kernel with two different methods: \\n{}\\n{}\\n{}\\n{}'.format(\n",
    "        func, leftgrad, rightgrad, gradgrad))\n",
    "\n",
    "    # try empty tensor\n",
    "    x = torch.rand(0, 7)\n",
    "    new(x, xx)\n",
    "\n",
    "    # test kernels gradients\n",
    "    kern = ScaledInput(Normed(DotProd()), scale=torch.rand(3))\n",
    "    print('test gradients of kernel: {}'.format(kern.state))\n",
    "    test_kernel_gradients(kern)\n",
    "    from torch import tensor\n",
    "    assert eval(kern.state).state == kern.state\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
