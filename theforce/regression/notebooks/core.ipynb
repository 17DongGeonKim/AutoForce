{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" experimental \"\"\"\n",
    "import torch\n",
    "from torch.nn import Module, Parameter\n",
    "from theforce.regression.algebra import free_form, positive\n",
    "\n",
    "\"\"\" \n",
    "1. The kernels are defined such that they return derivatives\n",
    "of Gram matrix wrt r (r=x-x'). To convert them wrt x, x'\n",
    "usually just a multiplication by -1 maybe needed.\n",
    "For consistency the gradgrad in LazyWhite is manually \n",
    "multiplied by -1.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class LazyWhite(Module):\n",
    "    \"\"\" special stationary kernel \"\"\"\n",
    "\n",
    "    def __init__(self, dim=1, signal=0.0, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self._signal = Parameter(free_form(torch.as_tensor(signal)),\n",
    "                                 requires_grad=requires_grad)\n",
    "        self.params = [self._signal]\n",
    "\n",
    "    def forward(self, x=None, xx=None, operation='func'):\n",
    "        x_in = x is not None\n",
    "        xx_in = xx is not None\n",
    "        if not x_in and not xx_in:\n",
    "            k = self.diag()\n",
    "        elif x_in and not xx_in:\n",
    "            k = self.diag(x).diag()\n",
    "        elif xx_in and not x_in:\n",
    "            k = self.diag(xx).diag()\n",
    "        elif x_in and xx_in:\n",
    "            if x.shape == xx.shape and torch.allclose(x, xx):\n",
    "                k = self.diag(x).diag()\n",
    "            else:\n",
    "                k = torch.zeros(x.size(0), xx.size(0))\n",
    "        if k.dim() == 0 or operation == 'func':\n",
    "            return k\n",
    "        if operation == 'grad':\n",
    "            return k[..., None]*torch.ones(self.dim)\n",
    "        elif operation == 'gradgrad':\n",
    "            return (k[..., None, None]*torch.eye(self.dim)\n",
    "                    ).permute(0, 2, 1, 3) * (-1)             # NOTE 1.\n",
    "\n",
    "    def diag(self, x=None, operation='func'):\n",
    "        if x is None:\n",
    "            return positive(self._signal).pow(2)\n",
    "        else:\n",
    "            if operation == 'func':\n",
    "                return positive(self._signal).pow(2)*torch.ones(x.size(0))\n",
    "            elif operation == 'grad':\n",
    "                raise NotImplementedError('This is not supposed to happen!')\n",
    "            elif operation == 'gradgrad':\n",
    "                return positive(self._signal).pow(2)*torch.ones(x.numel())\n",
    "\n",
    "\n",
    "class Displacement(Module):\n",
    "\n",
    "    def __init__(self, dim=1):\n",
    "        super().__init__()\n",
    "        self._scale = Parameter(free_form(torch.ones(dim)))\n",
    "\n",
    "    def x_xx(self, x=None, xx=None):\n",
    "        if x is None and xx is None:\n",
    "            x = torch.ones(0, self._scale.size(0))\n",
    "            xx = torch.ones(0, self._scale.size(0))\n",
    "        elif x is None:\n",
    "            x = xx\n",
    "        elif xx is None:\n",
    "            xx = x\n",
    "        return x, xx\n",
    "\n",
    "    def forward(self, x=None, xx=None):\n",
    "        x, xx = self.x_xx(x, xx)\n",
    "        return (x[:, None]-xx[None])/positive(self._scale)\n",
    "\n",
    "    def delta(self):\n",
    "        return torch.eye(self._scale.size(0))\n",
    "\n",
    "    def divide(self, operation):\n",
    "        if operation is 'func':\n",
    "            return 1.0\n",
    "        else:\n",
    "            scale = positive(self._scale)\n",
    "            if operation is 'grad':\n",
    "                return scale\n",
    "            elif operation is 'gradgrad':\n",
    "                return (scale[None]*scale[:, None])[:, None]\n",
    "\n",
    "    def extra_repr(self):\n",
    "        print('length scales: {}'.format(positive(self._scale)))\n",
    "\n",
    "\n",
    "class Stationary(Module):\n",
    "    \"\"\" [dim=1, signal=1] \"\"\"\n",
    "\n",
    "    def __init__(self, dim=1, signal=1.0):\n",
    "        super().__init__()\n",
    "        self.r = Displacement(dim=dim)\n",
    "        self._signal = Parameter(free_form(torch.as_tensor(signal)))\n",
    "        self.params = [self.r._scale, self._signal]\n",
    "\n",
    "    def forward(self, x=None, xx=None, operation='func'):\n",
    "        return positive(self._signal).pow(2)*getattr(self, operation)(self.r(x=x, xx=xx)) \\\n",
    "            / self.r.divide(operation)\n",
    "\n",
    "    def diag(self, x=None, operation='func'):\n",
    "        if x is None:\n",
    "            return positive(self._signal).pow(2)\n",
    "        else:\n",
    "            if operation == 'func':\n",
    "                return positive(self._signal).pow(2)*torch.ones(x.size(0))\n",
    "            elif operation == 'grad':\n",
    "                raise NotImplementedError('This is not supposed to happen!')\n",
    "            elif operation == 'gradgrad':\n",
    "                return (positive(self._signal).pow(2)*self.d2diag() *\n",
    "                        (1./self.r.divide('grad'))**2).repeat(x.size(0))\n",
    "\n",
    "    def extra_repr(self):\n",
    "        print('signal variance: {}'.format(positive(self._signal).pow(2)))\n",
    "\n",
    "\n",
    "class SquaredExp(Stationary):\n",
    "    \"\"\" [dim=1, signal=1] \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def func(self, r):\n",
    "        return (-(r**2).sum(dim=-1)/2).exp()\n",
    "\n",
    "    def grad(self, r):\n",
    "        cov = self.func(r)\n",
    "        return -r*cov[..., None]\n",
    "\n",
    "    def gradgrad(self, r):\n",
    "        cov = self.func(r)\n",
    "        return ((r[..., None, :]*r[..., None]-self.r.delta())*cov[..., None, None]\n",
    "                ).permute(0, 2, 1, 3)\n",
    "\n",
    "    def d2diag(self):\n",
    "        \"\"\" second deriv of kernel wrt r[i] at r[i]=0 \"\"\"\n",
    "        return torch.tensor(-1.0)\n",
    "\n",
    "\n",
    "def test():\n",
    "    if 1:\n",
    "        dim = 2\n",
    "        kern = SquaredExp(dim=dim)\n",
    "        x = torch.rand(19, dim)\n",
    "        xx = torch.rand(37, dim)\n",
    "        K = kern(x=x, xx=xx)\n",
    "        assert torch.allclose(K, (-(x[:, None]-xx[None])**2/2)\n",
    "                              .sum(dim=-1).exp())\n",
    "\n",
    "    if 1:\n",
    "        white = LazyWhite(signal=1.0)\n",
    "        x = torch.rand(13, dim)\n",
    "        assert (white(x, x) == torch.eye(13)).all()\n",
    "\n",
    "    if 1:\n",
    "        kern = SquaredExp(dim=dim)\n",
    "        white = LazyWhite(dim=dim, signal=1.0)\n",
    "        assert kern(x, xx, 'func').shape == white(x, xx, 'func').shape\n",
    "        assert kern(x, xx, 'grad').shape == white(x, xx, 'grad').shape\n",
    "        assert kern(x, xx, 'gradgrad').shape == white(x, xx, 'gradgrad').shape\n",
    "        K = white(x, operation='gradgrad')\n",
    "        assert (K.reshape(x.numel(), x.numel()) == (-1) *\n",
    "                torch.eye(x.numel())).all()  # see NOTE 1. for -1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
