{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Parameter\n",
    "from theforce.regression.kernel import Kernel\n",
    "from theforce.regression.algebra import free_form, positive\n",
    "\n",
    "\n",
    "class Stationary(Kernel):\n",
    "    \"\"\"depends only on (x-xx)/l\"\"\"\n",
    "\n",
    "    def __init__(self, signal=1.0, lengthscale=1.0):\n",
    "        super().__init__()\n",
    "        self.signal = signal\n",
    "        self.lengthscale = lengthscale\n",
    "\n",
    "    @property\n",
    "    def signal(self):\n",
    "        return positive(self._signal)\n",
    "\n",
    "    @signal.setter\n",
    "    def signal(self, value):\n",
    "        v = torch.as_tensor(value)\n",
    "        assert v > 0\n",
    "        self._signal = Parameter(free_form(v))\n",
    "        self.params.append(self._signal)\n",
    "\n",
    "    @property\n",
    "    def lengthscale(self):\n",
    "        return positive(self._lengthscale)\n",
    "\n",
    "    @lengthscale.setter\n",
    "    def lengthscale(self, value):\n",
    "        v = torch.as_tensor(value).view(-1)\n",
    "        assert (v > 0).all()\n",
    "        self._lengthscale = Parameter(free_form(v))\n",
    "        self.params.append(self._lengthscale)\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return 'signal={}, lengthscale={}'.format(self.signal.data, self.lengthscale.data)\n",
    "\n",
    "    def l(self, x):\n",
    "        l = self.lengthscale\n",
    "        while l.dim() < x.dim():\n",
    "            l = l.unsqueeze(-1)\n",
    "        return l\n",
    "\n",
    "    def r(self, x, xx):\n",
    "        return (x-xx)/self.l(x)\n",
    "\n",
    "    def get_func(self, x, xx):\n",
    "        return self.signal*self.get_k(self.r(x, xx))\n",
    "\n",
    "    def get_leftgrad(self, x, xx):\n",
    "        return self.signal*self.get_dk(self.r(x, xx))/self.l(x)\n",
    "\n",
    "    def get_rightgrad(self, x, xx):\n",
    "        return -self.signal*self.get_dk(self.r(x, xx))/self.l(x)\n",
    "\n",
    "    def get_gradgrad(self, x, xx):\n",
    "        l = self.l(x)\n",
    "        return -self.signal*self.get_d2k(self.r(x, xx))/(l[:, None]*l[None])\n",
    "\n",
    "    # overload the following methods\n",
    "    def get_k(self, r):\n",
    "        \"\"\"considered to be 1 at r=0 but not a real constraint\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'get_k in {}'.format(self.__class__.__name__))\n",
    "\n",
    "    def get_dk(self, r):\n",
    "        raise NotImplementedError(\n",
    "            'get_dk in {}'.format(self.__class__.__name__))\n",
    "\n",
    "    def get_d2k(self, r):\n",
    "        raise NotImplementedError(\n",
    "            'get_d2k in {}'.format(self.__class__.__name__))\n",
    "\n",
    "\n",
    "class RBF(Stationary):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def get_k(self, r):\n",
    "        return (-0.5*(r**2).sum(dim=0)).exp()\n",
    "\n",
    "    def get_dk(self, r):\n",
    "        return -r*self.get_k(r)\n",
    "\n",
    "    def get_d2k(self, r):\n",
    "        eye = torch.eye(r.size(0))\n",
    "        while eye.dim() < r.dim()+1:\n",
    "            eye = eye.unsqueeze(-1)\n",
    "        return (r[:, None]*r[None]-eye)*self.get_k(r)\n",
    "\n",
    "\n",
    "def test():\n",
    "    from theforce.regression.core import SquaredExp\n",
    "    from theforce.regression.gp import Covariance\n",
    "\n",
    "    x = torch.rand(23, 7)\n",
    "    xx = torch.rand(19, 7)\n",
    "    l = torch.rand(7)\n",
    "    s = 2.7\n",
    "    old = Covariance(SquaredExp(signal=s, dim=7, scale=l))\n",
    "    new = RBF(signal=s**2, lengthscale=l)\n",
    "    func = old(x, xx).allclose(new(x, xx))\n",
    "    leftgrad = old.leftgrad(x, xx).allclose(new.leftgrad(\n",
    "        x, xx).permute(1, 0, 2).reshape(x.numel(), xx.size(0)))\n",
    "    rightgrad = old.rightgrad(x, xx).allclose(new.rightgrad(\n",
    "        x, xx).permute(1, 2, 0).reshape(x.size(0), xx.numel()))\n",
    "    gradgrad = old.gradgrad(x, xx).allclose(new.gradgrad(\n",
    "        x, xx).permute(2, 0, 3, 1).reshape(x.numel(), xx.numel()))\n",
    "    print('Squared-Exponential kernel with two different methods: \\n{}\\n{}\\n{}\\n{}'.format(\n",
    "        func, leftgrad, rightgrad, gradgrad))\n",
    "\n",
    "    # test if backward works\n",
    "    new(x, xx).sum().backward()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
