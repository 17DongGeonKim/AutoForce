{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Two classes are defined: GPR and SparseGPR.\n",
    "The latter is a thin wrapper around the former.\n",
    "Essentially the only responsibility of the latter is to\n",
    "control the inducing data points.\n",
    "They could be variational parameters, greedily selected,\n",
    "or just simple (constant) tensors.\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.nn import Module, Parameter\n",
    "from torch.distributions import LowRankMultivariateNormal\n",
    "from theforce.regression.algebra import positive, free_form, sum_packed_dim\n",
    "from theforce.regression.algebra import low_rank_factor, jitcholesky\n",
    "from theforce.regression.kernels import RBF\n",
    "import warnings\n",
    "\n",
    "\n",
    "class GPR(Module):\n",
    "\n",
    "    def __init__(self, X, Z, Y, chunks=None, derivatives=None):\n",
    "        super(GPR, self).__init__()  # TODO: if Z is None: do full GPR\n",
    "\n",
    "        self.X = X\n",
    "\n",
    "        # values\n",
    "        if Y is not None:\n",
    "            self.chunks = chunks\n",
    "            if chunks:  # TODO: generalize Const mean to more general types\n",
    "                _s = torch.as_tensor(chunks).type(Y.dtype)\n",
    "                self.mean = (Y/_s).mean()\n",
    "            else:\n",
    "                _s = 1\n",
    "                self.mean = Y.mean()\n",
    "            data = [Y - self.mean*_s]\n",
    "            self.use_values = True\n",
    "        else:\n",
    "            self.mean = 0\n",
    "            data = []\n",
    "            self.use_values = False\n",
    "\n",
    "        # derivatives\n",
    "        if derivatives is not None:\n",
    "            self.dX_dt = torch.as_tensor(derivatives[0])\n",
    "            data += [torch.as_tensor(derivatives[1]).view(-1)]\n",
    "            self.use_derivatives = True\n",
    "        else:\n",
    "            self.use_derivatives = False\n",
    "\n",
    "        # parameters\n",
    "        self._noise = Parameter(free_form(torch.tensor(1., dtype=X.dtype)))\n",
    "        self.Z = Z       # Parameter or not is controled from SparseGPR\n",
    "        self.kern = RBF(torch.ones_like(X[0]), torch.tensor(1., dtype=X.dtype))\n",
    "\n",
    "        # combine values and derivatives:\n",
    "        self.Y = torch.cat(data)\n",
    "        self.zeros = torch.zeros_like(self.Y)\n",
    "        self.ones = torch.ones_like(self.Y)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        print('\\nSGPR:\\nnoise: {}'.format(positive(self._noise)))\n",
    "        print('mean function used: constant {}\\n'.format(self.mean))\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    def covariances(self):\n",
    "        ZZ = self.kern.cov_matrix(self.Z, self.Z)\n",
    "\n",
    "        # values\n",
    "        if self.use_values:\n",
    "            ZX = self.kern.cov_matrix(self.Z, self.X)\n",
    "            if self.chunks:\n",
    "                ZX = sum_packed_dim(ZX, self.chunks)\n",
    "                tr = torch.stack([self.kern.cov_matrix(x, x).sum()\n",
    "                                  for x in torch.split(self.X, self.chunks)]).sum()\n",
    "            else:\n",
    "                tr = self.X.size()[0]*self.kern.diag()\n",
    "        else:\n",
    "            tr = 0\n",
    "            ZX = None\n",
    "\n",
    "        # derivatives\n",
    "        if self.use_derivatives:\n",
    "            dZX = self.kern.cov_matrix(self.Z, self.X, d_dtheta=self.dX_dt,\n",
    "                                       wrt=1).view(self.Z.size()[0], -1)\n",
    "            tr = tr + self.kern.diag_derivatives(self.dX_dt)\n",
    "            if ZX is None:\n",
    "                ZX = dZX\n",
    "            else:\n",
    "                ZX = torch.cat((ZX, dZX), dim=1)\n",
    "        return ZZ, ZX, tr\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "        # covariances\n",
    "        ZZ, ZX, tr = self.covariances()\n",
    "        noise = positive(self._noise)\n",
    "\n",
    "        # trace term\n",
    "        Q, _, ridge = low_rank_factor(ZZ, ZX)\n",
    "        trace = 0.5*(tr - torch.einsum('ij,ij', Q, Q))/noise**2\n",
    "\n",
    "        # low rank MVN\n",
    "        p = LowRankMultivariateNormal(self.zeros, Q.t(), self.ones*noise**2)\n",
    "\n",
    "        # loss\n",
    "        loss = -p.log_prob(self.Y) + trace\n",
    "        return loss\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    def evaluate(self):\n",
    "\n",
    "        ZZ, ZX, _ = self.covariances()\n",
    "        XZ = ZX.t()\n",
    "        noise = positive(self._noise)\n",
    "\n",
    "        # numerically stable calculation of _mu\n",
    "        L, ridge = jitcholesky(ZZ, jitbase=2)\n",
    "        A = torch.cat((XZ, noise * L.t()))\n",
    "        Y = torch.cat((self.Y, torch.zeros(self.Z.size()[0],\n",
    "                                           dtype=self.Y.dtype)))\n",
    "        Q, R = torch.qr(A)\n",
    "        self._mu = torch.mv(R.inverse(), torch.mv(Q.t(), Y))\n",
    "\n",
    "        # inducing function values (Z, u)\n",
    "        self.u = self.mean + torch.mv(ZZ, self._mu)\n",
    "\n",
    "        # covariance ------------------------------ TODO: this is slightly ugly!\n",
    "        ZZ_i = torch.mm(L.t().inverse(), L.inverse())\n",
    "        SIGMA = ZZ + torch.mm(XZ.t(), XZ) / noise**2\n",
    "        # SIGMA_i = SIGMA.inverse()\n",
    "        Q, R = torch.qr(SIGMA)\n",
    "        SIGMA_i = torch.mm(R.inverse(), Q.t())\n",
    "        self._sig = SIGMA_i - ZZ_i\n",
    "        # ------------------------------------------------------------------------\n",
    "\n",
    "        self.ready = 1\n",
    "\n",
    "    @staticmethod\n",
    "    def out(x, array=False):\n",
    "        if x is None:\n",
    "            return x\n",
    "        else:\n",
    "            if array:\n",
    "                return x.detach().numpy()\n",
    "            else:\n",
    "                return x\n",
    "\n",
    "    def predict(self, X, var=False, array=True, derivative=None):\n",
    "        if not hasattr(self, 'ready') or not self.ready:\n",
    "            self.evaluate()\n",
    "        _X = torch.as_tensor(X)\n",
    "\n",
    "        # predictive mean\n",
    "        XZ = self.kern.cov_matrix(_X, self.Z)\n",
    "        mu = self.mean + torch.mv(XZ, self._mu)\n",
    "\n",
    "        # predictive variance\n",
    "        if var:\n",
    "            sig = torch.ones(_X.size()[0], dtype=self.X.dtype)*self.kern.diag() + \\\n",
    "                torch.mm(XZ, torch.mm(self._sig, XZ.t())).diag()\n",
    "            if (sig < 0).any():\n",
    "                sig = torch.clamp(sig, 0)\n",
    "                warnings.warn(\n",
    "                    'variance clamped! variance is not numerically stable yet!')\n",
    "        else:\n",
    "            sig = None\n",
    "\n",
    "        # derivative\n",
    "        if derivative is not None:\n",
    "            dXZ = self.kern.cov_matrix(_X, self.Z, d_dtheta=derivative)\n",
    "            deriv = torch.einsum('ij...,j->i...', dXZ, self._mu)\n",
    "        else:\n",
    "            deriv = None\n",
    "\n",
    "        return (self.out(out, array=array) for out in (mu, sig, deriv))\n",
    "\n",
    "    # training -------------------------------------------------------------------\n",
    "    def train(self, steps=100, optimizer=None, lr=0.1):\n",
    "\n",
    "        if not hasattr(self, 'losses'):\n",
    "            self.losses = []\n",
    "            self.starts = []\n",
    "        self.starts += [len(self.losses)]\n",
    "\n",
    "        if optimizer is None:\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        for _ in range(steps):\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.forward()\n",
    "            self.losses += [loss.data]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('trained for {} steps'.format(steps))\n",
    "\n",
    "        self.ready = 0\n",
    "\n",
    "\n",
    "class SparseGPR(GPR):\n",
    "\n",
    "    def __init__(self, X, Y, num_inducing, chunks=None, derivatives=None):\n",
    "        Z = Parameter(X[torch.randint(X.size()[0], (num_inducing,))])\n",
    "        super(SparseGPR, self).__init__(X, Z, Y, chunks=chunks,\n",
    "                                        derivatives=derivatives)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        super(SparseGPR, self).extra_repr()\n",
    "        print('\\nSparseGPR:\\nZ:\\n{}\\n'.format(self.Z.data))\n",
    "\n",
    "    def pre_forward(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self):\n",
    "        self.pre_forward()\n",
    "        return super(SparseGPR, self).forward()\n",
    "\n",
    "\n",
    "# testing -------------------------------------------------------------------------\n",
    "def test_if_works():\n",
    "    import numpy as np\n",
    "    import pylab as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "    # dummy data\n",
    "    chunks = torch.randint(1, 10, (100,)).tolist()\n",
    "    X = torch.cat([(torch.rand(size, 1)-0.5)*5 for size in chunks])\n",
    "    Y = (X.tanh() * (-X**2).exp()).view(-1) + 1 * 10.\n",
    "\n",
    "    # transorm Y -> YY, trans\n",
    "    YY = sum_packed_dim(Y, chunks)\n",
    "\n",
    "    # define model\n",
    "    #model = SparseGPR(X, Y, 6, None)\n",
    "    model = SparseGPR(X, YY, 6, chunks)\n",
    "\n",
    "    # training\n",
    "    model.train(100)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    ax1.plot(model.losses)\n",
    "\n",
    "    # predict\n",
    "    with torch.no_grad():\n",
    "        ax2.scatter(X, Y)\n",
    "        Xtest = torch.arange(-3, 3, 0.1, dtype=X.dtype).view(-1, 1)\n",
    "        Ytest, var, deriv = model.predict(Xtest, var=True, array=False)\n",
    "        x, y, err = Xtest.numpy().reshape(-1), Ytest.numpy(), torch.sqrt(var).numpy()*10\n",
    "        ax2.plot(x, y, color='green')\n",
    "        ax2.fill_between(x, y-err, y+err, alpha=0.2)\n",
    "        Z = model.Z.detach().numpy().reshape(-1)\n",
    "        u = model.u.detach().numpy()\n",
    "        ax2.scatter(Z, u, marker='x', s=200, color='red')\n",
    "\n",
    "        Ytest, _, _ = model.predict(Xtest, var=False)\n",
    "        assert Ytest.__class__ == np.ndarray\n",
    "\n",
    "    print(model)\n",
    "\n",
    "\n",
    "def second_test():\n",
    "    \"\"\"\n",
    "    Here we have a chain relationship as Y=Y(X(T)).\n",
    "    The data we are given are (X, dX_dT, dY_dT).\n",
    "    We want to regress the latent function Y = f(X).\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from theforce.regression.algebra import _2pi\n",
    "    import pylab as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "    chunks = torch.randint(1, 10, (10,)).tolist()\n",
    "    T = torch.cat([torch.rand(size, 1) for size in chunks])\n",
    "    X = _2pi*torch.cos(_2pi*T)\n",
    "    dX_dT = -_2pi**2*torch.sin(_2pi*T)\n",
    "    Y = torch.squeeze(torch.sin(X))\n",
    "    dY_dT = dX_dT*torch.cos(X)\n",
    "    YY = sum_packed_dim(Y, chunks)\n",
    "\n",
    "    # define model\n",
    "    dX_dT = torch.unsqueeze(dX_dT, dim=-1)\n",
    "    model = SparseGPR(X, YY, 6, chunks, derivatives=(dX_dT, dY_dT))\n",
    "\n",
    "    # return 0\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    ax1.scatter(X, dX_dT)\n",
    "    ax2.scatter(X, dY_dT)\n",
    "    ax3.scatter(X, Y)\n",
    "\n",
    "    # training\n",
    "    model.train(300)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    ax1.plot(model.losses)\n",
    "\n",
    "    # predict\n",
    "    with torch.no_grad():\n",
    "        ax2.scatter(X, Y)\n",
    "        #T = torch.arange(-1., 1., 0.01, dtype=X.dtype).view(-1, 1)\n",
    "        #Xtest = _2pi*torch.cos(_2pi*T)\n",
    "        Xtest = torch.arange(-_2pi, _2pi, 0.1, dtype=X.dtype).view(-1, 1)\n",
    "        Ytest, var, _ = model.predict(Xtest, var=True, array=False)\n",
    "        x, y, err = Xtest.numpy().reshape(-1), Ytest.numpy(), torch.sqrt(var).numpy()*10\n",
    "        ax2.plot(x, y, color='green')\n",
    "        ax2.fill_between(x, y-err, y+err, alpha=0.2)\n",
    "        Z = model.Z.detach().numpy().reshape(-1)\n",
    "        u = model.u.detach().numpy()\n",
    "        ax2.scatter(Z, u, marker='x', s=200, color='red')\n",
    "\n",
    "        Ytest, _, _ = model.predict(Xtest, var=False)\n",
    "        assert Ytest.__class__ == np.ndarray\n",
    "\n",
    "    print(model)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # test_if_works()\n",
    "\n",
    "    second_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
