{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Two classes are defined: SGPR and SparseGPR.\n",
    "The latter is a thin wrapper around the former.\n",
    "Essentially the only responsibility of the latter is to\n",
    "control the inducing data points.\n",
    "They could be variational parameters, greedily selected,\n",
    "or just simple (constant) tensors.\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.nn import Module, Parameter\n",
    "from torch.distributions import LowRankMultivariateNormal\n",
    "from theforce.regression.algebra import positive, free_form, sum_packed_dim\n",
    "from theforce.regression.algebra import low_rank_factor, jitcholesky\n",
    "from theforce.regression.kernels import RBF\n",
    "import warnings\n",
    "\n",
    "\n",
    "class SGPR(Module):\n",
    "\n",
    "    def __init__(self, X, Y, Z, sizes=None):\n",
    "        super(SGPR, self).__init__()\n",
    "\n",
    "        self.X = X\n",
    "        self.sizes = sizes\n",
    "        if sizes:\n",
    "            _s = torch.as_tensor(sizes).type(Y.dtype)\n",
    "            self.mean = (Y/_s).mean()\n",
    "        else:\n",
    "            _s = 1\n",
    "            self.mean = Y.mean()\n",
    "        self.Y = Y - self.mean*_s           # TODO: generalize Const mean to more types\n",
    "\n",
    "        # parameters\n",
    "        self._noise = Parameter(free_form(torch.tensor(1., dtype=X.dtype)))\n",
    "        self.Z = Z       # Parameter or not is controled from SparseGPR\n",
    "        self.kern = RBF(torch.ones_like(X[0]), torch.tensor(1., dtype=X.dtype))\n",
    "\n",
    "        # zeros and ones\n",
    "        self.zeros = torch.zeros_like(self.Y)\n",
    "        self.ones = torch.ones_like(self.Y)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        print('\\nSGPR:\\nnoise: {}\\n'.format(positive(self._noise)))\n",
    "        print('\\nSGPR:\\nmean function used: constant {}\\n'.format(self.mean))\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "\n",
    "    def forward(self):\n",
    "        noise = positive(self._noise)\n",
    "        ZZ = self.kern.cov_matrix(self.Z, self.Z)\n",
    "        ZX = self.kern.cov_matrix(self.Z, self.X)\n",
    "\n",
    "        # if a transformation on ZX is needed or not # TODO: more efficient way?\n",
    "        if self.sizes:\n",
    "            ZX = sum_packed_dim(ZX, self.sizes)\n",
    "            tr = torch.stack([self.kern.cov_matrix(x, x).sum()\n",
    "                              for x in torch.split(self.X, self.sizes)]).sum()\n",
    "        else:\n",
    "            tr = self.X.size()[0]*self.kern.diag()\n",
    "\n",
    "        # trace term\n",
    "        Q, _, ridge = low_rank_factor(ZZ, ZX)\n",
    "        trace = 0.5*(tr - torch.einsum('ij,ij', Q, Q))/noise**2\n",
    "\n",
    "        # low rank MVN\n",
    "        p = LowRankMultivariateNormal(self.zeros, Q.t(), self.ones*noise**2)\n",
    "\n",
    "        # loss\n",
    "        loss = -p.log_prob(self.Y) + trace\n",
    "        return loss\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    def evaluate(self):\n",
    "        noise = positive(self._noise)\n",
    "        ZZ = self.kern.cov_matrix(self.Z, self.Z)\n",
    "        XZ = self.kern.cov_matrix(self.X, self.Z)\n",
    "        if self.sizes:\n",
    "            XZ = sum_packed_dim(XZ, self.sizes, dim=0)\n",
    "\n",
    "        # numerically stable calculation of _mu\n",
    "        L, ridge = jitcholesky(ZZ, jitbase=2)\n",
    "        A = torch.cat((XZ, noise * L.t()))\n",
    "        Y = torch.cat((self.Y, torch.zeros(self.Z.size()[0])))\n",
    "        Q, R = torch.qr(A)\n",
    "        self._mu = torch.mv(R.inverse(), torch.mv(Q.t(), Y))\n",
    "\n",
    "        # inducing function values (Z, u)\n",
    "        self.u = self.mean + torch.mv(ZZ, self._mu)\n",
    "\n",
    "        # covariance ------------------------------ TODO: this is slightly ugly!\n",
    "        ZZ_i = torch.mm(L.t().inverse(), L.inverse())\n",
    "        SIGMA = ZZ + torch.mm(XZ.t(), XZ) / noise**2\n",
    "        # SIGMA_i = SIGMA.inverse()\n",
    "        Q, R = torch.qr(SIGMA)\n",
    "        SIGMA_i = torch.mm(R.inverse(), Q.t())\n",
    "        self._sig = SIGMA_i - ZZ_i\n",
    "        # ------------------------------------------------------------------------\n",
    "\n",
    "        self.ready = 1\n",
    "\n",
    "    @staticmethod\n",
    "    def out(x, array=False):\n",
    "        if array:\n",
    "            return x.detach().numpy()\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def predict(self, X, var=True, array=True):\n",
    "        if not hasattr(self, 'ready') or not self.ready:\n",
    "            self.evaluate()\n",
    "        _X = torch.as_tensor(X)\n",
    "        XZ = self.kern.cov_matrix(_X, self.Z)\n",
    "        mu = self.mean + torch.mv(XZ, self._mu)\n",
    "        if var:\n",
    "            sig = torch.ones(_X.size()[0])*self.kern.diag() + \\\n",
    "                torch.mm(XZ, torch.mm(self._sig, XZ.t())).diag()\n",
    "            if (sig < 0).any():\n",
    "                sig = torch.clamp(sig, 0)\n",
    "                warnings.warn(\n",
    "                    'variance clamped! variance is not numerically stable yet!')\n",
    "            return self.out(mu, array=array), self.out(sig, array=array)\n",
    "        else:\n",
    "            return self.out(mu, array=array)\n",
    "\n",
    "\n",
    "class SparseGPR(SGPR):\n",
    "\n",
    "    def __init__(self, X, Y, num_inducing, sizes=None):\n",
    "        Z = Parameter(X[torch.randint(Y.size()[0], (num_inducing,))])\n",
    "        super(SparseGPR, self).__init__(X, Y, Z, sizes=sizes)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        super(SparseGPR, self).extra_repr()\n",
    "        print('\\nSparseGPR:\\nZ:\\n{}\\n'.format(self.Z.data))\n",
    "\n",
    "    def pre_forward(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self):\n",
    "        self.pre_forward()\n",
    "        return super(SparseGPR, self).forward()\n",
    "\n",
    "\n",
    "def train(model, steps=100, optimizer=None, lr=0.1, losses=[]):\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for _ in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.forward()\n",
    "        losses += [loss.data]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('trained for {} staps'.format(steps))\n",
    "    return losses\n",
    "\n",
    "\n",
    "# testing -------------------------------------------------------------------------\n",
    "def test_if_works():\n",
    "    import numpy as np\n",
    "    import pylab as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "    # dummy data\n",
    "    sizes = torch.randint(1, 10, (100,)).tolist()\n",
    "    X = torch.cat([(torch.rand(size, 1)-0.5)*5 for size in sizes])\n",
    "    Y = (X.tanh() * (-X**2).exp()).view(-1) + 1 * 10.\n",
    "\n",
    "    # transorm Y -> YY, trans\n",
    "    YY = sum_packed_dim(Y, sizes)\n",
    "\n",
    "    # define model\n",
    "    #model = SparseGPR(X, Y, 6, None)\n",
    "    model = SparseGPR(X, YY, 6, sizes)\n",
    "\n",
    "    # training\n",
    "    losses = train(model, 60)\n",
    "    losses = train(model, 60, losses=losses)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    ax1.plot(losses)\n",
    "\n",
    "    # predict\n",
    "    with torch.no_grad():\n",
    "        ax2.scatter(X, Y)\n",
    "        Xtest = torch.arange(-3, 3, 0.1, dtype=X.dtype).view(-1, 1)\n",
    "        Ytest, var = model.predict(Xtest, array=False)\n",
    "        x, y, err = Xtest.numpy().reshape(-1), Ytest.numpy(), torch.sqrt(var).numpy()*10\n",
    "        ax2.plot(x, y, color='green')\n",
    "        ax2.fill_between(x, y-err, y+err, alpha=0.2)\n",
    "        Z = model.Z.detach().numpy().reshape(-1)\n",
    "        u = model.u.detach().numpy()\n",
    "        ax2.scatter(Z, u, marker='x', s=200, color='red')\n",
    "\n",
    "        Ytest = model.predict(Xtest, var=False)\n",
    "        assert Ytest.__class__ == np.ndarray\n",
    "\n",
    "    print(model)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    test_if_works()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
