{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" experimental \"\"\"\n",
    "import torch\n",
    "from torch.nn import Module, Parameter\n",
    "from torch.distributions import MultivariateNormal, LowRankMultivariateNormal\n",
    "from theforce.regression.core import LazyWhite\n",
    "from theforce.regression.algebra import jitcholesky\n",
    "\"\"\"\n",
    "NOTES:\n",
    "1. Hessian has to be multiplied by -1 because kernels return \n",
    "derivatives wrt r and not x, x'.\n",
    "2. When Y are grad data, (-1) is multiplied to mu because X\n",
    "will play the role of xx in PosteriorGP.mean. This will also\n",
    "satisfy the -1 needed in PosteriorGP.grad.\n",
    "\n",
    "TODO:\n",
    "1. Make Inducing kernel work with grad data.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ConstMean(Module):\n",
    "\n",
    "    def __init__(self, c=0., requires_grad=True):\n",
    "        super().__init__()\n",
    "        self.c = Parameter(torch.tensor(c), requires_grad=requires_grad)\n",
    "\n",
    "    def forward(self, X, operation='func'):\n",
    "        if operation == 'func':\n",
    "            return torch.ones((X.size(0),)) * self.c\n",
    "        elif operation == 'grad':\n",
    "            return torch.zeros_like(X)\n",
    "        else:\n",
    "            raise NotImplementedError(operation+'is not implemented!')\n",
    "\n",
    "    def extra_repr(self):\n",
    "        print('c: {}'.format(self.c))\n",
    "\n",
    "\n",
    "class Covariance(Module):\n",
    "\n",
    "    def __init__(self, kernels):\n",
    "        super().__init__()\n",
    "        self.kernels = (kernels if hasattr(kernels, '__iter__')\n",
    "                        else (kernels,))\n",
    "        self.params = [par for kern in self.kernels for par in kern.params]\n",
    "\n",
    "    def calculate(self, x=None, xx=None, operation='func'):\n",
    "        return torch.stack([kern(x=x, xx=xx, operation=operation)\n",
    "                            for kern in self.kernels]).sum(dim=0)\n",
    "\n",
    "    def diag(self, x=None):\n",
    "        return torch.stack([kern.diag(x=x) for kern in self.kernels]).sum(dim=0)\n",
    "\n",
    "    def forward(self, x=None, xx=None, operation='func'):\n",
    "        return self.calculate(x=x, xx=xx, operation=operation)\n",
    "\n",
    "\n",
    "class Inducing(Covariance):        # TODO 1.\n",
    "\n",
    "    def __init__(self, kernels, x, num=None, learn=False, signal=5e-2):\n",
    "        super().__init__(kernels)\n",
    "        self.xind = Parameter(x.clone() if num is None else\n",
    "                              x[torch.randint(0, x.size(0), (num,))],\n",
    "                              requires_grad=learn)\n",
    "        self.white = LazyWhite(dim=x.size(0), signal=signal)\n",
    "        self.white._signal.requires_grad = True\n",
    "        self.params += [self.white._signal, self.xind]\n",
    "\n",
    "    def extra_repr(self):\n",
    "        print('num of inducing points: {}'.format(self.xind.size(0)))\n",
    "\n",
    "    def decompose(self, x=None, xx=None):\n",
    "        x_in = x is not None\n",
    "        xx_in = xx is not None\n",
    "        if not x_in and not xx_in:\n",
    "            left = torch.ones(0, self.xind.size(0))\n",
    "            right = left.t()\n",
    "        elif x_in and not xx_in:\n",
    "            left = self.calculate(x, self.xind)\n",
    "            right = left.t()\n",
    "        elif xx_in and not x_in:\n",
    "            right = self.calculate(self.xind, xx)\n",
    "            left = right.t()\n",
    "        elif x_in and xx_in:\n",
    "            left = self.calculate(x, self.xind)\n",
    "            if x.shape == xx.shape and torch.allclose(x, xx):\n",
    "                right = left.t()\n",
    "            else:\n",
    "                right = self.calculate(self.xind, xx)\n",
    "        chol, _ = jitcholesky(self.calculate(self.xind, self.xind))\n",
    "        return left, chol.inverse(), right\n",
    "\n",
    "    def cov_factor(self, x):\n",
    "        L, M, R = self.decompose(x=x)\n",
    "        Q = L @ M.t()\n",
    "        cov_loss = 0.5*(self.diag(x).sum() - torch.einsum('ij,ij', Q, Q)) \\\n",
    "            / self.white.diag()\n",
    "        return Q, self.white.diag(x), cov_loss\n",
    "\n",
    "    def forward(self, x=None, xx=None, operation='placeholder=func'):\n",
    "        L, M, R = self.decompose(x=x, xx=xx)\n",
    "        return L @ M.t() @ M @ R + self.white(x=x, xx=xx)\n",
    "\n",
    "\n",
    "class GaussianProcess(Module):\n",
    "\n",
    "    def __init__(self, mean, cov):\n",
    "        super().__init__()\n",
    "        self.mean = mean\n",
    "        self.cov = cov\n",
    "        self.params = self.cov.params + list(self.mean.parameters())\n",
    "\n",
    "    def forward(self, x, op='func'):\n",
    "        self.covariance_loss = 0\n",
    "        if op == 'func':\n",
    "            if hasattr(self.cov, 'cov_factor'):\n",
    "                Q, diag, self.covariance_loss = self.cov.cov_factor(x)\n",
    "                return LowRankMultivariateNormal(self.mean(x), Q, diag)\n",
    "            else:\n",
    "                return MultivariateNormal(self.mean(x),\n",
    "                                          covariance_matrix=self.cov(x))\n",
    "        elif op == 'grad':\n",
    "            if hasattr(self.cov, 'cov_factor'):\n",
    "                raise NotImplementedError(\n",
    "                    'Inducing kernel is not implemented for grads yet!')\n",
    "            else:\n",
    "                cov = (-1)*self.cov(x, operation='gradgrad').reshape(     # NOTE 1.\n",
    "                    x.numel(), x.numel())\n",
    "                return MultivariateNormal(self.mean(x, operation='grad').reshape(-1),\n",
    "                                          covariance_matrix=cov)\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        if y.dim() == 1:\n",
    "            return -self(x, op='func').log_prob(y) + self.covariance_loss\n",
    "        elif y.shape == x.shape:\n",
    "            return -self(x, op='grad').log_prob(y.reshape(-1)) + self.covariance_loss\n",
    "        else:\n",
    "            raise RuntimeError('Shape of Y is not consistent!')\n",
    "\n",
    "\n",
    "class PosteriorGP(Module):\n",
    "\n",
    "    def __init__(self, gp, X, Y):\n",
    "        super().__init__()\n",
    "        self.x = X\n",
    "        self.gp = gp\n",
    "        if X.shape == Y.shape:\n",
    "            self.Y_type = 'grad'\n",
    "            sign = -1                     # NOTE 2.\n",
    "        else:\n",
    "            self.Y_type = 'func'\n",
    "            sign = 1\n",
    "        p = gp(X, op=self.Y_type)\n",
    "        self.mu = p.precision_matrix @ (Y.reshape(-1)-p.loc) * sign\n",
    "\n",
    "    def mean(self, X):\n",
    "        mean = self.gp.mean(X)\n",
    "        cov = self.gp.cov(X, self.x, operation=self.Y_type)\n",
    "        return mean + cov.reshape(-1, self.mu.size(0)) @ self.mu\n",
    "\n",
    "    def grad(self, X):\n",
    "        mean = self.gp.mean(X, operation='grad').reshape(-1)\n",
    "        op = 'grad'\n",
    "        if self.Y_type == 'grad':\n",
    "            op = 'gradgrad'\n",
    "        cov = self.gp.cov(X, self.x, operation=op)\n",
    "        return mean + cov.reshape(-1, self.mu.size(0)) @ self.mu\n",
    "\n",
    "    def cov(self, X):\n",
    "        raise NotImplementedError('Covariance has not been implemented yet!')\n",
    "\n",
    "    def forward(self, X):\n",
    "        raise NotImplementedError(''.join(('Similar to GaussianProcess class, this should return',\n",
    "                                           'a MultivariateNormal instance which is not implemented yet')))\n",
    "\n",
    "\n",
    "def train_gp(gp, X, Y, steps=100, lr=0.1):\n",
    "    optimizer = torch.optim.Adam(gp.params, lr=lr)\n",
    "    for _ in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        loss = gp.loss(X, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test_basic():\n",
    "    from theforce.regression.core import SquaredExp, LazyWhite\n",
    "    import pylab as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "    # data\n",
    "    n = 100\n",
    "    dim = 1\n",
    "    torch.random.manual_seed(534654647)\n",
    "    X = (torch.rand(n, dim)-0.5)*10\n",
    "    #Y = (-(X**2).sum(dim=-1)).exp()\n",
    "    Y = X.sin().sum(dim=1)\n",
    "\n",
    "    # model\n",
    "    #cov = Covariance((SquaredExp(dim=dim), LazyWhite(dim=dim, signal=1e-2)))\n",
    "    cov = Inducing(SquaredExp(dim=dim), X, 9, learn=True)\n",
    "    gp = GaussianProcess(ConstMean(), cov)\n",
    "    train_gp(gp, X, Y, steps=500, lr=0.1)\n",
    "    gpr = PosteriorGP(gp, X, Y)\n",
    "    with torch.no_grad():\n",
    "        XX = torch.linspace(X.min(), X.max(), 100).view(-1, 1)\n",
    "        f = gpr.mean(XX)\n",
    "    plt.scatter(X, Y)\n",
    "    plt.scatter(XX, f)\n",
    "    if hasattr(cov, 'xind'):\n",
    "        plt.scatter(cov.xind.detach().numpy(),\n",
    "                    gpr.mean(cov.xind).detach().numpy(),\n",
    "                    marker='x', s=200)\n",
    "\n",
    "\n",
    "def test_grad():\n",
    "    from theforce.regression.core import SquaredExp, LazyWhite\n",
    "    n = 77\n",
    "    dim = 1\n",
    "    torch.random.manual_seed(534654647)\n",
    "    X = (torch.rand(n, dim)-0.5)*10\n",
    "    Y = X.sin().sum(dim=1)\n",
    "    dY = X.cos()\n",
    "    Y_data = dY             # use Y or dY\n",
    "\n",
    "    kernels = (SquaredExp(dim=dim), LazyWhite(dim=dim, signal=0.01))\n",
    "\n",
    "    # model\n",
    "    cov = Covariance(kernels)\n",
    "    gp = GaussianProcess(ConstMean(), cov)\n",
    "    train_gp(gp, X, Y_data, steps=100)\n",
    "    gpr = PosteriorGP(gp, X, Y_data)\n",
    "\n",
    "    if 1:\n",
    "        import pylab as plt\n",
    "        %matplotlib inline\n",
    "        with torch.no_grad():\n",
    "            XX = torch.linspace(X.min(), X.max(), 50).view(-1, 1)\n",
    "            f = gpr.mean(XX)\n",
    "            df = gpr.grad(XX)\n",
    "\n",
    "        plt.scatter(X, Y, label='Y')\n",
    "        plt.scatter(X, dY, label='dY')\n",
    "        plt.scatter(XX, f, label='f')\n",
    "        plt.scatter(XX, df, label='df')\n",
    "        if hasattr(cov, 'xind'):\n",
    "            plt.scatter(cov.xind.detach().numpy(),\n",
    "                        gpr.mean(cov.xind).detach().numpy(),\n",
    "                        marker='x', s=200)\n",
    "        plt.legend()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_basic()\n",
    "    test_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
