{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module\n",
    "from torch.distributions import MultivariateNormal, LowRankMultivariateNormal\n",
    "from theforce.regression.core import LazyWhite\n",
    "from theforce.regression.algebra import jitcholesky\n",
    "from theforce.util.util import iterable\n",
    "import copy\n",
    "\n",
    "\n",
    "class EnergyForceKernel(Module):\n",
    "\n",
    "    def __init__(self, similaritykernels):\n",
    "        super().__init__()\n",
    "        self.kernels = iterable(similaritykernels)\n",
    "        self.params = [par for kern in self.kernels for par in kern.params]\n",
    "\n",
    "    def forward(self, first, second=None, cov='energy_energy', inducing=None):\n",
    "        sec = first if second is None else second\n",
    "        if inducing is None:\n",
    "            return getattr(self, cov)(first, sec)\n",
    "        else:\n",
    "            middle = getattr(self, 'energy_energy')(inducing, inducing)\n",
    "            chol, _ = jitcholesky(middle)\n",
    "            invchol = chol.inverse()\n",
    "            lcov, rcov = cov.split('_')\n",
    "            left = getattr(self, lcov+'_energy')(first, inducing) @ invchol.t()\n",
    "            if second is None and rcov == lcov:\n",
    "                right = left.t()\n",
    "            else:\n",
    "                right = invchol @ getattr(self, 'energy_'+rcov)(inducing, sec)\n",
    "            return left, right\n",
    "\n",
    "    def diag(self, data):\n",
    "        return torch.cat([self.energy_energy(sys, sys).view(1) for sys in data] +\n",
    "                         [self.forces_forces(sys, sys).diag() for sys in data])\n",
    "\n",
    "    def energy_energy(self, first, second):\n",
    "        return self.base_kerns(first, second, 'func')\n",
    "\n",
    "    def forces_energy(self, first, second):\n",
    "        return -self.base_kerns(first, second, 'leftgrad')\n",
    "\n",
    "    def energy_forces(self, first, second):\n",
    "        return -self.base_kerns(first, second, 'rightgrad')\n",
    "\n",
    "    def forces_forces(self, first, second):\n",
    "        return self.base_kerns(first, second, 'gradgrad')\n",
    "\n",
    "    def base_kerns(self, first, second, operation):\n",
    "        return torch.stack([kern(first, second, operation)\n",
    "                            for kern in self.kernels]).sum(dim=0)\n",
    "\n",
    "\n",
    "class GaussianProcessPotential(Module):\n",
    "\n",
    "    def __init__(self, kernels):\n",
    "        super().__init__()\n",
    "        self.kern = EnergyForceKernel(kernels)\n",
    "        self.noise = LazyWhite(signal=0.01, requires_grad=True)\n",
    "        self.params = self.kern.params + self.noise.params\n",
    "\n",
    "    def forward(self, data, inducing=None):\n",
    "        if inducing is None:\n",
    "            L = torch.cat([self.kern(data, cov='energy_energy'),\n",
    "                           self.kern(data, cov='forces_energy')], dim=0)\n",
    "            R = torch.cat([self.kern(data, cov='energy_forces'),\n",
    "                           self.kern(data, cov='forces_forces')], dim=0)\n",
    "            return MultivariateNormal(torch.zeros(L.size(0)),\n",
    "                                      covariance_matrix=torch.cat([L, R], dim=1) +\n",
    "                                      torch.eye(L.size(0))*self.noise.diag())\n",
    "        else:\n",
    "            Q = torch.cat([self.kern(data, cov='energy_energy', inducing=inducing)[0],\n",
    "                           self.kern(data, cov='forces_forces', inducing=inducing)[0]], dim=0)\n",
    "            return LowRankMultivariateNormal(torch.zeros(Q.size(0)), Q,\n",
    "                                             torch.ones(Q.size(0))*self.noise.diag())\n",
    "\n",
    "    def Y(self, data):\n",
    "        return torch.cat([torch.tensor([sys.energy for sys in data])] +\n",
    "                         [sys.forces.view(-1) for sys in data])\n",
    "\n",
    "    def loss(self, data, Y=None, inducing=None, logprob_loss=True, cov_loss=False):\n",
    "        p = self(data, inducing=inducing)\n",
    "        if hasattr(p, 'cov_factor'):\n",
    "            if cov_loss:\n",
    "                covariance_loss = 0.5*(self.kern.diag(data).sum() - torch.einsum(\n",
    "                    'ij,ij', p.cov_factor, p.cov_factor))/self.noise.diag()\n",
    "            else:\n",
    "                covariance_loss = 0\n",
    "        else:\n",
    "            covariance_loss = 0\n",
    "        if logprob_loss:\n",
    "            lp_loss = -p.log_prob(self.Y(data) if Y is None else Y)\n",
    "        else:\n",
    "            lp_loss = 0\n",
    "        return lp_loss + covariance_loss\n",
    "\n",
    "\n",
    "class PosteriorPotential(Module):\n",
    "\n",
    "    def __init__(self, gp, data, inducing=None):\n",
    "        super().__init__()\n",
    "        with torch.no_grad():\n",
    "            self.gp = gp\n",
    "            p = gp(data, inducing)\n",
    "            if inducing is None:\n",
    "                self.X = copy.deepcopy(data)\n",
    "                self.mu = p.precision_matrix @ (gp.Y(data)-p.loc)\n",
    "            else:\n",
    "                K = torch.cat([gp.kern(data, inducing, cov='energy_energy'),\n",
    "                               gp.kern(data, inducing, cov='forces_energy')], dim=0)\n",
    "                M = gp.kern.energy_energy(inducing, inducing)\n",
    "                L, _ = jitcholesky(M)\n",
    "                A = torch.cat([K, gp.noise.diag().sqrt()*L.t()], dim=0)\n",
    "                Y = torch.cat([gp.Y(data), torch.zeros(L.size(0))], dim=0)\n",
    "                Q, R = torch.qr(A)\n",
    "                self.mu = R.inverse() @ Q.t() @ Y\n",
    "                self.X = inducing\n",
    "                self.inducing = 1\n",
    "                for sys, e in zip(*[inducing, M @ self.mu]):\n",
    "                    sys.energy = e\n",
    "\n",
    "    def forward(self, test):\n",
    "        with torch.no_grad():\n",
    "            A = self.gp.kern(test, self.X, cov='energy_energy')\n",
    "            B = self.gp.kern(test, self.X, cov='forces_energy')\n",
    "            if not hasattr(self, 'inducing'):\n",
    "                A = torch.cat([A, self.gp.kern(test, self.X, cov='energy_forces')],\n",
    "                              dim=1)\n",
    "                B = torch.cat([B, self.gp.kern(test, self.X, cov='forces_forces')],\n",
    "                              dim=1)\n",
    "\n",
    "            energy = A @ self.mu\n",
    "            forces = B @ self.mu\n",
    "        return energy, forces.view(-1, 3)\n",
    "\n",
    "\n",
    "def train_gpp(gp, X, inducing=None, steps=10, lr=0.1, Y=None, logprob_loss=True, cov_loss=False):\n",
    "    if not logprob_loss and not cov_loss:\n",
    "        raise RuntimeError('both loss terms are ignored!')\n",
    "\n",
    "    if not hasattr(gp, 'optimizer'):\n",
    "        gp.optimizer = torch.optim.Adam([{'params': gp.params}], lr=lr)\n",
    "        if inducing is not None and type(inducing) != list:\n",
    "            gp.optimizer.add_param_group({'params': inducing.params})\n",
    "\n",
    "    for _ in range(steps):\n",
    "        if inducing is not None and type(inducing) != list:\n",
    "            inducing.update_nl_if_requires_grad()\n",
    "        gp.optimizer.zero_grad()\n",
    "        loss = gp.loss(X, Y, inducing, logprob_loss, cov_loss)\n",
    "        loss.backward()\n",
    "        gp.optimizer.step()\n",
    "\n",
    "    report = []\n",
    "    if inducing is None:\n",
    "        report += ['Full']\n",
    "    else:\n",
    "        report += ['Sparse']\n",
    "        if cov_loss:\n",
    "            report += ['Variational-ELBO']\n",
    "            if not logprob_loss:\n",
    "                report += ['~only trace of Gram matrix considered']\n",
    "        else:\n",
    "            report += ['Projected-Process']\n",
    "    report = ' '.join(report)\n",
    "    print('trained for {} steps ({})'.format(steps, report))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
