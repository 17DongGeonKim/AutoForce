{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module\n",
    "from torch.distributions import MultivariateNormal, LowRankMultivariateNormal\n",
    "from theforce.regression.kernel import White\n",
    "from theforce.regression.algebra import jitcholesky, projected_process_auxiliary_matrices_D\n",
    "from theforce.util.util import iterable, mkdir_p, safe_dirname\n",
    "from theforce.optimize.optimizers import ClampedSGD\n",
    "import copy\n",
    "import os\n",
    "import functools\n",
    "\n",
    "\n",
    "class EnergyForceKernel(Module):\n",
    "\n",
    "    def __init__(self, similaritykernels):\n",
    "        super().__init__()\n",
    "        self.kernels = iterable(similaritykernels)\n",
    "        self.params = [par for kern in self.kernels for par in kern.params]\n",
    "\n",
    "    def forward(self, first, second=None, cov='energy_energy', inducing=None):\n",
    "        sec = first if second is None else second\n",
    "        if inducing is None:\n",
    "            return getattr(self, cov)(first, sec)\n",
    "        else:\n",
    "            middle = getattr(self, 'energy_energy')(inducing, inducing)\n",
    "            chol, _ = jitcholesky(middle)\n",
    "            invchol = chol.inverse()\n",
    "            lcov, rcov = cov.split('_')\n",
    "            left = getattr(self, lcov+'_energy')(first, inducing) @ invchol.t()\n",
    "            if second is None and rcov == lcov:\n",
    "                right = left.t()\n",
    "            else:\n",
    "                right = invchol @ getattr(self, 'energy_'+rcov)(inducing, sec)\n",
    "            return left, right\n",
    "\n",
    "    def energy_energy(self, first, second):\n",
    "        return self.base_kerns(first, second, 'func')\n",
    "\n",
    "    def forces_energy(self, first, second):\n",
    "        return -self.base_kerns(first, second, 'leftgrad')\n",
    "\n",
    "    def energy_forces(self, first, second):\n",
    "        return -self.base_kerns(first, second, 'rightgrad')\n",
    "\n",
    "    def forces_forces(self, first, second):\n",
    "        return self.base_kerns(first, second, 'gradgrad')\n",
    "\n",
    "    def base_kerns(self, first, second, operation):\n",
    "        return torch.stack([kern(first, second, operation=operation)\n",
    "                            for kern in self.kernels]).sum(dim=0)\n",
    "\n",
    "    # diagonal elements:\n",
    "    def diag(self, data, operation='energy'):\n",
    "        return getattr(self, operation+'_diag')(data)\n",
    "\n",
    "    def full_diag(self, data):\n",
    "        return self.energy_forces_diag(data)\n",
    "\n",
    "    def energy_forces_diag(self, data):\n",
    "        return torch.cat([self.energy_diag(data), self.forces_diag(data)])\n",
    "\n",
    "    def energy_diag(self, data):\n",
    "        return self.base_kerns_diag(data, 'func')\n",
    "\n",
    "    def forces_diag(self, data):\n",
    "        return self.base_kerns_diag(data, 'gradgrad')\n",
    "\n",
    "    def base_kerns_diag(self, data, operation):\n",
    "        return torch.stack([kern.diag(data, operation=operation)\n",
    "                            for kern in self.kernels]).sum(dim=0)\n",
    "\n",
    "    @property\n",
    "    def method_caching(self):\n",
    "        return [kern.method_caching if hasattr(kern, 'method_caching') else False\n",
    "                for kern in self.kernels]\n",
    "\n",
    "    @method_caching.setter\n",
    "    def method_caching(self, value):\n",
    "        if hasattr(value, '__iter__'):\n",
    "            val = value\n",
    "        else:\n",
    "            val = len(self.kernels)*[value]\n",
    "        for kern, v in zip(*[self.kernels, val]):\n",
    "            kern.method_caching = v\n",
    "\n",
    "    def clear_cached(self):\n",
    "        for kern in self.kernels:\n",
    "            try:\n",
    "                kern.cached.clear()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return '[{}]'.format(', '.join([kern.state for kern in self.kernels]))\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return 'EnergyForceKernel({})'.format(self.state_args)\n",
    "\n",
    "\n",
    "class GaussianProcessPotential(Module):\n",
    "\n",
    "    def __init__(self, kernels, noise=White(signal=0.01, requires_grad=True), parametric=None):\n",
    "        super().__init__()\n",
    "        self.kern = EnergyForceKernel(kernels)\n",
    "        self.noise = noise\n",
    "        for i, kern in enumerate(self.kern.kernels):\n",
    "            kern.name = 'kern_{}'.format(i)\n",
    "        self.parametric = parametric\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        p = self.kern.params + self.noise.params\n",
    "        if self.parametric is not None:\n",
    "            p += self.parametric.unique_params\n",
    "        return p\n",
    "\n",
    "    @property\n",
    "    def requires_grad(self):\n",
    "        return [p.requires_grad for p in self.params]\n",
    "\n",
    "    @requires_grad.setter\n",
    "    def requires_grad(self, value):\n",
    "        for p in self.params:\n",
    "            p.requires_grad = value\n",
    "\n",
    "    def forward(self, data, inducing=None):\n",
    "        if inducing is None:\n",
    "            L = torch.cat([self.kern(data, cov='energy_energy'),\n",
    "                           self.kern(data, cov='forces_energy')], dim=0)\n",
    "            R = torch.cat([self.kern(data, cov='energy_forces'),\n",
    "                           self.kern(data, cov='forces_forces')], dim=0)\n",
    "            return MultivariateNormal(torch.zeros(L.size(0)),\n",
    "                                      covariance_matrix=torch.cat([L, R], dim=1) +\n",
    "                                      torch.eye(L.size(0))*self.noise.signal**2)\n",
    "        else:\n",
    "            Q = torch.cat([self.kern(data, cov='energy_energy', inducing=inducing)[0],\n",
    "                           self.kern(data, cov='forces_forces', inducing=inducing)[0]], dim=0)\n",
    "            return LowRankMultivariateNormal(torch.zeros(Q.size(0)), Q, self.diagonal_ridge(data))\n",
    "\n",
    "    def diagonal_ridge(self, data, operation='full'):\n",
    "        s = self.noise.signal**2\n",
    "        e_diag = torch.tensor(data.natoms, dtype=s.dtype) * s\n",
    "        f_diag = torch.ones(3*sum(data.natoms)) * s\n",
    "        if operation == 'energy':\n",
    "            return e_diag\n",
    "        elif operation == 'forces':\n",
    "            return f_diag\n",
    "        elif operation == 'full':\n",
    "            return torch.cat([e_diag, f_diag])\n",
    "\n",
    "    def mean(self, data, forces=True, cat=True):\n",
    "        if self.parametric is None:\n",
    "            if forces:\n",
    "                if cat:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 0, 0\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            e = [self.parametric(sys, forces=forces) for sys in iterable(data)]\n",
    "            if forces:\n",
    "                e, f = zip(*e)\n",
    "                e = torch.cat([_e.view(-1) for _e in e])\n",
    "                f = torch.cat(f).view(-1)\n",
    "                if cat:\n",
    "                    return torch.cat([e, f])\n",
    "                else:\n",
    "                    return e, f\n",
    "            else:\n",
    "                return torch.cat([_e.view(-1) for _e in e])\n",
    "\n",
    "    def Y(self, data):\n",
    "        y = torch.cat([torch.tensor([sys.target_energy for sys in data])] +\n",
    "                      [sys.target_forces.view(-1) for sys in data])\n",
    "        return y - self.mean(data)\n",
    "\n",
    "    def loss(self, data, Y=None, inducing=None, logprob_loss=True, cov_loss=False):\n",
    "        p = self(data, inducing=inducing)\n",
    "        if hasattr(p, 'cov_factor'):\n",
    "            if cov_loss:\n",
    "                covariance_loss = 0.5 * ((self.kern.diag(data, 'full') - p.cov_factor.pow(2).sum(dim=-1)\n",
    "                                          )/self.diagonal_ridge(data)).sum()\n",
    "            else:\n",
    "                covariance_loss = 0\n",
    "        else:\n",
    "            covariance_loss = 0\n",
    "        if logprob_loss:\n",
    "            lp_loss = -p.log_prob(self.Y(data) if Y is None else Y)\n",
    "        else:\n",
    "            lp_loss = 0\n",
    "        return lp_loss + covariance_loss\n",
    "\n",
    "    @property\n",
    "    def method_caching(self):\n",
    "        return self.kern.method_caching\n",
    "\n",
    "    @method_caching.setter\n",
    "    def method_caching(self, value):\n",
    "        self.kern.method_caching = value\n",
    "\n",
    "    def clear_cached(self, X=None):\n",
    "        if X is None:\n",
    "            self.kern.clear_cached()\n",
    "        else:\n",
    "            for x in iterable(X):\n",
    "                if hasattr(x, 'UID'):\n",
    "                    UID = x.UID()\n",
    "                    for a in self.cached:\n",
    "                        for b in a.values():\n",
    "                            for c in list(b.keys()):\n",
    "                                if UID in c:\n",
    "                                    del b[c]\n",
    "\n",
    "    @property\n",
    "    def cached(self):\n",
    "        return [kern.cached if hasattr(kern, 'cached') else {}\n",
    "                for kern in self.kern.kernels]\n",
    "\n",
    "    @cached.setter\n",
    "    def cached(self, values):\n",
    "        for kern, val in zip(*[self.kern.kernels, value]):\n",
    "            kern.cached = val\n",
    "\n",
    "    def del_cached(self):\n",
    "        for kern in self.kern.kernels:\n",
    "            if hasattr(kern, 'cached'):\n",
    "                del kern.cached\n",
    "\n",
    "    def attach_process_group(self, group=torch.distributed.group.WORLD):\n",
    "        for kern in self.kern.kernels:\n",
    "            kern.process_group = group\n",
    "\n",
    "    def detach_process_group(self):\n",
    "        for kern in self.kern.kernels:\n",
    "            del kern.process_group\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return '{}, noise={}, parametric={}'.format(self.kern.state_args, self.noise.state,\n",
    "                                                    self.parametric)\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return 'GaussianProcessPotential({})'.format(self.state_args)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.state\n",
    "\n",
    "    def to_file(self, file, flag='', mode='a'):\n",
    "        from theforce.util.util import one_liner\n",
    "        with open(file, mode) as f:\n",
    "            f.write('\\n\\n\\n#flag: {}\\n'.format(flag))\n",
    "            f.write(one_liner(self.state))\n",
    "\n",
    "\n",
    "def context_setting(method):\n",
    "\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, *args, use_caching=False, enable_grad=False, **kwargs):\n",
    "        caching_status = self.gp.method_caching\n",
    "        self.gp.method_caching = use_caching\n",
    "        with torch.set_grad_enabled(enable_grad):\n",
    "            result = method(self, *args, **kwargs)\n",
    "        self.gp.method_caching = caching_status\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class PosteriorPotential(Module):\n",
    "\n",
    "    def __init__(self, gp, data, inducing=None, group=None, **setting):\n",
    "        super().__init__()\n",
    "        self.gp = gp\n",
    "        if group is not None:\n",
    "            self.attach_process_group(group)\n",
    "        else:\n",
    "            self.is_distributed = False\n",
    "        self.set_data(data, inducing=inducing, **setting)\n",
    "\n",
    "    @context_setting\n",
    "    def set_data(self, data, inducing=None):\n",
    "        self.data = data\n",
    "        if inducing is None:\n",
    "            raise RuntimeWarning(\n",
    "                'This (inducing=None) has not been used in long while!')\n",
    "            p = self.gp(data, inducing)\n",
    "            self.X = copy.deepcopy(data)  # TODO: consider not copying\n",
    "            self.mu = p.precision_matrix @ (gp.Y(data)-p.loc)\n",
    "            self.nu = p.precision_matrix\n",
    "            self.has_target_forces = True\n",
    "        else:\n",
    "            self.Ke = self.gp.kern(data, inducing, cov='energy_energy')\n",
    "            self.Kf = self.gp.kern(data, inducing, cov='forces_energy')\n",
    "            self.M = self.gp.kern(inducing, inducing, cov='energy_energy')\n",
    "            self.X = inducing\n",
    "            self.make_munu()\n",
    "            self.has_target_forces = False\n",
    "\n",
    "    @property\n",
    "    def K(self):\n",
    "        return torch.cat([self.Ke, self.Kf], dim=0)\n",
    "\n",
    "    def make_munu(self):\n",
    "        self.mu, self.nu, self.ridge, self.choli = projected_process_auxiliary_matrices_D(\n",
    "            self.K, self.M, self.gp.Y(self.data), self.gp.diagonal_ridge(self.data), chol_inverse=True)\n",
    "\n",
    "    @property\n",
    "    def ref_M(self):\n",
    "        return self.M + self.ridge*torch.eye(self.M.size(0))\n",
    "\n",
    "    @context_setting\n",
    "    def leakage(self, loc):\n",
    "        a = self.gp.kern(self.X, loc, cov='energy_energy')\n",
    "        b = self.choli @ a\n",
    "        c = b.t()@b\n",
    "        d = self.gp.kern(loc, loc, cov='energy_energy') + self.ridge\n",
    "        return (1-c/d).view(1)\n",
    "\n",
    "    def leakages(self, X):\n",
    "        return torch.cat([self.leakage(x) for x in iterable(X)])\n",
    "\n",
    "    @context_setting\n",
    "    def remake_all(self):\n",
    "        self.set_data(self.data, self.X)\n",
    "\n",
    "    @context_setting\n",
    "    def add_data(self, data, remake=True):\n",
    "        Ke = self.gp.kern(data, self.X, cov='energy_energy')\n",
    "        Kf = self.gp.kern(data, self.X, cov='forces_energy')\n",
    "        self.Ke = torch.cat([self.Ke, Ke], dim=0)\n",
    "        self.Kf = torch.cat([self.Kf, Kf], dim=0)\n",
    "        self.data += data\n",
    "        if remake:\n",
    "            self.make_munu()\n",
    "\n",
    "    @context_setting\n",
    "    def add_inducing(self, X, remake=True):\n",
    "        Ke = self.gp.kern(self.data, X, cov='energy_energy')\n",
    "        Kf = self.gp.kern(self.data, X, cov='forces_energy')\n",
    "        self.Ke = torch.cat([self.Ke, Ke], dim=1)\n",
    "        self.Kf = torch.cat([self.Kf, Kf], dim=1)\n",
    "        a = self.gp.kern(self.X, X, cov='energy_energy')\n",
    "        b = self.gp.kern(X, X, cov='energy_energy')\n",
    "        self.M = torch.cat(\n",
    "            [torch.cat([self.M, a.t()]), torch.cat([a, b])], dim=1)\n",
    "        self.X += X\n",
    "        if remake:\n",
    "            self.make_munu()\n",
    "\n",
    "    def pop_1data(self, remake=True, clear_cached=True):\n",
    "        self.Ke = self.Ke[:-1]\n",
    "        self.Kf = self.Kf[:-3*self.data[-1].natoms]\n",
    "        if clear_cached:\n",
    "            self.gp.clear_cached([self.data.X[-1]])\n",
    "        del self.data.X[-1]\n",
    "        if remake:\n",
    "            self.make_munu()\n",
    "\n",
    "    def pop_1inducing(self, remake=True, clear_cached=True):\n",
    "        self.Ke = self.Ke[:, :-1]\n",
    "        self.Kf = self.Kf[:, :-1]\n",
    "        self.M = self.M[:-1, :-1]\n",
    "        if clear_cached:\n",
    "            self.gp.clear_cached([self.X.X[-1]])\n",
    "        del self.X.X[-1]\n",
    "        if remake:\n",
    "            self.make_munu()\n",
    "\n",
    "    def downsize(self, n, m):\n",
    "        while len(self.data) > n:\n",
    "            self.pop_1data()\n",
    "        while len(self.X) > m:\n",
    "            self.pop_1inducing()\n",
    "\n",
    "    def add_1atoms(self, atoms, ediff, fdiff):\n",
    "        kwargs = {'use_caching': True}\n",
    "        e1 = self([atoms], **kwargs)\n",
    "        if fdiff < float('inf'):\n",
    "            f1 = self([atoms], 'forces', **kwargs)\n",
    "        self.add_data([atoms], **kwargs)\n",
    "        e2 = self([atoms], **kwargs)\n",
    "        de = abs(e1-e2)\n",
    "        if fdiff < float('inf'):\n",
    "            f2 = self([atoms], 'forces', **kwargs)\n",
    "            df = (f2-f1).abs().max()\n",
    "        else:\n",
    "            df = 0\n",
    "        blind = torch.cat([e1, e2]).allclose(torch.zeros(1))\n",
    "        if de < ediff and df < fdiff and not blind:\n",
    "            self.pop_1data(clear_cached=True)\n",
    "        return de, df\n",
    "\n",
    "    def add_1inducing(self, _loc, ediff, detach=True):\n",
    "        kwargs = {'use_caching': True}\n",
    "        if detach:\n",
    "            loc = _loc.detach()\n",
    "            loc.stage(self.gp.kern.kernels, dont_save_grads=True)\n",
    "        else:\n",
    "            loc = _loc\n",
    "        e1 = self(loc, **kwargs)\n",
    "        self.add_inducing(loc, **kwargs)\n",
    "        e2 = self(loc, **kwargs)\n",
    "        de = abs(e1-e2)\n",
    "        blind = torch.cat([e1, e2]).allclose(torch.zeros(1))\n",
    "        if de < ediff and not blind:\n",
    "            self.pop_1inducing(clear_cached=True)\n",
    "        return de\n",
    "\n",
    "    def select_inducing(self, indices, deleted=None, remake=True):\n",
    "        i = torch.as_tensor(indices)\n",
    "        self.Ke = self.Ke.index_select(1, i)\n",
    "        self.Kf = self.Kf.index_select(1, i)\n",
    "        self.M = self.M.index_select(0, i).index_select(1, i)\n",
    "        if deleted:\n",
    "            self.gp.clear_cached([self.X.X[j] for j in deleted])\n",
    "        self.X.X = [self.X.X[j] for j in i]\n",
    "        if remake:\n",
    "            self.make_munu()\n",
    "\n",
    "    def attach_process_group(self, *args, **kwargs):\n",
    "        self.gp.attach_process_group(*args, **kwargs)\n",
    "        self.is_distributed = True\n",
    "\n",
    "    def detach_process_group(self, *args, **kwargs):\n",
    "        self.gp.detach_process_group(*args, **kwargs)\n",
    "        self.is_distributed = False\n",
    "\n",
    "    def train(self, *args, **kwargs):\n",
    "        train_gpp(self.gp, *args, **kwargs)\n",
    "\n",
    "    def save(self, file, supress_warnings=True):\n",
    "        import warnings\n",
    "        cached = self.gp.cached\n",
    "        self.gp.del_cached()\n",
    "        data = self.data\n",
    "        del self.data\n",
    "        with warnings.catch_warnings():\n",
    "            if supress_warnings:\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "            torch.save(self, file)\n",
    "        self.data = data\n",
    "        self.gp.cahced = cached\n",
    "\n",
    "    def to_folder(self, folder, info=None, overwrite=True, supress_warnings=True):\n",
    "        if not overwrite:\n",
    "            folder = safe_dirname(folder)\n",
    "        mkdir_p(folder)\n",
    "        with open(os.path.join(folder, 'cutoff'), 'w') as file:\n",
    "            try:\n",
    "                file.write('{}\\n'.format(self._cutoff))\n",
    "            except:\n",
    "                file.write('{}\\n'.format(self.data[0].cutoff))\n",
    "        self.data.to_traj(os.path.join(folder, 'data.traj'))\n",
    "        self.X.to_traj(os.path.join(folder, 'inducing.traj'))\n",
    "        self.gp.to_file(os.path.join(folder, 'gp'))\n",
    "        self.save(os.path.join(folder, 'model'),\n",
    "                  supress_warnings=supress_warnings)\n",
    "        # info\n",
    "        with open(os.path.join(folder, 'info'), 'w') as file:\n",
    "            file.write('data: {}, inducing: {}\\n'.format(\n",
    "                len(self.data), len(self.X)))\n",
    "            if info is not None:\n",
    "                if type(info) == str:\n",
    "                    file.write('{}\\n'.format(info))\n",
    "                elif hasattr(info, '__iter__'):\n",
    "                    for inf in info:\n",
    "                        file.write('{}\\n'.format(inf))\n",
    "                else:\n",
    "                    file.write('{}\\n'.format(info))\n",
    "\n",
    "    @context_setting\n",
    "    def forward(self, test, quant='energy', variance=False, all_reduce=False):\n",
    "        shape = {'energy': (-1,), 'forces': (-1, 3)}\n",
    "        A = self.gp.kern(test, self.X, cov=quant+'_energy')\n",
    "        if self.has_target_forces:\n",
    "            A = torch.cat([A, self.gp.kern(test, self.X, cov=quant+'_forces')],\n",
    "                          dim=1)\n",
    "        if quant == 'energy':\n",
    "            mean = self.gp.mean(test, forces=False)\n",
    "        else:\n",
    "            _, mean = self.gp.mean(test, forces=True, cat=False)\n",
    "        out = (mean + A @ self.mu).view(*shape[quant])\n",
    "        if all_reduce:\n",
    "            torch.distributed.all_reduce(out)\n",
    "        if variance:\n",
    "            if all_reduce:\n",
    "                raise NotImplementedError(\n",
    "                    'all_reduce with variance=True is not implemented')\n",
    "            var = (self.gp.kern.diag(test, quant) -\n",
    "                   (A @ self.nu @ A.t()).diag()).view(*shape[quant])\n",
    "            return out, var\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    @context_setting\n",
    "    def predict(self, test, variance=False):\n",
    "        if self.gp.parametric is not None:\n",
    "            raise NotImplementedError(\n",
    "                'this method is not updated to include parametric potential')\n",
    "        A = self.gp.kern(test, self.X, cov='energy_energy')\n",
    "        B = self.gp.kern(test, self.X, cov='forces_energy')\n",
    "        if self.has_target_forces:\n",
    "            A = torch.cat([A, self.gp.kern(test, self.X, cov='energy_forces')],\n",
    "                          dim=1)\n",
    "            B = torch.cat([B, self.gp.kern(test, self.X, cov='forces_forces')],\n",
    "                          dim=1)\n",
    "\n",
    "        energy = A @ self.mu\n",
    "        forces = B @ self.mu\n",
    "\n",
    "        out = (energy, forces.view(-1, 3))\n",
    "\n",
    "        if variance:\n",
    "            energy_var = (self.gp.kern.diag(test, 'energy') -\n",
    "                          (A @ self.nu @ A.t()).diag())\n",
    "            forces_var = (self.gp.kern.diag(test, 'forces') -\n",
    "                          (B @ self.nu @ B.t()).diag())\n",
    "            out += (energy_var, forces_var.view(-1, 3))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def PosteriorPotentialFromFolder(folder, load_data=True, update_data=True):\n",
    "    from theforce.descriptor.atoms import AtomsData\n",
    "    from theforce.util.caching import strip_uid\n",
    "    self = torch.load(os.path.join(folder, 'model'))\n",
    "    strip_uid(self.X)\n",
    "    with open(os.path.join(folder, 'cutoff'), 'r') as file:\n",
    "        cutoff = float(file.readline().split()[0])\n",
    "        self._cutoff = cutoff\n",
    "    if load_data:\n",
    "        self.data = AtomsData(traj=os.path.join(folder, 'data.traj'))\n",
    "        if update_data:\n",
    "            self.data.update(cutoff=cutoff, descriptors=self.gp.kern.kernels)\n",
    "    return self\n",
    "\n",
    "\n",
    "def train_gpp(gp, X, inducing=None, steps=10, lr=0.1, Y=None, logprob_loss=True, cov_loss=False,\n",
    "              move=0.1, shake=0):\n",
    "    if not logprob_loss and not cov_loss:\n",
    "        raise RuntimeError('both loss terms are ignored!')\n",
    "\n",
    "    if not hasattr(gp, 'optimizer'):\n",
    "        gp.optimizer = torch.optim.Adam([{'params': gp.params}], lr=lr)\n",
    "        if inducing is not None and inducing.trainable and not hasattr(inducing, 'optimizer'):\n",
    "            inducing.optimizer = ClampedSGD(\n",
    "                [{'params': inducing.params}], lr=move)\n",
    "\n",
    "    caching_status = gp.method_caching\n",
    "    gp.method_caching = False\n",
    "    gp.clear_cached()\n",
    "\n",
    "    for _ in range(steps):\n",
    "        if inducing is not None and inducing.trainable:\n",
    "            if shake > 0:\n",
    "                inducing.shake(update=False)\n",
    "            inducing.optimizer.zero_grad()\n",
    "            inducing.update_nl_if_requires_grad(descriptors=gp.kern.kernels)\n",
    "        gp.optimizer.zero_grad()\n",
    "        loss = gp.loss(X, Y, inducing, logprob_loss, cov_loss)\n",
    "        loss.backward()\n",
    "        gp.optimizer.step()\n",
    "        if inducing is not None and inducing.trainable:\n",
    "            inducing.optimizer.step()\n",
    "\n",
    "    gp.method_caching = caching_status\n",
    "\n",
    "    report = []\n",
    "    if inducing is None:\n",
    "        report += ['Full']\n",
    "    else:\n",
    "        report += ['Sparse']\n",
    "        if cov_loss:\n",
    "            report += ['Variational-ELBO']\n",
    "            if not logprob_loss:\n",
    "                report += ['~only trace of Gram matrix considered']\n",
    "        else:\n",
    "            report += ['Projected-Process']\n",
    "    report = ' '.join(report)\n",
    "    print('trained for {} steps ({})'.format(steps, report))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
