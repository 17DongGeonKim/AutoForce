{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theforce.util.util import iterable\n",
    "import torch\n",
    "from theforce.math.ylm import Ylm\n",
    "from torch.nn import Module, Parameter\n",
    "from math import factorial as fac\n",
    "from theforce.regression.algebra import positive, free_form\n",
    "from theforce.math.func import I, Exp\n",
    "\n",
    "\n",
    "class HeteroSoap(Module):\n",
    "\n",
    "    def __init__(self, lmax, nmax, radial, numbers, unit=None):\n",
    "        super().__init__()\n",
    "        self.ylm = Ylm(lmax)\n",
    "        self.nmax = nmax\n",
    "\n",
    "        self._radial = radial\n",
    "        if unit:\n",
    "            self.unit = unit\n",
    "        else:\n",
    "            self.unit = radial.rc/3\n",
    "        self.radial = Exp(-0.5*I()**2/self.unit**2)*radial\n",
    "        self.numbers = sorted(iterable(numbers))\n",
    "\n",
    "        one = torch.ones(lmax+1, lmax+1)\n",
    "        self.Yr = 2*torch.torch.tril(one) - torch.eye(lmax+1)\n",
    "        self.Yi = 2*torch.torch.triu(one, diagonal=1)\n",
    "\n",
    "        a = torch.tensor([[1./((2*l+1)*2**(2*n+l)*fac(n)*fac(n+l))\n",
    "                           for l in range(lmax+1)] for n in range(nmax+1)])\n",
    "        self.nnl = (a[None]*a[:, None]).sqrt()\n",
    "\n",
    "        #m = torch.arange(len(self.numbers))\n",
    "        #n = torch.arange(self.nmax+1)\n",
    "        #i = torch.ones(m.size(0), m.size(0), n.size(0), n.size(0))\n",
    "        # self.mask = (i * (m[:, None] >= m[None]).to(torch.int)[..., None, None] *\n",
    "        #             (n[:, None] >= n[None]).to(torch.int)).to(torch.bool)\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return \"{}, {}, {}, {}, unit={}\".format(self.ylm.lmax, self.nmax, self._radial.state,\n",
    "                                                self.numbers, self.unit)\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.__class__.__name__+'({})'.format(self.state_args)\n",
    "\n",
    "    def forward(self, coo, numbers, grad=True):\n",
    "        xyz = coo/self.unit\n",
    "        d = xyz.pow(2).sum(dim=-1).sqrt()\n",
    "        n = 2*torch.arange(self.nmax+1).type(xyz.type())\n",
    "        r, dr = self.radial(self.unit*d)\n",
    "        dr = self.unit*dr\n",
    "        f = (r*d[None]**n[:, None])\n",
    "        Y = self.ylm(xyz, grad=grad)\n",
    "        if grad:\n",
    "            Y, dY = Y\n",
    "        ff = f[:, None, None]*Y[None]\n",
    "        i = torch.arange(r.size(0))\n",
    "        c = []\n",
    "        for num in self.numbers:\n",
    "            t = torch.index_select(ff, -1, i[numbers == num])\n",
    "            c += [t.sum(dim=-1)]\n",
    "        c = torch.stack(c)\n",
    "        nnp = c[None, :, None, ]*c[:, None, :, None]\n",
    "        p = (nnp*self.Yr).sum(dim=-1) + (nnp*self.Yi).sum(dim=-2)\n",
    "        if grad:\n",
    "            df = dr*d[None]**n[:, None] + r*n[:, None]*d[None]**(n[:, None]-1)\n",
    "            df = df[..., None]*xyz/d[:, None]\n",
    "            dc = (df[:, None, None]*Y[None, ..., None] +\n",
    "                  f[:, None, None, :, None]*dY[None])\n",
    "            dc = torch.stack([(numbers == num).type(r.type())[:, None] * dc\n",
    "                              for num in self.numbers])\n",
    "            dnnp = (c[None, :, None, ..., None, None]*dc[:, None, :, None] +\n",
    "                    dc[None, :, None, ]*c[:, None, :, None, ..., None, None])\n",
    "            dp = ((dnnp*self.Yr[..., None, None]).sum(dim=-3) +\n",
    "                  (dnnp*self.Yi[..., None, None]).sum(dim=-4))\n",
    "            p, dp = p*self.nnl, dp*self.nnl[..., None, None]/self.unit\n",
    "            return p, dp\n",
    "        else:\n",
    "            p = p*self.nnl\n",
    "            return p\n",
    "\n",
    "\n",
    "class AbsSeriesSoap(Module):\n",
    "\n",
    "    def __init__(self, lmax, nmax, radial, unit=None):\n",
    "        super().__init__()\n",
    "        self.ylm = Ylm(lmax)\n",
    "        self.nmax = nmax\n",
    "        self.radial = radial\n",
    "        if unit:\n",
    "            self.unit = unit\n",
    "        else:\n",
    "            self.unit = radial.rc/3\n",
    "        one = torch.ones(lmax+1, lmax+1)\n",
    "        self.Yr = 2*torch.torch.tril(one) - torch.eye(lmax+1)\n",
    "        self.Yi = 2*torch.torch.triu(one, diagonal=1)\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return \"{}, {}, {}, unit={}\".format(self.ylm.lmax, self.nmax, self.radial.state, self.unit)\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.__class__.__name__+'({})'.format(self.state_args)\n",
    "\n",
    "    def forward(self, coo, grad=True):\n",
    "        xyz = coo/self.unit\n",
    "        d = xyz.pow(2).sum(dim=-1).sqrt()\n",
    "        n = 2*torch.arange(self.nmax+1).type(xyz.type())\n",
    "        r, dr = self.radial(self.unit*d)\n",
    "        dr = self.unit*dr\n",
    "        f = (r*d[None]**n[:, None])\n",
    "        Y = self.ylm(xyz, grad=grad)\n",
    "        if grad:\n",
    "            Y, dY = Y\n",
    "        c = (f[:, None, None]*Y[None]).sum(dim=-1)\n",
    "        nnp = c[None, ]*c[:, None]\n",
    "        p = (nnp*self.Yr).sum(dim=-1) + (nnp*self.Yi).sum(dim=-2)\n",
    "        if grad:\n",
    "            df = dr*d[None]**n[:, None] + r*n[:, None]*d[None]**(n[:, None]-1)\n",
    "            df = df[..., None]*xyz/d[:, None]\n",
    "            dc = (df[:, None, None]*Y[None, ..., None] +\n",
    "                  f[:, None, None, :, None]*dY[None])\n",
    "            dnnp = (c[None, ..., None, None]*dc[:, None] +\n",
    "                    dc[None, ]*c[:, None, ..., None, None])\n",
    "            dp = ((dnnp*self.Yr[..., None, None]).sum(dim=-3) +\n",
    "                  (dnnp*self.Yi[..., None, None]).sum(dim=-4))\n",
    "            return p, dp/self.unit\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "\n",
    "class RealSeriesSoap(Module):\n",
    "\n",
    "    def __init__(self, lmax, nmax, radial, atomic_unit=None):\n",
    "        \"\"\"radial: usually a cutoff function, should have an rc attr.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.radial = radial\n",
    "        if atomic_unit is None:\n",
    "            atomic_unit = radial.rc/3\n",
    "        R = Exp(-0.5*I()**2/atomic_unit**2)*radial\n",
    "        self.abs = AbsSeriesSoap(lmax, nmax, R, unit=atomic_unit)\n",
    "\n",
    "        a = torch.tensor([[1./((2*l+1)*2**(2*n+l)*fac(n)*fac(n+l))\n",
    "                           for l in range(lmax+1)] for n in range(nmax+1)])\n",
    "        self.nnl = (a[None]*a[:, None]).sqrt()\n",
    "\n",
    "    def forward(self, xyz, grad=True):\n",
    "        p = self.abs(xyz, grad=grad)\n",
    "        if grad:\n",
    "            p, q = p\n",
    "            q = q*self.nnl[..., None, None]\n",
    "        p = p*self.nnl\n",
    "\n",
    "        if grad:\n",
    "            return p, q\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return \"{}, {}, {}, atomic_unit={}\".format(self.abs.ylm.lmax, self.abs.nmax,\n",
    "                                                   self.radial.state, self.abs.unit)\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.__class__.__name__+'({})'.format(self.state_args)\n",
    "\n",
    "\n",
    "class TailoredSoap(Module):\n",
    "\n",
    "    def __init__(self, soap, corners=0, symm=False):\n",
    "        super().__init__()\n",
    "        self.soap = soap\n",
    "        n = torch.arange(soap.abs.nmax+1)\n",
    "        self.mask = ((n[:, None]-n[None]).abs() <=\n",
    "                     soap.abs.nmax-corners)\n",
    "\n",
    "        if not symm:\n",
    "            self.mask = (self.mask & (n[:, None] >= n[None]))\n",
    "\n",
    "        self._state_args = \"corners={}, symm={}\".format(corners, symm)\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, xyz, grad=True):\n",
    "        p = self.soap(xyz, grad=grad)\n",
    "        if grad:\n",
    "            p, q = p\n",
    "\n",
    "        p = p[self.mask].view(-1)\n",
    "        if grad:\n",
    "            q = q[self.mask].view(p.size(0), *xyz.size())\n",
    "\n",
    "        if grad:\n",
    "            return p, q\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.mask.sum()*(self.soap.abs.ylm.lmax+1)\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return \"{}, {}\".format(self.soap.state, self._state_args)\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.__class__.__name__+'({})'.format(self.state_args)\n",
    "\n",
    "\n",
    "class MultiSoap(Module):\n",
    "    def __init__(self, soaps):\n",
    "        super().__init__()\n",
    "        self.soaps = iterable(soaps)\n",
    "        self.params = [par for soap in self.soaps for par in soap.params]\n",
    "\n",
    "    def forward(self, xyz, masks, grad=True):\n",
    "        p = [soap(xyz[m], grad=grad) for soap, m in zip(*[self.soaps, masks])]\n",
    "        if grad:\n",
    "            p, _q = zip(*p)\n",
    "            n = xyz.size(0)\n",
    "            i = torch.arange(n).long()\n",
    "            q = torch.cat([torch.zeros(soap.dim, n, 3).index_add(1, i[m], qq)\n",
    "                           for soap, m, qq in zip(*[self.soaps, masks, _q])])\n",
    "        p = torch.cat(p)\n",
    "\n",
    "        if grad:\n",
    "            return p, q\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return sum([soap.dim for soap in self.soaps])\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return \"[\" + \", \".join(\"{}\".format(soap.state) for soap in self.soaps) + \"]\"\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.__class__.__name__+'({})'.format(self.state_args)\n",
    "\n",
    "\n",
    "class ScaledSoap(Module):\n",
    "\n",
    "    def __init__(self, soap, scales=None):\n",
    "        super().__init__()\n",
    "        self.soap = soap\n",
    "        self.params = [par for par in soap.params]\n",
    "        self.scales = scales\n",
    "\n",
    "    @property\n",
    "    def scales(self):\n",
    "        return positive(self._scales)\n",
    "\n",
    "    @scales.setter\n",
    "    def scales(self, value):\n",
    "        if value is None:\n",
    "            v = torch.ones(self.soap.dim)\n",
    "        else:\n",
    "            v = torch.as_tensor(value).view(-1)\n",
    "        assert (v > 0).all()\n",
    "        self._scales = Parameter(free_form(v))\n",
    "        self.params.append(self._scales)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        if 'grad' in kwargs:\n",
    "            grad = kwargs['grad']\n",
    "        else:\n",
    "            grad = True\n",
    "\n",
    "        p = self.soap(*args, **kwargs)\n",
    "        if grad:\n",
    "            p, q = p\n",
    "            q = q/self.scales[..., None, None]\n",
    "        p = p/self.scales\n",
    "\n",
    "        if grad:\n",
    "            return p, q\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.soap.dim\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return \"{}, scales={}\".format(self.soap.state, self.scales.data)\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.__class__.__name__+'({})'.format(self.state_args)\n",
    "\n",
    "\n",
    "class NormalizedSoap(Module):\n",
    "\n",
    "    def __init__(self, soap):\n",
    "        super().__init__()\n",
    "        self.soap = soap\n",
    "        self.params = [par for par in soap.params]\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        if 'grad' in kwargs:\n",
    "            grad = kwargs['grad']\n",
    "        else:\n",
    "            grad = True\n",
    "        p = self.soap(*args, **kwargs)\n",
    "        if grad:\n",
    "            p, q = p\n",
    "\n",
    "        norm = p.norm()\n",
    "        if norm > 0.0:\n",
    "            p = p/norm\n",
    "            if grad:\n",
    "                q = q/norm\n",
    "                q = q - p[..., None, None] * (p[..., None, None] * q\n",
    "                                              ).sum(dim=(0))\n",
    "        if grad:\n",
    "            return p, q\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.soap.dim\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return \"{}\".format(self.soap.state)\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.__class__.__name__+'({})'.format(self.state_args)\n",
    "\n",
    "\n",
    "class SeriesSoap(Module):\n",
    "    \"\"\"deprecated\"\"\"\n",
    "\n",
    "    def __init__(self, lmax, nmax, radial, unit=None, modify=None, normalize=False,\n",
    "                 cutcorners=0, symm=False):\n",
    "        super().__init__()\n",
    "        self.abs = AbsSeriesSoap(lmax, nmax, radial, unit=unit)\n",
    "\n",
    "        if modify:\n",
    "            a = torch.tensor([[modify**(2*n+l)/((2*l+1)*2**(2*n+l)*fac(n)*fac(n+l))\n",
    "                               for l in range(lmax+1)] for n in range(nmax+1)])\n",
    "            self.nnl = (a[None]*a[:, None]).sqrt()\n",
    "        else:\n",
    "            self.nnl = torch.ones(nmax+1, nmax+1, lmax+1)\n",
    "\n",
    "        n = torch.arange(nmax+1)\n",
    "        self.mask = ((n[:, None]-n[None]).abs() <= nmax-cutcorners).byte()\n",
    "        if not symm:\n",
    "            self.mask = (self.mask & (n[:, None] >= n[None]).byte())\n",
    "\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.kwargs = 'modify={}, normalize={}, cutcorners={}, symm={}'.format(\n",
    "            modify, normalize, cutcorners, symm)\n",
    "\n",
    "        import warnings\n",
    "        warnings.warn(\"class {} is Deprecated\".format(self.__class__.__name__))\n",
    "\n",
    "    def forward(self, xyz, grad=True):\n",
    "        p = self.abs(xyz, grad=grad)\n",
    "        if grad:\n",
    "            p, q = p\n",
    "            q = q*self.nnl[..., None, None]\n",
    "        p = p*self.nnl\n",
    "\n",
    "        p = p[self.mask].view(-1)\n",
    "        if grad:\n",
    "            q = q[self.mask].view(p.size(0), *xyz.size())\n",
    "\n",
    "        if self.normalize:\n",
    "            norm = p.norm()\n",
    "            if norm > 0.0:\n",
    "                p = p/norm\n",
    "                if grad:\n",
    "                    q = q/norm\n",
    "                    q = q - p[..., None, None] * (p[..., None, None] * q\n",
    "                                                  ).sum(dim=(0))\n",
    "        if grad:\n",
    "            return p, q\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.mask.sum()*(self.abs.ylm.lmax+1)\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return \"{}, {}\".format(self.abs.state_args, self.kwargs)\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.__class__.__name__+'({})'.format(self.state_args)\n",
    "\n",
    "\n",
    "def test_validity():\n",
    "    import torch\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "\n",
    "    xyz = torch.tensor([[0.175, 0.884, -0.87, 0.354, -0.082, 3.1],\n",
    "                        [-0.791, 0.116, 0.19, -0.832, 0.184, 0.],\n",
    "                        [0.387, 0.761, 0.655, -0.528, 0.973, 0.]]).t()\n",
    "    xyz.requires_grad = True\n",
    "\n",
    "    target = torch.tensor([[[0.36174603, 0.39013356, 0.43448023],\n",
    "                            [0.39013356, 0.42074877, 0.46857549],\n",
    "                            [0.43448023, 0.46857549, 0.5218387]],\n",
    "\n",
    "                           [[0.2906253, 0.30558356, 0.33600938],\n",
    "                            [0.30558356, 0.3246583, 0.36077952],\n",
    "                            [0.33600938, 0.36077952, 0.40524778]],\n",
    "\n",
    "                           [[0.16241845, 0.18307552, 0.20443194],\n",
    "                            [0.18307552, 0.22340802, 0.26811937],\n",
    "                            [0.20443194, 0.26811937, 0.34109511]]])\n",
    "\n",
    "    s = AbsSeriesSoap(2, 2, PolyCut(3.0))\n",
    "    p, dp = s(xyz)\n",
    "    p = p.permute(2, 0, 1)\n",
    "    print('fits pre-calculated values: {}'.format(p.allclose(target)))\n",
    "\n",
    "    p.sum().backward()\n",
    "    print('fits gradients calculated by autograd: {}'.format(\n",
    "        xyz.grad.allclose(dp.sum(dim=(0, 1, 2)))))\n",
    "\n",
    "    # test with normalization turned on\n",
    "    s = SeriesSoap(3, 7, PolyCut(3.0), normalize=True)\n",
    "    xyz.grad *= 0\n",
    "    p, dp = s(xyz)\n",
    "    p.sum().backward()\n",
    "    print('fits gradients calculated by autograd (normalize=True):{}'.format(\n",
    "        xyz.grad.allclose(dp.sum(dim=(0)))))\n",
    "\n",
    "    assert s.state == eval(s.state).state\n",
    "\n",
    "    # test if works with empty tensors\n",
    "    s(torch.rand(0, 3))\n",
    "\n",
    "\n",
    "def test_units():\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "    xyz = torch.tensor([[0.175, 0.884, -0.87, 0.354, -0.082, 3.1],\n",
    "                        [-0.791, 0.116, 0.19, -0.832, 0.184, 0.],\n",
    "                        [0.387, 0.761, 0.655, -0.528, 0.973, 0.]]).t()\n",
    "    xyz = xyz*3\n",
    "    cutoff = 3.0*3\n",
    "    xyz.requires_grad = True\n",
    "\n",
    "    s = SeriesSoap(3, 3, PolyCut(cutoff), normalize=True)\n",
    "    p, dp = s(xyz)\n",
    "    p.sum().backward()\n",
    "    print('grads are consistent with larger length scale: {}'.format(\n",
    "        xyz.grad.allclose(dp.sum(dim=(0)))))\n",
    "\n",
    "\n",
    "def test_speed(N=100):\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "    import time\n",
    "    s = AbsSeriesSoap(5, 5, PolyCut(3.0))\n",
    "    start = time.time()\n",
    "    for _ in range(N):\n",
    "        xyz = torch.rand(30, 3)\n",
    "        p = s(xyz)\n",
    "    finish = time.time()\n",
    "    delta = (finish-start)/N\n",
    "    print(\"speed of {}: {} sec\".format(s.state, delta))\n",
    "\n",
    "\n",
    "def example():\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "\n",
    "    lengthscale = 2.\n",
    "    cutoff = 8.\n",
    "    xyz = torch.tensor([[1., 0, 0], [-1., 0, 0],\n",
    "                        [0, 1., 0], [0, -1., 0],\n",
    "                        [0, 0, 1.], [0, 0, -1.]]) * lengthscale\n",
    "    xyz.requires_grad = True\n",
    "    s = SeriesSoap(2, 2, PolyCut(cutoff), normalize=True)\n",
    "    p, dp = s(xyz)\n",
    "    print(p)\n",
    "\n",
    "\n",
    "def test_realseriessoap():\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "    xyz = torch.tensor([[0.175, 0.884, -0.87, 0.354, -0.082, 3.1],\n",
    "                        [-0.791, 0.116, 0.19, -0.832, 0.184, 0.],\n",
    "                        [0.387, 0.761, 0.655, -0.528, 0.973, 0.]]).t()\n",
    "    xyz = xyz*3\n",
    "    cutoff = 3.0*3\n",
    "    xyz.requires_grad = True\n",
    "\n",
    "    s = NormalizedSoap(TailoredSoap(RealSeriesSoap(2, 2, PolyCut(cutoff),\n",
    "                                                   atomic_unit=1.5)))\n",
    "\n",
    "    p, dp = s(xyz)\n",
    "    p.sum().backward()\n",
    "    test_grad = xyz.grad.allclose(dp.sum(dim=(0)))\n",
    "    err_grad = (xyz.grad-dp.sum(dim=(0))).abs().max()\n",
    "    print('RealSeriesSoap: grads are consistent with autograd: {} ({})'.format(\n",
    "        test_grad, err_grad))\n",
    "    assert eval(s.state).state == s.state\n",
    "\n",
    "\n",
    "def test_multisoap():\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "    from torch import tensor\n",
    "    xyz = torch.tensor([[0.175, 0.884, -0.87, 0.354, -0.082, 3.1],\n",
    "                        [-0.791, 0.116, 0.19, -0.832, 0.184, 0.],\n",
    "                        [0.387, 0.761, 0.655, -0.528, 0.973, 0.]]).t()\n",
    "    xyz = xyz*3\n",
    "    cutoff = 3.0*3\n",
    "    xyz.requires_grad = True\n",
    "\n",
    "    soaps = [TailoredSoap(RealSeriesSoap(2, 2, PolyCut(cutoff))),\n",
    "             TailoredSoap(RealSeriesSoap(3, 2, PolyCut(cutoff)))]\n",
    "    ms = NormalizedSoap(ScaledSoap(MultiSoap(soaps)))\n",
    "\n",
    "    masks = [xyz[:, 0] >= 0., xyz[:, 0] < 0.]\n",
    "    a, b = ms(xyz, masks)\n",
    "    a.sum().backward()\n",
    "    err = (xyz.grad-b.sum(dim=0)).abs().max()\n",
    "    test = xyz.grad.allclose(b.sum(dim=0))\n",
    "    assert ms.dim == a.size(0)\n",
    "    assert ms.state == eval(ms.state).state\n",
    "    print('MultiSoap: grads are consistent with autograd: {} ({})'.format(\n",
    "        test, err))\n",
    "\n",
    "\n",
    "def test_heterosoap():\n",
    "    import torch\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "\n",
    "    xyz = (torch.rand(10, 3) - 0.5) * 5\n",
    "    xyz.requires_grad = True\n",
    "    s = HeteroSoap(7, 5, PolyCut(8.0), [10, 18])\n",
    "    numbers = torch.tensor(4*[10]+6*[18])\n",
    "    p, dp = s(xyz, numbers)\n",
    "    p.sum().backward()\n",
    "    print('fits gradients calculated by autograd: {}'.format(\n",
    "        xyz.grad.allclose(dp.sum(dim=(0, 1, 2, 3, 4)))))\n",
    "\n",
    "\n",
    "if __name__ == '__main__' and True:\n",
    "    test_validity()\n",
    "    test_units()\n",
    "    test_realseriessoap()\n",
    "    test_multisoap()\n",
    "    test_speed()\n",
    "    test_heterosoap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
