{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from theforce.math.ylm import Ylm\n",
    "from torch.nn import Module\n",
    "from math import factorial as fac\n",
    "\n",
    "\n",
    "class AbsSeriesSoap(Module):\n",
    "\n",
    "    def __init__(self, lmax, nmax, radial, unit=None):\n",
    "        super().__init__()\n",
    "        self.ylm = Ylm(lmax)\n",
    "        self.nmax = nmax\n",
    "        self.radial = radial\n",
    "        if unit:\n",
    "            self.unit = unit\n",
    "        else:\n",
    "            self.unit = radial.rc/3\n",
    "        one = torch.ones(lmax+1, lmax+1)\n",
    "        self.Yr = 2*torch.torch.tril(one) - torch.eye(lmax+1)\n",
    "        self.Yi = 2*torch.torch.triu(one, diagonal=1)\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return \"{}, {}, {}, unit={}\".format(self.ylm.lmax, self.nmax, self.radial.state, self.unit)\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.__class__.__name__+'({})'.format(self.state_args)\n",
    "\n",
    "    def forward(self, coo, grad=True):\n",
    "        xyz = coo/self.unit\n",
    "        d = xyz.pow(2).sum(dim=-1).sqrt()\n",
    "        n = 2*torch.arange(self.nmax+1).type(xyz.type())\n",
    "        r, dr = self.radial(self.unit*d)\n",
    "        dr = self.unit*dr\n",
    "        f = (r*d[None]**n[:, None])\n",
    "        Y = self.ylm(xyz, grad=grad)\n",
    "        if grad:\n",
    "            Y, dY = Y\n",
    "        c = (f[:, None, None]*Y[None]).sum(dim=-1)\n",
    "        nnp = c[None, ]*c[:, None]\n",
    "        p = (nnp*self.Yr).sum(dim=-1) + (nnp*self.Yi).sum(dim=-2)\n",
    "        if grad:\n",
    "            df = dr*d[None]**n[:, None] + r*n[:, None]*d[None]**(n[:, None]-1)\n",
    "            df = df[..., None]*xyz/d[:, None]\n",
    "            dc = (df[:, None, None]*Y[None, ..., None] +\n",
    "                  f[:, None, None, :, None]*dY[None])\n",
    "            dnnp = (c[None, ..., None, None]*dc[:, None] +\n",
    "                    dc[None, ]*c[:, None, ..., None, None])\n",
    "            dp = ((dnnp*self.Yr[..., None, None]).sum(dim=-3) +\n",
    "                  (dnnp*self.Yi[..., None, None]).sum(dim=-4))\n",
    "            return p, dp/self.unit\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "\n",
    "class RealSeriesSoap(Module):\n",
    "    \"\"\"\n",
    "    Let s = atomic_unit,\n",
    "    if radial is e^{-0.5*(r/s)^2}, then this function will return \n",
    "    the exact descriptor vector (with indices n, n', l) obtained from\n",
    "    series expansion of the modified Bessel function of the first kind\n",
    "    in smooth overlap of atomic positions (SOAP).\n",
    "    We call this approach \"Series Expansion SOAP\" or SE-SOAP.\n",
    "    To understand meaning of atomic_unit, density of an atom (centered \n",
    "    at x) in space (at r) is described as: e^(-(|r-x|/s)^2)\n",
    "    Cutoff is controlled from outside this function through radial.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lmax, nmax, radial, atomic_unit=0.5):\n",
    "\n",
    "        super().__init__()\n",
    "        self.abs = AbsSeriesSoap(lmax, nmax, radial, unit=atomic_unit)\n",
    "\n",
    "        a = torch.tensor([[1./((2*l+1)*2**(2*n+l)*fac(n)*fac(n+l))\n",
    "                           for l in range(lmax+1)] for n in range(nmax+1)])\n",
    "        self.nnl = (a[None]*a[:, None]).sqrt()\n",
    "\n",
    "    def forward(self, xyz, grad=True):\n",
    "        p = self.abs(xyz, grad=grad)\n",
    "        if grad:\n",
    "            p, q = p\n",
    "            q = q*self.nnl[..., None, None]\n",
    "        p = p*self.nnl\n",
    "\n",
    "        if grad:\n",
    "            return p, q\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return \"{}, {}, {}, atomic_unit={}\".format(self.abs.ylm.lmax, self.abs.nmax,\n",
    "                                                   self.abs.radial.state, self.abs.unit)\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.__class__.__name__+'({})'.format(self.state_args)\n",
    "\n",
    "\n",
    "class SeriesSoap(Module):\n",
    "    \"\"\"deprecated\"\"\"\n",
    "\n",
    "    def __init__(self, lmax, nmax, radial, unit=None, modify=None, normalize=False,\n",
    "                 cutcorners=0, symm=False):\n",
    "        super().__init__()\n",
    "        self.abs = AbsSeriesSoap(lmax, nmax, radial, unit=unit)\n",
    "\n",
    "        if modify:\n",
    "            a = torch.tensor([[modify**(2*n+l)/((2*l+1)*2**(2*n+l)*fac(n)*fac(n+l))\n",
    "                               for l in range(lmax+1)] for n in range(nmax+1)])\n",
    "            self.nnl = (a[None]*a[:, None]).sqrt()\n",
    "        else:\n",
    "            self.nnl = torch.ones(nmax+1, nmax+1, lmax+1)\n",
    "\n",
    "        n = torch.arange(nmax+1)\n",
    "        self.mask = ((n[:, None]-n[None]).abs() <= nmax-cutcorners).byte()\n",
    "        if not symm:\n",
    "            self.mask = (self.mask & (n[:, None] >= n[None]).byte())\n",
    "\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.kwargs = 'modify={}, normalize={}, cutcorners={}, symm={}'.format(\n",
    "            modify, normalize, cutcorners, symm)\n",
    "\n",
    "        import warnings\n",
    "        warnings.warn(\"class {} is Deprecated\".format(self.__class__.__name__))\n",
    "\n",
    "    def forward(self, xyz, grad=True):\n",
    "        p = self.abs(xyz, grad=grad)\n",
    "        if grad:\n",
    "            p, q = p\n",
    "            q = q*self.nnl[..., None, None]\n",
    "        p = p*self.nnl\n",
    "\n",
    "        p = p[self.mask].view(-1)\n",
    "        if grad:\n",
    "            q = q[self.mask].view(p.size(0), *xyz.size())\n",
    "\n",
    "        if self.normalize:\n",
    "            norm = p.norm()\n",
    "            if norm > 0.0:\n",
    "                p = p/norm\n",
    "                if grad:\n",
    "                    q = q/norm\n",
    "                    q = q - p[..., None, None] * (p[..., None, None] * q\n",
    "                                                  ).sum(dim=(0))\n",
    "        if grad:\n",
    "            return p, q\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.mask.sum()*(self.abs.ylm.lmax+1)\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return \"{}, {}\".format(self.abs.state_args, self.kwargs)\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.__class__.__name__+'({})'.format(self.state_args)\n",
    "\n",
    "\n",
    "def test_validity():\n",
    "    import torch\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "\n",
    "    xyz = torch.tensor([[0.175, 0.884, -0.87, 0.354, -0.082, 3.1],\n",
    "                        [-0.791, 0.116, 0.19, -0.832, 0.184, 0.],\n",
    "                        [0.387, 0.761, 0.655, -0.528, 0.973, 0.]]).t()\n",
    "    xyz.requires_grad = True\n",
    "\n",
    "    target = torch.tensor([[[0.36174603, 0.39013356, 0.43448023],\n",
    "                            [0.39013356, 0.42074877, 0.46857549],\n",
    "                            [0.43448023, 0.46857549, 0.5218387]],\n",
    "\n",
    "                           [[0.2906253, 0.30558356, 0.33600938],\n",
    "                            [0.30558356, 0.3246583, 0.36077952],\n",
    "                            [0.33600938, 0.36077952, 0.40524778]],\n",
    "\n",
    "                           [[0.16241845, 0.18307552, 0.20443194],\n",
    "                            [0.18307552, 0.22340802, 0.26811937],\n",
    "                            [0.20443194, 0.26811937, 0.34109511]]])\n",
    "\n",
    "    s = AbsSeriesSoap(2, 2, PolyCut(3.0))\n",
    "    p, dp = s(xyz)\n",
    "    p = p.permute(2, 0, 1)\n",
    "    print('fits pre-calculated values: {}'.format(p.allclose(target)))\n",
    "\n",
    "    p.sum().backward()\n",
    "    print('fits gradients calculated by autograd: {}'.format(\n",
    "        xyz.grad.allclose(dp.sum(dim=(0, 1, 2)))))\n",
    "\n",
    "    # test with normalization turned on\n",
    "    s = SeriesSoap(3, 7, PolyCut(3.0), normalize=True)\n",
    "    xyz.grad *= 0\n",
    "    p, dp = s(xyz)\n",
    "    p.sum().backward()\n",
    "    print('fits gradients calculated by autograd (normalize=True):{}'.format(\n",
    "        xyz.grad.allclose(dp.sum(dim=(0)))))\n",
    "\n",
    "    assert s.state == eval(s.state).state\n",
    "\n",
    "    # test if works with empty tensors\n",
    "    s(torch.rand(0, 3))\n",
    "\n",
    "\n",
    "def test_units():\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "    xyz = torch.tensor([[0.175, 0.884, -0.87, 0.354, -0.082, 3.1],\n",
    "                        [-0.791, 0.116, 0.19, -0.832, 0.184, 0.],\n",
    "                        [0.387, 0.761, 0.655, -0.528, 0.973, 0.]]).t()\n",
    "    xyz = xyz*3\n",
    "    cutoff = 3.0*3\n",
    "    xyz.requires_grad = True\n",
    "\n",
    "    s = SeriesSoap(3, 3, PolyCut(cutoff), normalize=True)\n",
    "    p, dp = s(xyz)\n",
    "    p.sum().backward()\n",
    "    print('grads are consistent with larger length scale: {}'.format(\n",
    "        xyz.grad.allclose(dp.sum(dim=(0)))))\n",
    "\n",
    "\n",
    "def test_speed(N=100):\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "    import time\n",
    "    s = AbsSeriesSoap(5, 5, PolyCut(3.0))\n",
    "    start = time.time()\n",
    "    for _ in range(N):\n",
    "        xyz = torch.rand(30, 3)\n",
    "        p = s(xyz)\n",
    "    finish = time.time()\n",
    "    delta = (finish-start)/N\n",
    "    print(\"speed of {}: {} sec\".format(s.state, delta))\n",
    "\n",
    "\n",
    "def example():\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "\n",
    "    lengthscale = 2.\n",
    "    cutoff = 8.\n",
    "    xyz = torch.tensor([[1., 0, 0], [-1., 0, 0],\n",
    "                        [0, 1., 0], [0, -1., 0],\n",
    "                        [0, 0, 1.], [0, 0, -1.]]) * lengthscale\n",
    "    xyz.requires_grad = True\n",
    "    s = SeriesSoap(2, 2, PolyCut(cutoff), normalize=True)\n",
    "    p, dp = s(xyz)\n",
    "    print(p)\n",
    "\n",
    "\n",
    "def test_realseriessoap():\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "    xyz = torch.tensor([[0.175, 0.884, -0.87, 0.354, -0.082, 3.1],\n",
    "                        [-0.791, 0.116, 0.19, -0.832, 0.184, 0.],\n",
    "                        [0.387, 0.761, 0.655, -0.528, 0.973, 0.]]).t()\n",
    "    xyz = xyz*3\n",
    "    cutoff = 3.0*3\n",
    "    xyz.requires_grad = True\n",
    "\n",
    "    s = RealSeriesSoap(2, 2, PolyCut(cutoff), atomic_unit=1.5)\n",
    "    p, dp = s(xyz)\n",
    "    p.sum().backward()\n",
    "    test_grad = xyz.grad.allclose(dp.sum(dim=(0, 1, 2)))\n",
    "    err_grad = (xyz.grad-dp.sum(dim=(0, 1, 2))).abs().max()\n",
    "    print('RealSeriesSoap: grads are consistent with autograd: {} ({})'.format(\n",
    "        test_grad, err_grad))\n",
    "    assert eval(s.state).state == s.state\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_validity()\n",
    "    test_units()\n",
    "    test_realseriessoap()\n",
    "    test_speed()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
