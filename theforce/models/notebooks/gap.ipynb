{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theforce.descriptor.sesoap import SeSoap\n",
    "from theforce.descriptor.radial_funcs import quadratic_cutoff\n",
    "from theforce.descriptor.clustersoap import ClusterSoap\n",
    "from theforce.regression.kernels import RBF\n",
    "from theforce.regression.algebra import low_rank_factor, jitcholesky\n",
    "from theforce.regression.algebra import positive, free_form, sum_packed_dim\n",
    "import ase\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Module, Parameter\n",
    "from torch.distributions import LowRankMultivariateNormal\n",
    "import warnings\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "\n",
    "class GAP(Module):\n",
    "\n",
    "    def __init__(self, lmax, nmax, cutoff):\n",
    "        super(GAP, self).__init__()\n",
    "        self.csoap = ClusterSoap(SeSoap(lmax, nmax, quadratic_cutoff(cutoff)))\n",
    "        self.data = []\n",
    "\n",
    "    def add_data(self, cluster):\n",
    "\n",
    "        # configure\n",
    "        if type(cluster) == tuple or type(cluster) == list:\n",
    "            pbc, cell, positions, atomic_numbers, energy, forces = cluster\n",
    "        elif isinstance(cluster, ase.Atoms):\n",
    "            pbc, cell, positions = cluster.pbc, cluster.cell, cluster.positions\n",
    "            atomic_numbers = cluster.get_atomic_numbers()\n",
    "            energy = cluster.get_potential_energy()\n",
    "            forces = cluster.get_forces()\n",
    "\n",
    "        # atomic numbers\n",
    "        if atomic_numbers is not None:\n",
    "            if np.any(atomic_numbers != atomic_numbers[0]):\n",
    "                raise NotImplementedError(\n",
    "                    'heterogeneous atomic numbers are not implemented yet')\n",
    "            else:\n",
    "                atomic_numbers = None\n",
    "\n",
    "        # descriptors\n",
    "        if forces is not None:\n",
    "            p, q, indices = self.csoap.descriptors_derivatives(pbc, cell, positions,\n",
    "                                                               sumj=False, jsorted=True)\n",
    "        elif energy is not None:\n",
    "            p = self.csoap.descriptors(pbc, cell, positions)\n",
    "            q, indices = None, None\n",
    "        else:\n",
    "            p, q, indices = None, None, None\n",
    "\n",
    "        # apply torch.as_tensor\n",
    "        if energy is not None:\n",
    "            energy = torch.as_tensor([energy])\n",
    "        if forces is not None:\n",
    "            forces = torch.as_tensor(forces)\n",
    "        if p is not None:\n",
    "            p = torch.as_tensor(p)\n",
    "        if q is not None:\n",
    "            q = [torch.as_tensor(v) for v in q]\n",
    "            indices = [torch.as_tensor(v) for v in indices]\n",
    "\n",
    "        # add to data\n",
    "        self.data += [(p, q, indices, energy, forces)]\n",
    "\n",
    "    def select_Z(self, num_inducing):\n",
    "        X = torch.cat([a[0] for a in self.data])\n",
    "        rnd = torch.randint(len(self.data), (num_inducing,))\n",
    "        Z = X[rnd]\n",
    "        return Z\n",
    "\n",
    "    def parameterize(self, num_inducing, use_energies=1, use_forces=1, kern=RBF):\n",
    "        # kernel param\n",
    "        self._noise = Parameter(torch.tensor(1.))\n",
    "        self.kern = kern(torch.ones(self.csoap.soap.dim), torch.tensor(1.))\n",
    "\n",
    "        # inducing\n",
    "        self.Z = Parameter(self.select_Z(num_inducing), requires_grad=False)\n",
    "\n",
    "        # flags\n",
    "        self.parameterized = 1\n",
    "        self.use_energies = use_energies\n",
    "        self.use_forces = use_forces\n",
    "\n",
    "    def covariances(self):\n",
    "\n",
    "        for (p, q, indices, energy, forces) in self.data:\n",
    "\n",
    "            # TODO: d_dx, d_dxdxx are only needed if forces are present\n",
    "            # TODO: d_dxdxx is not fully needed at a given instance\n",
    "            zx, _, d_dx, _ = self.kern.matrices(self.Z, p, False,\n",
    "                                                True, False)\n",
    "            xx, _, _, d_dxdxx = self.kern.matrices(p, p, False,\n",
    "                                                   False, True)\n",
    "\n",
    "            if self.use_energies and p is not None and energy is not None:\n",
    "                ZX = zx.sum(dim=-1)\n",
    "                diag = xx.sum()\n",
    "                yield ZX.view(-1, 1), diag.view(1), energy.view(1)\n",
    "\n",
    "            if self.use_forces and q is not None and forces is not None and indices is not None:\n",
    "                for i in range(len(q)):\n",
    "                    dxj_dri, j = q[i], indices[i]\n",
    "                    ZF = -torch.einsum('ijp,pjm->im', d_dx[:, j], dxj_dri)\n",
    "                    diag = torch.einsum('qim,ijqp,pjm->m',\n",
    "                                        dxj_dri, d_dxdxx[j][:, j], dxj_dri)\n",
    "                    yield ZF, diag.view(3), forces[i].view(3)\n",
    "\n",
    "    def matrices(self):\n",
    "        ZZ, _, _, _ = self.kern.matrices(self.Z, self.Z)\n",
    "        ZX, diag, Y = zip(*self.covariances())\n",
    "        return ZZ, torch.cat(ZX, dim=1), torch.cat(diag), torch.cat(Y)\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "        # covariances\n",
    "        ZZ, ZX, diag, Y = self.matrices()\n",
    "        tr = diag.sum()\n",
    "        noise = positive(self._noise)\n",
    "\n",
    "        # trace term\n",
    "        Q, _, ridge = low_rank_factor(ZZ, ZX)\n",
    "        trace = 0.5*(tr - torch.einsum('ij,ij', Q, Q))/noise**2\n",
    "\n",
    "        # low rank MVN\n",
    "        p = LowRankMultivariateNormal(torch.zeros_like(Y), Q.t(),\n",
    "                                      torch.ones_like(Y)*noise**2)\n",
    "\n",
    "        # loss\n",
    "        loss = -p.log_prob(Y) + trace\n",
    "        return loss\n",
    "\n",
    "    def train(self, steps=100, optimizer=None, lr=0.1):\n",
    "\n",
    "        if not self.parameterized:\n",
    "            warnings.warn(\n",
    "                'model is not parameterized yet! returned without training!')\n",
    "            return\n",
    "\n",
    "        if not hasattr(self, 'losses'):\n",
    "            self.losses = []\n",
    "            self.starts = []\n",
    "        self.starts += [len(self.losses)]\n",
    "\n",
    "        if optimizer is None:\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        for _ in range(steps):\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.forward()\n",
    "            self.losses += [loss.data]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('trained for {} steps'.format(steps))\n",
    "\n",
    "        self.ready = 0\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        ZZ, ZX, _, Y = self.matrices()\n",
    "        XZ = ZX.t()\n",
    "        noise = positive(self._noise)\n",
    "\n",
    "        # numerically stable calculation of _mu\n",
    "        L, ridge = jitcholesky(ZZ, jitbase=2)\n",
    "        A = torch.cat((XZ, noise * L.t()))\n",
    "        Y = torch.cat((Y, torch.zeros(self.Z.size(0),\n",
    "                                      dtype=Y.dtype)))\n",
    "        Q, R = torch.qr(A)\n",
    "        self._mu = torch.mv(R.inverse(), torch.mv(Q.t(), Y))\n",
    "\n",
    "        # inducing function values (Z, u)\n",
    "        self.u = torch.mv(ZZ, self._mu)\n",
    "\n",
    "        # TODO: predicted covariance\n",
    "\n",
    "        self.ready = 1\n",
    "\n",
    "    def predict(self, cluster):\n",
    "        if not hasattr(self, 'ready') or not self.ready:\n",
    "            self.evaluate()\n",
    "\n",
    "        # configure\n",
    "        if type(cluster) == tuple or type(cluster) == list:\n",
    "            pbc, cell, positions, atomic_numbers = cluster\n",
    "        elif isinstance(cluster, ase.Atoms):\n",
    "            pbc, cell, positions = cluster.pbc, cluster.cell, cluster.positions\n",
    "            atomic_numbers = cluster.get_atomic_numbers()\n",
    "\n",
    "        # descriptors\n",
    "        p, q, indices = self.csoap.descriptors_derivatives(pbc, cell, positions,\n",
    "                                                           sumj=False, jsorted=True)\n",
    "        p = torch.as_tensor(p)\n",
    "        q = [torch.as_tensor(v) for v in q]\n",
    "        indices = [torch.as_tensor(v) for v in indices]\n",
    "\n",
    "        # covariances\n",
    "        ZX, _, d_dx, _ = self.kern.matrices(self.Z, p, False, True, False)\n",
    "        ZF = []\n",
    "        for i in range(len(q)):\n",
    "            dxj_dri, j = q[i], indices[i]\n",
    "            ZF += [-torch.einsum('ijp,pjm->im', d_dx[:, j], dxj_dri)]\n",
    "        XZ = torch.cat([ZX, *ZF], dim=1).t()\n",
    "\n",
    "        # predict\n",
    "        mu = torch.mv(XZ, self._mu)\n",
    "        energy = mu[0:p.size(0)].sum()\n",
    "        forces = mu[p.size(0):].view(-1, 3)\n",
    "        return energy, forces"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
