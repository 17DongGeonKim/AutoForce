{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theforce.descriptor.sesoap import SeSoap\n",
    "from theforce.descriptor.radial_funcs import quadratic_cutoff\n",
    "from theforce.descriptor.clustersoap import ClusterSoap\n",
    "from theforce.regression.kernels import RBF\n",
    "from theforce.regression.algebra import low_rank_factor, jitcholesky\n",
    "from theforce.regression.algebra import positive, free_form, sum_packed_dim\n",
    "from theforce.util.tensors import SparseTensor, stretch_tensor\n",
    "import ase\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Module, Parameter\n",
    "from torch.distributions import LowRankMultivariateNormal\n",
    "import warnings\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "\n",
    "def unnamed_operation(s):\n",
    "    s._sort(key=1)\n",
    "    s._split()\n",
    "    size = max(s.i_max, s.j_max) + 1  # number of particles\n",
    "    h = torch.zeros(s.shape[0], s.shape[0], size, size)\n",
    "    for i, j, a in zip(*[s.i, s.j, s.a]):\n",
    "        b = (stretch_tensor(a, (0, 1)) *\n",
    "             stretch_tensor(a, (1, 2))).sum(dim=-1)\n",
    "        k, l = torch.broadcast_tensors(i[None, ], i[:, None])\n",
    "        h[:, :, k, l] += b\n",
    "    return h.permute(2, 3, 0, 1)\n",
    "\n",
    "\n",
    "class GAP(Module):\n",
    "\n",
    "    def __init__(self, lmax, nmax, cutoff):\n",
    "        super(GAP, self).__init__()\n",
    "        self.csoap = ClusterSoap(SeSoap(lmax, nmax, quadratic_cutoff(cutoff)))\n",
    "        self.data = []\n",
    "\n",
    "    def add_data(self, cluster):\n",
    "\n",
    "        # configure\n",
    "        if type(cluster) == tuple or type(cluster) == list:\n",
    "            pbc, cell, positions, atomic_numbers, energy, forces = cluster\n",
    "        elif isinstance(cluster, ase.Atoms):\n",
    "            pbc, cell, positions = cluster.pbc, cluster.cell, cluster.positions\n",
    "            atomic_numbers = cluster.get_atomic_numbers()\n",
    "            energy = cluster.get_potential_energy()\n",
    "            forces = cluster.get_forces()\n",
    "\n",
    "        # atomic numbers\n",
    "        if atomic_numbers is not None:\n",
    "            if np.any(atomic_numbers != atomic_numbers[0]):\n",
    "                raise NotImplementedError(\n",
    "                    'heterogeneous atomic numbers are not implemented yet')\n",
    "            else:\n",
    "                atomic_numbers = None\n",
    "\n",
    "        # descriptors\n",
    "        if forces is not None:\n",
    "            p, q, i, j = self.csoap.descriptors_derivatives(pbc, cell, positions,\n",
    "                                                            sumj=False, jsorted=True)\n",
    "        elif energy is not None:\n",
    "            p = self.csoap.descriptors(pbc, cell, positions)\n",
    "            q, i, j = None, None, None\n",
    "        else:\n",
    "            p, q, i, j = None, None, None, None\n",
    "\n",
    "        # apply torch.as_tensor\n",
    "        if energy is not None:\n",
    "            energy = torch.as_tensor([energy])\n",
    "        if forces is not None:\n",
    "            forces = torch.as_tensor(forces)\n",
    "        if p is not None:\n",
    "            p = torch.as_tensor(p)\n",
    "        if q is not None:\n",
    "            q = [torch.as_tensor(v) for v in q]\n",
    "            i = [torch.as_tensor(v) for v in i]\n",
    "            s = SparseTensor(shape=(self.csoap.soap.dim, 0, 3))\n",
    "            s.add(i, j, q)\n",
    "            h = unnamed_operation(s)\n",
    "            s._cat()\n",
    "\n",
    "        # add to data\n",
    "        self.data += [(p, s, h, energy, forces)]\n",
    "\n",
    "    def select_Z(self, num_inducing):\n",
    "        X = torch.cat([a[0] for a in self.data])\n",
    "        rnd = torch.randint(len(self.data), (num_inducing,))\n",
    "        Z = X[rnd]\n",
    "        return Z\n",
    "\n",
    "    def parameterize(self, num_inducing, use_energies=1, use_forces=1, kern=RBF):\n",
    "        # kernel param\n",
    "        self._noise = Parameter(torch.tensor(1.))\n",
    "        self.kern = kern(torch.ones(self.csoap.soap.dim), torch.tensor(1.))\n",
    "\n",
    "        # inducing\n",
    "        self.Z = Parameter(self.select_Z(num_inducing), requires_grad=False)\n",
    "\n",
    "        # flags\n",
    "        self.parameterized = 1\n",
    "        self.use_energies = use_energies\n",
    "        self.use_forces = use_forces\n",
    "\n",
    "    def covariances(self):\n",
    "\n",
    "        for (p, s, h, energy, forces) in self.data:\n",
    "\n",
    "            # TODO: d_dx, d_dxdxx are only needed if forces are present\n",
    "            zx, _, d_dx, _ = self.kern.matrices(self.Z, p, False,\n",
    "                                                True, False)\n",
    "            xx, _, _, d_dxdxx = self.kern.matrices(p, p, False,\n",
    "                                                   False, True)\n",
    "\n",
    "            if self.use_energies and p is not None and energy is not None:\n",
    "                ZX = zx.sum(dim=-1)\n",
    "                diag = xx.sum()\n",
    "                yield ZX.view(-1, 1), diag.view(1), energy.view(1)\n",
    "\n",
    "            if self.use_forces and s is not None and forces is not None and h is not None:\n",
    "                temp = -(d_dx[:, s.i, :, None]*s.a.permute(1, 0, 2)).sum(dim=2)\n",
    "                m = self.Z.size(0)\n",
    "                ZF = torch.zeros(m, *forces.size()).index_add(1, s.j, temp\n",
    "                                                              ).view(m, -1)\n",
    "                sum_diag = (d_dxdxx*h).sum()\n",
    "                yield ZF, sum_diag.view(1), forces.view(-1)\n",
    "\n",
    "    def matrices(self):\n",
    "        ZZ, _, _, _ = self.kern.matrices(self.Z, self.Z)\n",
    "        ZX, diag, Y = zip(*self.covariances())\n",
    "        return ZZ, torch.cat(ZX, dim=1), torch.cat(diag), torch.cat(Y)\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "        # covariances\n",
    "        ZZ, ZX, diag, Y = self.matrices()\n",
    "        tr = diag.sum()\n",
    "        noise = positive(self._noise)\n",
    "\n",
    "        # trace term\n",
    "        Q, _, ridge = low_rank_factor(ZZ, ZX)\n",
    "        trace = 0.5*(tr - torch.einsum('ij,ij', Q, Q))/noise**2\n",
    "\n",
    "        # low rank MVN\n",
    "        p = LowRankMultivariateNormal(torch.zeros_like(Y), Q.t(),\n",
    "                                      torch.ones_like(Y)*noise**2)\n",
    "\n",
    "        # loss\n",
    "        loss = -p.log_prob(Y) + trace\n",
    "        return loss\n",
    "\n",
    "    def train(self, steps=100, optimizer=None, lr=0.1):\n",
    "\n",
    "        if not self.parameterized:\n",
    "            warnings.warn(\n",
    "                'model is not parameterized yet! returned without training!')\n",
    "            return\n",
    "\n",
    "        if not hasattr(self, 'losses'):\n",
    "            self.losses = []\n",
    "            self.starts = []\n",
    "        self.starts += [len(self.losses)]\n",
    "\n",
    "        if optimizer is None:\n",
    "            if not hasattr(self, 'optimizer'):\n",
    "                self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "            optimizer = self.optimizer\n",
    "\n",
    "        for _ in range(steps):\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.forward()\n",
    "            self.losses += [loss.data]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        optimizer.zero_grad()  # NOTE: maybe unnecessary\n",
    "\n",
    "        print('trained for {} steps'.format(steps))\n",
    "\n",
    "        self.ready = 0\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        ZZ, ZX, _, Y = self.matrices()\n",
    "        XZ = ZX.t()\n",
    "        noise = positive(self._noise)\n",
    "\n",
    "        # numerically stable calculation of _mu\n",
    "        L, ridge = jitcholesky(ZZ, jitbase=2)\n",
    "        A = torch.cat((XZ, noise * L.t()))\n",
    "        Y = torch.cat((Y, torch.zeros(self.Z.size(0),\n",
    "                                      dtype=Y.dtype)))\n",
    "        Q, R = torch.qr(A)\n",
    "        self._mu = torch.mv(R.inverse(), torch.mv(Q.t(), Y))\n",
    "\n",
    "        # inducing function values (Z, u)\n",
    "        self.u = torch.mv(ZZ, self._mu)\n",
    "\n",
    "        # TODO: predicted covariance\n",
    "\n",
    "        self.ready = 1\n",
    "\n",
    "    def predict(self, cluster):\n",
    "        if not hasattr(self, 'ready') or not self.ready:\n",
    "            self.evaluate()\n",
    "\n",
    "        # configure\n",
    "        if type(cluster) == tuple or type(cluster) == list:\n",
    "            pbc, cell, positions, atomic_numbers = cluster\n",
    "        elif isinstance(cluster, ase.Atoms):\n",
    "            pbc, cell, positions = cluster.pbc, cluster.cell, cluster.positions\n",
    "            atomic_numbers = cluster.get_atomic_numbers()\n",
    "\n",
    "        # descriptors\n",
    "        p, q, I, J = self.csoap.descriptors_derivatives(pbc, cell, positions,\n",
    "                                                        sumj=False, jsorted=True)\n",
    "        p = torch.as_tensor(p)\n",
    "        q = [torch.as_tensor(v) for v in q]\n",
    "        I = [torch.as_tensor(v) for v in I]\n",
    "\n",
    "        # covariances\n",
    "        ZX, _, d_dx, _ = self.kern.matrices(self.Z, p, False, True, False)\n",
    "        ZF = []\n",
    "        for i, j, dxi_drj in zip(*[I, J, q]):\n",
    "            ZF += [-torch.einsum('ijp,pjm->im', d_dx[:, i], dxi_drj)]\n",
    "        XZ = torch.cat([ZX, *ZF], dim=1).t()\n",
    "\n",
    "        # predict\n",
    "        mu = torch.mv(XZ, self._mu)\n",
    "        energy = mu[0:p.size(0)].sum()\n",
    "        forces = torch.zeros(p.size(0), 3)\n",
    "        forces[J] = mu[p.size(0):].view(-1, 3)\n",
    "        # NOTE: if env of an atom is empty, it will not be present in J\n",
    "        # and the zero will be passed as the force\n",
    "        return energy, forces"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
