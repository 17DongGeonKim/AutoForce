{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import ones_like, as_tensor, from_numpy, cat\n",
    "from ase.atoms import Atoms\n",
    "from ase.neighborlist import NeighborList\n",
    "from ase.calculators.singlepoint import SinglePointCalculator\n",
    "import copy\n",
    "import warnings\n",
    "from theforce.util.util import iterable\n",
    "from theforce.util.parallel import balance_work\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "\n",
    "def lex3(x):\n",
    "    if x[0] != 0:\n",
    "        return x[0] > -x[0]\n",
    "    elif x[1] != 0:\n",
    "        return x[1] > -x[1]\n",
    "    elif x[2] != 0:\n",
    "        return x[2] > -x[2]\n",
    "    else:\n",
    "        return True  # True or False here shouldn't make any difference\n",
    "\n",
    "\n",
    "class Local:\n",
    "\n",
    "    def __init__(self, i, j, a, b, r, off=None, descriptors=[], device=None):\n",
    "        \"\"\"\n",
    "        i, j: indices\n",
    "        a, b: atomic numbers\n",
    "        r : r[j] - r[i]\n",
    "        off: offsets\n",
    "        \"\"\"\n",
    "        self._i = from_numpy(np.full_like(j, i)).to(device)\n",
    "        self._j = from_numpy(j).to(device)\n",
    "        self._a = from_numpy(np.full_like(b, a)).to(device)\n",
    "        self._b = from_numpy(b).to(device)\n",
    "        self._r = r.to(device)\n",
    "        self._m = ones_like(self._i).to(torch.bool).to(device)\n",
    "        self.off = off\n",
    "        self.stage(descriptors)\n",
    "\n",
    "    def to(self, device):\n",
    "        self._i = self._i.to(device)\n",
    "        self._j = self._j.to(device)\n",
    "        self._a = self._a.to(device)\n",
    "        self._b = self._b.to(device)\n",
    "        self._r = self._r.to(device)\n",
    "        self._m = self._m.to(device)\n",
    "\n",
    "    def stage(self, descriptors, device=None):\n",
    "        if device is not None:\n",
    "            self.to(device)\n",
    "        for desc in descriptors:\n",
    "            desc.precalculate(self)\n",
    "\n",
    "    @property\n",
    "    def loc(self):\n",
    "        \"\"\"This is defined as a property in order to avoid memory leak.\"\"\"\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def i(self):\n",
    "        return self._i[self._m]\n",
    "\n",
    "    @property\n",
    "    def j(self):\n",
    "        return self._j[self._m]\n",
    "\n",
    "    @property\n",
    "    def a(self):\n",
    "        return self._a[self._m]\n",
    "\n",
    "    @property\n",
    "    def b(self):\n",
    "        return self._b[self._m]\n",
    "\n",
    "    @property\n",
    "    def r(self):\n",
    "        return self._r[self._m]\n",
    "\n",
    "    @property\n",
    "    def _lex(self):\n",
    "        \"\"\"This is defined as a property in order to avoid memory leak.\"\"\"\n",
    "        if self.off is None:\n",
    "            return ones_like(self._i, device=self._i.device).to(torch.bool)\n",
    "        else:\n",
    "            return torch.tensor([lex3(a) for a in self.off], dtype=torch.bool, device=self._i.device)\n",
    "\n",
    "    @property\n",
    "    def lex(self):\n",
    "        return self._lex[self._m]\n",
    "\n",
    "    def select(self, a, b, bothways=False, in_place=True):\n",
    "        m = (self._a == a) & (self._b == b)\n",
    "        if a == b:\n",
    "            if bothways:\n",
    "                pass\n",
    "            else:\n",
    "                m = m & ((self._j > self._i) | ((self._j == self._i) &\n",
    "                                                self._lex))\n",
    "        elif a != b:\n",
    "            if bothways:\n",
    "                m = (m | ((self._a == b) & (self._b == a)))\n",
    "            else:\n",
    "                pass\n",
    "        if in_place:\n",
    "            self._m = m.to(torch.bool)\n",
    "        return m.to(torch.bool)\n",
    "\n",
    "    def unselect(self):\n",
    "        self._m = ones_like(self._i).to(torch.bool)\n",
    "\n",
    "    def as_atoms(self):\n",
    "        a = self._a.unique().detach().numpy()\n",
    "        atoms = (TorchAtoms(numbers=a, positions=len(a)*[(0, 0, 0)]) +\n",
    "                 TorchAtoms(numbers=self._b.detach().numpy(), positions=self._r.detach().numpy()))\n",
    "        if 'target_energy' in self.__dict__:\n",
    "            atoms.set_calculator(\n",
    "                SinglePointCalculator(atoms, energy=self.target_energy))\n",
    "        return atoms\n",
    "\n",
    "    def detach(self, keepids=False, device=None):\n",
    "        a = self._a.detach().cpu().numpy()\n",
    "        b = self._b.detach().cpu().numpy()\n",
    "        r = self._r.clone()\n",
    "        if keepids:\n",
    "            i = self._i.detach().cpu().numpy()\n",
    "            j = self._j.detach().cpu().numpy()\n",
    "        else:\n",
    "            i = np.zeros(a.shape[0], dtype=np.int)\n",
    "            j = np.arange(1, a.shape[0]+1, dtype=np.int)\n",
    "        return Local(i, j, a, b, r, device=device)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if other.__class__ == TorchAtoms:\n",
    "            return False\n",
    "        elif (self._i != other._i).any():\n",
    "            return False\n",
    "        elif (self._j != other._j).any():\n",
    "            return False\n",
    "        elif (self._a != other._a).any():\n",
    "            return False\n",
    "        elif (self._b != other._b).any():\n",
    "            return False\n",
    "        elif (self._r != other._r).any():\n",
    "            return False\n",
    "        elif (self._lex != other._lex).any():\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "class AtomsChanges:\n",
    "\n",
    "    def __init__(self, atoms):\n",
    "        self._ref = atoms\n",
    "        self.update_references()\n",
    "\n",
    "    def update_references(self):\n",
    "        self._natoms = self._ref.natoms\n",
    "        self._numbers = self._ref.numbers.copy()\n",
    "        self._positions = self._ref.positions.copy()\n",
    "        self._cell = self._ref.cell.copy()\n",
    "        self._pbc = self._ref.pbc.copy()\n",
    "        self._descriptors = [kern.state for kern in\n",
    "                             self._ref.descriptors]\n",
    "\n",
    "    @property\n",
    "    def natoms(self):\n",
    "        return self._ref.natoms != self._natoms\n",
    "\n",
    "    @property\n",
    "    def atomic_numbers(self):\n",
    "        return (self._ref.numbers != self._numbers).any()\n",
    "\n",
    "    @property\n",
    "    def numbers(self):\n",
    "        return (self.natoms or self.atomic_numbers)\n",
    "\n",
    "    @property\n",
    "    def positions(self):\n",
    "        return not np.allclose(self._ref.positions, self._positions)\n",
    "\n",
    "    @property\n",
    "    def cell(self):\n",
    "        return not np.allclose(self._ref.cell, self._cell)\n",
    "\n",
    "    @property\n",
    "    def pbc(self):\n",
    "        return (self._ref.pbc != self._pbc).any()\n",
    "\n",
    "    @property\n",
    "    def atoms(self):\n",
    "        return any([self.numbers, self.positions, self.cell, self.pbc])\n",
    "\n",
    "    @property\n",
    "    def descriptors(self):\n",
    "        return [c != r.state for c, r in zip(*[self._descriptors, self._ref.descriptors])]\n",
    "\n",
    "\n",
    "class TorchAtoms(Atoms):\n",
    "\n",
    "    def __init__(self, ase_atoms=None, energy=None, forces=None, cutoff=None,\n",
    "                 descriptors=[], group=None, device=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if ase_atoms:\n",
    "            self.__dict__ = ase_atoms.__dict__\n",
    "\n",
    "        # ------------------------------- ----------\n",
    "        if group is not None:\n",
    "            self.attach_process_group(group)\n",
    "        else:\n",
    "            self.is_distributed = False\n",
    "        self.cutoff = cutoff\n",
    "        self.descriptors = descriptors\n",
    "        self.changes = AtomsChanges(self)\n",
    "        if cutoff is not None:\n",
    "            self.build_nl(cutoff)\n",
    "            self.update(forced=True, device=device)\n",
    "        # ------------------------------------------\n",
    "\n",
    "        try:\n",
    "            self.target_energy = as_tensor(energy).to(device)\n",
    "            self.target_forces = as_tensor(forces).to(device)\n",
    "        except RuntimeError:\n",
    "            if ase_atoms is not None and ase_atoms.get_calculator() is not None:\n",
    "                if 'energy' in ase_atoms.calc.results:\n",
    "                    self.target_energy = as_tensor(\n",
    "                        ase_atoms.get_potential_energy()).to(device)\n",
    "                if 'forces' in ase_atoms.calc.results:\n",
    "                    self.target_forces = as_tensor(\n",
    "                        ase_atoms.get_forces()).to(device)\n",
    "\n",
    "    def attach_process_group(self, group):\n",
    "        self.process_group = group\n",
    "        self.is_distributed = True\n",
    "\n",
    "    def detach_process_group(self):\n",
    "        del self.process_group\n",
    "        self.is_distributed = False\n",
    "\n",
    "    def build_nl(self, rc):\n",
    "        self.nl = NeighborList(self.natoms * [rc / 2], skin=0.0,\n",
    "                               self_interaction=False, bothways=True)\n",
    "        self.cutoff = rc\n",
    "        self.xyz = torch.from_numpy(self.positions)\n",
    "        try:\n",
    "            self.lll = torch.from_numpy(self.cell)\n",
    "        except TypeError:\n",
    "            self.lll = torch.from_numpy(self.cell.array)\n",
    "        # distributed setup\n",
    "        if self.is_distributed:\n",
    "            workers = torch.distributed.get_world_size(\n",
    "                group=self.process_group)\n",
    "            indices = balance_work(self.natoms, workers)\n",
    "            rank = torch.distributed.get_rank(group=self.process_group)\n",
    "            self.indices = range(*indices[rank])\n",
    "        else:\n",
    "            self.indices = range(self.natoms)\n",
    "\n",
    "    def update(self, cutoff=None, descriptors=None, forced=False,\n",
    "               posgrad=False, cellgrad=False, device=None):\n",
    "        if cutoff or self.changes.numbers:\n",
    "            self.build_nl(cutoff if cutoff else self.cutoff)\n",
    "            forced = True\n",
    "        if descriptors:\n",
    "            self.descriptors = descriptors\n",
    "            forced = True\n",
    "        if forced or self.changes.atoms:\n",
    "            self.nl.update(self)\n",
    "            self.loc = []\n",
    "            types = self.get_atomic_numbers()\n",
    "            self.xyz.requires_grad = posgrad\n",
    "            self.lll.requires_grad = cellgrad\n",
    "            for a in self.indices:\n",
    "                n, off = self.nl.get_neighbors(a)\n",
    "                cells = (from_numpy(off[..., None].astype(np.float)) *\n",
    "                         self.lll).sum(dim=1)\n",
    "                r = self.xyz[n] - self.xyz[a] + cells\n",
    "                self.loc += [Local(a, n, types[a], types[n],\n",
    "                                   r, off, self.descriptors, device=device)]\n",
    "            for loc in self.loc:\n",
    "                loc.natoms = self.natoms\n",
    "\n",
    "            self.changes.update_references()\n",
    "\n",
    "    @property\n",
    "    def natoms(self):\n",
    "        return self.get_number_of_atoms()\n",
    "\n",
    "    @property\n",
    "    def tnumbers(self):\n",
    "        return torch.from_numpy(self.numbers)\n",
    "\n",
    "    @property\n",
    "    def tpbc(self):\n",
    "        return torch.from_numpy(self.pbc)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        try:\n",
    "            return torch.cat([env.__dict__[attr] for env in self.loc])\n",
    "        except KeyError:\n",
    "            raise AttributeError()\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        \"\"\"This is a overloads the behavior of ase.Atoms.\"\"\"\n",
    "        return self.loc[k]\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"This is a overloads the behavior of ase.Atoms.\"\"\"\n",
    "        for env in self.loc:\n",
    "            yield env\n",
    "\n",
    "    def __eq__(self, other):  # Note: descriptors are excluded\n",
    "        if other.__class__ == Local:\n",
    "            return False\n",
    "        elif self.natoms != other.natoms:\n",
    "            return False\n",
    "        elif (self.pbc != other.pbc).any():\n",
    "            return False\n",
    "        elif not self.lll.allclose(other.lll):\n",
    "            return False\n",
    "        elif (self.numbers != other.numbers).any():\n",
    "            return False\n",
    "        elif not self.xyz.allclose(other.xyz):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def copy(self):\n",
    "        new = TorchAtoms(positions=self.positions.copy(),\n",
    "                         cell=self.cell.copy(),\n",
    "                         numbers=self.numbers.copy(),\n",
    "                         pbc=self.pbc.copy(),\n",
    "                         descriptors=self.descriptors,\n",
    "                         cutoff=self.cutoff,\n",
    "                         group=self.process_group if self.is_distributed else None)\n",
    "        vel = self.get_velocities()\n",
    "        if vel is not None:\n",
    "            new.set_velocities(vel)\n",
    "        return new\n",
    "\n",
    "    def set_cell(self, *args, **kwargs):\n",
    "        super().set_cell(*args, **kwargs)\n",
    "        try:\n",
    "            self.lll = torch.from_numpy(self.cell)\n",
    "        except TypeError:\n",
    "            self.lll = torch.from_numpy(self.cell.array)\n",
    "\n",
    "    def set_positions(self, *args, **kwargs):\n",
    "        super().set_positions(*args, **kwargs)\n",
    "        self.xyz = torch.from_numpy(self.positions)\n",
    "\n",
    "    def as_ase(self):\n",
    "        atoms = Atoms(positions=self.positions, cell=self.cell,\n",
    "                      pbc=self.pbc, numbers=self.numbers)  # TODO: e, f\n",
    "        vel = self.get_velocities()\n",
    "        if vel is not None:\n",
    "            atoms.set_velocities(vel)\n",
    "        return atoms\n",
    "\n",
    "    def as_local(self):\n",
    "        \"\"\" As the inverse of Local.as_atoms \"\"\"\n",
    "        # positions[0] should to be [0, 0, 0]\n",
    "        r = torch.as_tensor(self.positions[1:])\n",
    "        a, b = np.broadcast_arrays(self.numbers[0], self.numbers[1:])\n",
    "        _i = np.arange(self.natoms)\n",
    "        i, j = np.broadcast_arrays(_i[0], _i[1:])\n",
    "        loc = Local(i, j, a, b, r)\n",
    "        if 'target_energy' in self.__dict__:\n",
    "            loc.target_energy = self.target_energy\n",
    "        return loc\n",
    "\n",
    "    def shake(self, beta=0.05, update=True):\n",
    "        trans = np.random.laplace(0., beta, size=self.positions.shape)\n",
    "        self.translate(trans)\n",
    "        if update:\n",
    "            self.update()\n",
    "\n",
    "\n",
    "class AtomsData:\n",
    "\n",
    "    def __init__(self, X=None, traj=None, posgrad=False, cellgrad=False, **kwargs):\n",
    "        if X is not None:\n",
    "            self.X = X\n",
    "            assert self.check_content()\n",
    "        elif traj is not None:\n",
    "            self.X = []\n",
    "            from ase.io import Trajectory\n",
    "            t = Trajectory(traj, 'r')\n",
    "            self.X += [TorchAtoms(ase_atoms=atoms, **kwargs)\n",
    "                       for atoms in t]\n",
    "            t.close()\n",
    "        else:\n",
    "            raise RuntimeError('AtomsData invoked without any input')\n",
    "        self.posgrad = posgrad\n",
    "        self.cellgrad = cellgrad\n",
    "\n",
    "    def check_content(self):\n",
    "        return all([atoms.__class__ == TorchAtoms for atoms in self])\n",
    "\n",
    "    def numbers_set(self):\n",
    "        _num = set()\n",
    "        for atoms in self:\n",
    "            for n in set(atoms.numbers):\n",
    "                _num.add(n)\n",
    "        numbers = sorted(list(_num))\n",
    "        return numbers\n",
    "\n",
    "    def pairs_set(self, numbers=None):\n",
    "        if numbers is None:\n",
    "            numbers = self.numbers_set()\n",
    "        pairs = ([(a, b) for a, b in itertools.combinations(numbers, 2)] +\n",
    "                 [(a, a) for a in numbers])\n",
    "        return pairs\n",
    "\n",
    "    def apply(self, operation, *args, **kwargs):\n",
    "        for atoms in self.X:\n",
    "            getattr(atoms, operation)(*args, **kwargs)\n",
    "\n",
    "    def set_gpp(self, gpp, cutoff=None):\n",
    "        self.apply('update', cutoff=cutoff,\n",
    "                   descriptors=gpp.kern.kernels, forced=True)\n",
    "\n",
    "    def update(self, *args, **kwargs):\n",
    "        for atoms in self.X:\n",
    "            atoms.update(*args, **kwargs)\n",
    "\n",
    "    def update_nl_if_requires_grad(self, descriptors=None, forced=False):\n",
    "        if self.trainable:\n",
    "            for atoms in self.X:\n",
    "                atoms.update(descriptors=descriptors, forced=forced,\n",
    "                             posgrad=self.posgrad, cellgrad=self.cellgrad)\n",
    "\n",
    "    def set_per_atoms(self, quant, values):\n",
    "        vals = torch.split(values, split_size_or_sections=1)\n",
    "        for atoms, v in zip(*[self, vals]):\n",
    "            setattr(atoms, quant, v)\n",
    "\n",
    "    def set_per_atom(self, quant, values):\n",
    "        vals = torch.split(values, split_size_or_sections=self.natoms)\n",
    "        for atoms, v in zip(*[self, vals]):\n",
    "            setattr(atoms, quant, v)\n",
    "\n",
    "    def shake(self, **kwargs):\n",
    "        for atoms in self.X:\n",
    "            atoms.shake(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def natoms(self):\n",
    "        return [atoms.natoms for atoms in self]\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return [atoms.xyz for atoms in self.X if atoms.xyz.requires_grad]\n",
    "\n",
    "    @property\n",
    "    def trainable(self):\n",
    "        return self.posgrad or self.cellgrad\n",
    "\n",
    "    @property\n",
    "    def target_energy(self):\n",
    "        return torch.cat([atoms.target_energy.view(1) for atoms in self])\n",
    "\n",
    "    @property\n",
    "    def target_forces(self):\n",
    "        return torch.cat([atoms.target_forces for atoms in self])\n",
    "\n",
    "    def cat(self, attr):\n",
    "        return torch.cat([getattr(atoms, attr) for atoms in self])\n",
    "\n",
    "    def __iter__(self):\n",
    "        for atoms in self.X:\n",
    "            yield atoms\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        if isinstance(k, int):\n",
    "            return self.X[k]\n",
    "        else:\n",
    "            return AtomsData(self.X[k])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def to_traj(self, trajname, mode='w'):\n",
    "        from ase.io import Trajectory\n",
    "        t = Trajectory(trajname, mode)\n",
    "        for atoms in self:\n",
    "            t.write(atoms)\n",
    "        t.close()\n",
    "\n",
    "    def pick_random(self, n):\n",
    "        if n > len(self):\n",
    "            warnings.warn('n > len(AtomsData) in pick_random')\n",
    "        return AtomsData(X=[self[k] for k in torch.randperm(len(self))[:n]])\n",
    "\n",
    "    def append(self, others):\n",
    "        if id(self) == id(others):\n",
    "            _others = others.X[:]\n",
    "        else:\n",
    "            _others = iterable(others, ignore=TorchAtoms)\n",
    "        for atoms in _others:\n",
    "            assert atoms.__class__ == TorchAtoms\n",
    "            self.X += [atoms]\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if other.__class__ == AtomsData:\n",
    "            return AtomsData(X=self.X+other.X)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'AtomsData + {} is not implemented'.format(other.__class__))\n",
    "\n",
    "    def __iadd__(self, others):\n",
    "        self.append(others)\n",
    "        return self\n",
    "\n",
    "    def to_locals(self, keepids=False, device=None):\n",
    "        return LocalsData([loc.detach(keepids=keepids, device=device) for atoms in self for loc in atoms])\n",
    "\n",
    "    def sample_locals(self, size, keepids=False):\n",
    "        return LocalsData([random.choice(random.choice(self)).detach(keepids=keepids)\n",
    "                           for _ in range(size)])\n",
    "\n",
    "\n",
    "class LocalsData:\n",
    "\n",
    "    def __init__(self, X=None, traj=None):\n",
    "        if X is not None:\n",
    "            self.X = []\n",
    "            for loc in X:\n",
    "                assert loc.__class__ == Local\n",
    "                self.X += [loc]\n",
    "        elif traj is not None:\n",
    "            from ase.io import Trajectory\n",
    "            t = Trajectory(traj, 'r')\n",
    "            self.X = []\n",
    "            for atoms in t:\n",
    "                tatoms = TorchAtoms(ase_atoms=atoms)\n",
    "                if not np.allclose(tatoms.positions[0], np.zeros((3,))):\n",
    "                    raise RuntimeError\n",
    "                self.X += [tatoms.as_local()]\n",
    "            t.close()\n",
    "        else:\n",
    "            raise RuntimeError('LocalsData invoked without any input')\n",
    "        self.trainable = False\n",
    "\n",
    "    def stage(self, descriptors, device=None):\n",
    "        for loc in self:\n",
    "            loc.stage(descriptors, device=None)\n",
    "\n",
    "    def to_traj(self, trajname, mode='w'):\n",
    "        from ase.io import Trajectory\n",
    "        t = Trajectory(trajname, mode)\n",
    "        for loc in self:\n",
    "            t.write(loc.as_atoms())\n",
    "        t.close()\n",
    "\n",
    "    def __iter__(self):\n",
    "        for locs in self.X:\n",
    "            yield locs\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        if isinstance(k, int):\n",
    "            return self.X[k]\n",
    "        else:\n",
    "            return LocalsData(self.X[k])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def append(self, others, detach=False):\n",
    "        if id(self) == id(others):\n",
    "            _others = others.X[:]\n",
    "        else:\n",
    "            _others = iterable(others)\n",
    "        for loc in _others:\n",
    "            assert loc.__class__ == Local\n",
    "            self.X += [loc.detach() if detach else loc]\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if other.__class__ == LocalsData:\n",
    "            return LocalsData(X=self.X+other.X)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'AtomsData + {} is not implemented'.format(other.__class__))\n",
    "\n",
    "    def __iadd__(self, others):\n",
    "        self.append(others)\n",
    "        return self\n",
    "\n",
    "\n",
    "def sample_atoms(file, size=-1, chp=None, indices=None):\n",
    "    \"\"\"\n",
    "    If \n",
    "        A = sample_atomsdata('atoms.traj', size=n, chp='data.chp')\n",
    "        B = sample_atomsdata('data.chp')\n",
    "    then,\n",
    "        B = A\n",
    "    \"\"\"\n",
    "    from ase.io import Trajectory\n",
    "\n",
    "    # from traj\n",
    "    if file.endswith('.traj'):\n",
    "        traj = Trajectory(file)\n",
    "        if size > len(traj):\n",
    "            warnings.warn('size > len({})'.format(file))\n",
    "        if indices is None:\n",
    "            indices = np.random.permutation(len(traj))[:size].tolist()\n",
    "        if chp:\n",
    "            with open(chp, 'w') as ch:\n",
    "                ch.write(file+'\\n')\n",
    "                for k in indices:\n",
    "                    ch.write('{} '.format(k))\n",
    "        return AtomsData(X=[TorchAtoms(ase_atoms=traj[k]) for k in indices])\n",
    "\n",
    "    # from checkpoint\n",
    "    elif file.endswith('.chp'):\n",
    "        with open(file, 'r') as ch:\n",
    "            _file = ch.readline().strip()\n",
    "            _indices = [int(i) for i in ch.readline().split()]\n",
    "        return sample_atoms(_file, indices=_indices)\n",
    "\n",
    "    # other\n",
    "    else:\n",
    "        raise NotImplementedError('format {} is not recognized'.format(file))\n",
    "\n",
    "\n",
    "def diatomic(numbers, distances, pbc=False, cell=None):\n",
    "    from theforce.util.util import iterable\n",
    "    from itertools import combinations\n",
    "    if not hasattr(numbers[0], '__iter__'):\n",
    "        nums = ([(a, b) for a, b in combinations(set(numbers), 2)] +\n",
    "                [(a, a) for a in set(numbers)])\n",
    "    else:\n",
    "        nums = numbers\n",
    "    X = [TorchAtoms(positions=[[0., 0., 0.], [d, 0., 0.]], numbers=n, cell=cell, pbc=pbc)\n",
    "         for n in nums for d in iterable(distances)]\n",
    "    if len(X) > 1:\n",
    "        return AtomsData(X=X)\n",
    "    else:\n",
    "        return X[0]\n",
    "\n",
    "\n",
    "def namethem(descriptors, base='D'):\n",
    "    for i, desc in enumerate(descriptors):\n",
    "        desc.name = base+'_{}'.format(i)\n",
    "\n",
    "\n",
    "def example():\n",
    "    from theforce.similarity.pair import DistanceKernel\n",
    "    from theforce.regression.core import SquaredExp\n",
    "\n",
    "    kerns = [DistanceKernel(SquaredExp(), 10, 10),\n",
    "             DistanceKernel(SquaredExp(), 10, 18),\n",
    "             DistanceKernel(SquaredExp(), 18, 18)]\n",
    "    namethem(kerns)\n",
    "    xyz = np.stack(np.meshgrid([0, 1.5], [0, 1.5], [0, 1.5])\n",
    "                   ).reshape(3, -1).transpose()\n",
    "    numbers = 4*[10] + 4*[18]\n",
    "    atoms = TorchAtoms(positions=xyz, numbers=numbers,\n",
    "                       cutoff=3.0, descriptors=kerns)\n",
    "\n",
    "    other = atoms.copy()\n",
    "    print(other == atoms)\n",
    "\n",
    "    for loc in atoms:\n",
    "        print(loc.as_atoms().as_local() == loc.detach())\n",
    "\n",
    "    empty = TorchAtoms(positions=[(0, 0, 0)], cutoff=3.)\n",
    "    empty[0].detach()._r\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
