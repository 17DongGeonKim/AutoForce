{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theforce.similarity.similarity import SimilarityKernel\n",
    "from theforce.math.soap import SeriesSoap\n",
    "from theforce.util.util import iterable\n",
    "import torch\n",
    "\n",
    "\n",
    "class SoapKernel(SimilarityKernel):\n",
    "    \"\"\"\n",
    "    Notes:\n",
    "    1. the descriptor vectors for all the species will be concatenated, \n",
    "    and then will be normalized i.e. \"normalize=False\" when constructing\n",
    "    the descriptor for a single species.\n",
    "    2. \"scale\" and \"actual\" keywords in the SeriesSoap control the resolution.\n",
    "    3. Each species can have its own SeriesSoap object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel, a, b, lmax, nmax, radial):\n",
    "        super().__init__(kernel)\n",
    "        self.a = a\n",
    "        self.b = sorted(iterable(b))\n",
    "        self.descriptor = SeriesSoap(lmax, nmax, radial, scale=None, actual=False,\n",
    "                                     normalize=False, cutcorners=0, symm=False)\n",
    "        self.soapdim = self.descriptor.mask.sum()*(lmax+1)\n",
    "        self.dim = len(self.b)*self.soapdim\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return super().state_args + ', {}, {}, {}, {}, {}'.format(\n",
    "            self.a, self.b, self.descriptor.abs.ylm.lmax, self.descriptor.abs.nmax,\n",
    "            self.descriptor.abs.radial.state)\n",
    "\n",
    "    def precalculate(self, loc):\n",
    "        idx = torch.arange(loc._j.size(0)).long()\n",
    "        zero = torch.zeros(self.soapdim, loc._j.size(0), 3)\n",
    "        dat = []\n",
    "        for b in self.b:\n",
    "            loc.select(self.a, b, bothways=True, in_place=True)\n",
    "            d, _grad = self.descriptor(loc.r)\n",
    "            grad = zero.index_add(1, idx[loc._m], _grad)\n",
    "            dat += [(d, grad)]\n",
    "        d, grad = (torch.cat(a) for a in zip(*dat))\n",
    "        # normalize\n",
    "        norm = d.norm()\n",
    "        if norm > 0.0:\n",
    "            d = d/norm\n",
    "            grad = grad/norm\n",
    "            grad = (grad - d[..., None, None] *\n",
    "                    (d[..., None, None] * grad).sum(dim=0))\n",
    "        # save\n",
    "        data = {'value': d[None], 'grad': grad, 'j': loc._j}\n",
    "        self.save_for_later(loc, data)\n",
    "\n",
    "    def func(self, p, q):\n",
    "        d = self.saved(p, 'value')\n",
    "        dd = self.saved(q, 'value')\n",
    "        c = self.kern(d, dd)\n",
    "        return c.sum().view(1, 1)\n",
    "\n",
    "    def leftgrad(self, p, q):\n",
    "        d = self.saved(p, 'value')\n",
    "        dd = self.saved(q, 'value')\n",
    "        c = self.kern.leftgrad(d, dd)\n",
    "        g = []\n",
    "        for i, loc in enumerate(p):\n",
    "            grad = self.saved(loc, 'grad')\n",
    "            j = self.saved(loc, 'j')\n",
    "            t = ((c[:, j, :][..., None]*grad[..., None, :]).sum(dim=(0, 1, 2)) -\n",
    "                 (c[:, i, :][..., None]*grad.sum(dim=1)[..., None, :]).sum(dim=(0, 1)))\n",
    "            g += [t]\n",
    "        g = torch.stack(g)\n",
    "        return g.view(-1, 1)\n",
    "\n",
    "    def rightgrad(self, p, q):\n",
    "        d = self.saved(p, 'value')\n",
    "        dd = self.saved(q, 'value')\n",
    "        c = self.kern.rightgrad(d, dd)\n",
    "        g = []\n",
    "        for i, loc in enumerate(q):\n",
    "            grad = self.saved(loc, 'grad')\n",
    "            j = self.saved(loc, 'j')\n",
    "            t = ((c[:, :, j][..., None]*grad[:, None, :]).sum(dim=(0, 1, 2)) -\n",
    "                 (c[:, :, i][..., None]*grad.sum(dim=1)[:, None, :]).sum(dim=(0, 1)))\n",
    "            g += [t]\n",
    "        g = torch.stack(g)\n",
    "        return g.view(1, -1)\n",
    "\n",
    "    def gradgrad(self, p, q):\n",
    "        raise NotImplementedError('Not defined yet')\n",
    "\n",
    "    def gradgraddiag(self, p):\n",
    "        raise NotImplementedError('Not defined yet')\n",
    "\n",
    "\n",
    "def example():\n",
    "    from theforce.regression.kernel import Positive, DotProd\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "    kern = (Positive(1.0, requires_grad=True) *\n",
    "            (DotProd() + Positive(0.01, requires_grad=True))**4)\n",
    "    soap = SoapKernel(kern, 10, (18, 10), 2, 2, PolyCut(3.0))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
