{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theforce.similarity.similarity import SimilarityKernel\n",
    "from theforce.math.soap import RealSeriesSoap, TailoredSoap, NormalizedSoap, MultiSoap\n",
    "from theforce.util.util import iterable\n",
    "import torch\n",
    "\n",
    "\n",
    "class SoapKernel(SimilarityKernel):\n",
    "\n",
    "    def __init__(self, kernel, a, b, lmax, nmax, radial, atomic_unit=None):\n",
    "        super().__init__(kernel)\n",
    "        self.a = a\n",
    "        self.b = sorted(iterable(b))\n",
    "        self.descriptor = MultiSoap([TailoredSoap(RealSeriesSoap(\n",
    "            lmax, nmax, radial, atomic_unit=atomic_unit)) for _ in self.b])\n",
    "        self.dim = self.descriptor.dim\n",
    "        self._args = '{}, {}, {}, {}, {}, atomic_unit={}'.format(\n",
    "            a, b, lmax, nmax, radial.state, atomic_unit)\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return super().state_args + ', ' + self._args\n",
    "\n",
    "    def precalculate(self, loc):\n",
    "        if (self.a == loc._a.unique()).all():\n",
    "            masks = [loc.select(self.a, b, bothways=True) for b in self.b]\n",
    "            d, grad = self.descriptor(loc._r, masks, grad=True)\n",
    "            d = d[None]\n",
    "            grad = torch.cat([grad, -grad.sum(dim=1, keepdim=True)], dim=1)\n",
    "            j = torch.cat([loc._j, loc._i.unique()])\n",
    "            if j.numel() > 0:\n",
    "                empty = torch.tensor([False])\n",
    "            else:\n",
    "                empty = torch.tensor([True])\n",
    "                del d, grad\n",
    "                d = torch.zeros(0, self.dim)\n",
    "                grad = torch.zeros(self.dim, 0, 3)\n",
    "        else:\n",
    "            empty = torch.tensor([True])\n",
    "            d = torch.zeros(0, self.dim)\n",
    "            grad = torch.zeros(self.dim, 0, 3)\n",
    "            j = torch.empty(0).long()\n",
    "        # save\n",
    "        data = {'value': d, 'grad': grad, 'j': j, 'empty': empty}\n",
    "        self.save_for_later(loc, data)\n",
    "\n",
    "    def func(self, p, q):\n",
    "        d = self.saved(p, 'value')\n",
    "        dd = self.saved(q, 'value')\n",
    "        c = self.kern(d, dd)\n",
    "        return c.sum().view(1, 1)\n",
    "\n",
    "    def leftgrad(self, p, q):\n",
    "        d = self.saved(p, 'value')\n",
    "        dd = self.saved(q, 'value')\n",
    "        empty = self.saved(p, 'empty')\n",
    "        c = self.kern.leftgrad(d, dd)\n",
    "        g = torch.zeros(p.natoms, 3)\n",
    "        _i = 0\n",
    "        for i, loc in enumerate(p):\n",
    "            if not empty[i]:\n",
    "                grad = self.saved(loc, 'grad')\n",
    "                j = self.saved(loc, 'j')\n",
    "                t = (c[:, _i][..., None, None]*grad[:, None]).sum(dim=(0, 1))\n",
    "                g = g.index_add(0, j, t)\n",
    "                _i += 1\n",
    "        return g.view(-1, 1)\n",
    "\n",
    "    def rightgrad(self, p, q):\n",
    "        d = self.saved(p, 'value')\n",
    "        dd = self.saved(q, 'value')\n",
    "        empty = self.saved(q, 'empty')\n",
    "        c = self.kern.rightgrad(d, dd)\n",
    "        g = torch.zeros(q.natoms, 3)\n",
    "        _i = 0\n",
    "        for i, loc in enumerate(q):\n",
    "            if not empty[i]:\n",
    "                grad = self.saved(loc, 'grad')\n",
    "                j = self.saved(loc, 'j')\n",
    "                t = (c[..., _i][..., None, None]*grad[:, None]).sum(dim=(0, 1))\n",
    "                g = g.index_add(0, j, t)\n",
    "                _i += 1\n",
    "        return g.view(1, -1)\n",
    "\n",
    "    def gradgrad(self, p, q):\n",
    "        raise NotImplementedError('Not defined yet')\n",
    "\n",
    "    def gradgraddiag(self, p):\n",
    "        raise NotImplementedError('Not defined yet')\n",
    "\n",
    "\n",
    "def test_grad():\n",
    "    from theforce.descriptor.atoms import namethem\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "    from theforce.regression.kernel import Positive, DotProd, Normed\n",
    "    from theforce.regression.stationary import RBF\n",
    "    from theforce.descriptor.atoms import TorchAtoms, AtomsData\n",
    "    import numpy as np\n",
    "    torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "    # create kernel\n",
    "    kern = Positive(1.0) * Normed(DotProd())**4\n",
    "    #kern = RBF()\n",
    "    soap = SoapKernel(kern, 10, (18, 10), 2, 2, PolyCut(3.0))\n",
    "    namethem([soap])\n",
    "\n",
    "    # create atomic systems\n",
    "    # Note that when one of the displacement vectors becomes is exactly along the z-axis\n",
    "    # because of singularity some inconsistensies exist with autograd.\n",
    "    # For this reason we add a small random number to positions, until that bug is fixed.\n",
    "    cell = np.ones(3)*10\n",
    "    positions = np.array([(-1., 0., 0.), (1., 0., 0.),\n",
    "                          (0., -1., 0.), (0., 1., 0.),\n",
    "                          (0., 0., -1.), (0., 0., 1.),\n",
    "                          (0., 0., 0.)]) + cell/2 + np.random.uniform(-0.1, 0.1, size=(7, 3))\n",
    "\n",
    "    b = TorchAtoms(positions=positions, numbers=3*[10]+3*[18]+[10], cell=cell,\n",
    "                   pbc=True, cutoff=3.0, descriptors=[soap])\n",
    "    # make natoms different in a, b. P.S. add an isolated atom.\n",
    "    _pos = np.concatenate([positions, [[0., 0, 0]]])\n",
    "    a = TorchAtoms(positions=_pos, numbers=2*[10, 18, 10]+[18, 10], cell=cell,\n",
    "                   pbc=True, cutoff=3.0, descriptors=[soap])\n",
    "    a.update(posgrad=True, forced=True)\n",
    "    b.update(posgrad=True, forced=True)\n",
    "    soap([a], [b]).backward()\n",
    "\n",
    "    test_left = a.xyz.grad.allclose(soap.leftgrad(a, b).view(-1, 3))\n",
    "    max_left = (a.xyz.grad - soap.leftgrad(a, b).view(-1, 3)).max()\n",
    "    print(\"leftgrad: {}  \\t max diff: {}\".format(test_left, max_left))\n",
    "    test_right = b.xyz.grad.allclose(soap.rightgrad(a, b).view(-1, 3))\n",
    "    max_right = (b.xyz.grad - soap.rightgrad(a, b).view(-1, 3)).max()\n",
    "    print(\"rightgrad: {} \\t max diff: {}\".format(test_right, max_right))\n",
    "\n",
    "\n",
    "def example():\n",
    "    from theforce.regression.kernel import Positive, DotProd, Mul, Add, Pow\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "    kern = (Positive(1.0, requires_grad=True) *\n",
    "            (DotProd() + Positive(0.01, requires_grad=True))**0.1)\n",
    "    soap = SoapKernel(kern, 10, (18, 10), 2, 2, PolyCut(3.0))\n",
    "    assert eval(soap.state).state == soap.state\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    example()\n",
    "    test_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
