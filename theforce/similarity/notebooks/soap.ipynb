{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theforce.similarity.similarity import SimilarityKernel\n",
    "from theforce.math.soap import RealSeriesSoap, TailoredSoap, NormalizedSoap, MultiSoap\n",
    "from theforce.util.util import iterable\n",
    "import torch\n",
    "\n",
    "\n",
    "class SoapKernel(SimilarityKernel):\n",
    "\n",
    "    def __init__(self, kernel, a, b, lmax, nmax, radial, atomic_unit=None):\n",
    "        super().__init__(kernel)\n",
    "        self.a = a\n",
    "        self.b = sorted(iterable(b))\n",
    "        if atomic_unit == None or type(atomic_unit) == float or type(atomic_unit) == int:\n",
    "            units = {_b: atomic_unit for _b in self.b}\n",
    "        elif type(atomic_unit) == list or type(atomic_unit) == tuple:\n",
    "            # if au is a list or a tuple, so should b! self.b is sorted so we shoud use (arg) b.\n",
    "            units = {_b: au for _b, au in zip(*[b, atomic_unit])}\n",
    "        elif type(atomic_unit) == dict:\n",
    "            units = {_b: atomic_unit[(a, _b)] for _b in self.b}\n",
    "        self.descriptor = MultiSoap([TailoredSoap(RealSeriesSoap(\n",
    "            lmax, nmax, radial, atomic_unit=units[_b])) for _b in self.b])\n",
    "        self.dim = self.descriptor.dim\n",
    "        self._args = '{}, {}, {}, {}, {}, atomic_unit={}'.format(\n",
    "            a, b, lmax, nmax, radial.state, atomic_unit)\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return super().state_args + ', ' + self._args\n",
    "\n",
    "    def precalculate(self, loc):\n",
    "        if (self.a == loc._a.unique()).all():\n",
    "            masks = [loc.select(self.a, b, bothways=True) for b in self.b]\n",
    "            d, grad = self.descriptor(loc._r, masks, grad=True)\n",
    "            d = d[None]\n",
    "            grad = torch.cat([grad, -grad.sum(dim=1, keepdim=True)], dim=1)\n",
    "            j = torch.cat([loc._j, loc._i.unique()])\n",
    "            if j.numel() > 0:\n",
    "                empty = torch.tensor([False])\n",
    "            else:\n",
    "                empty = torch.tensor([True])\n",
    "                del d, grad\n",
    "                d = torch.zeros(0, self.dim)\n",
    "                grad = torch.zeros(self.dim, 0, 3)\n",
    "        else:\n",
    "            empty = torch.tensor([True])\n",
    "            d = torch.zeros(0, self.dim)\n",
    "            grad = torch.zeros(self.dim, 0, 3)\n",
    "            j = torch.empty(0).long()\n",
    "        # save\n",
    "        data = {'value': d, 'grad': grad, 'j': j, 'empty': empty}\n",
    "        self.save_for_later(loc, data)\n",
    "\n",
    "    def get_func(self, p, q):\n",
    "        d = self.saved(p, 'value')\n",
    "        dd = self.saved(q, 'value')\n",
    "        c = self.kern(d, dd)\n",
    "        return c.sum().view(1, 1)\n",
    "\n",
    "    def get_leftgrad(self, p, q):\n",
    "        d = self.saved(p, 'value')\n",
    "        dd = self.saved(q, 'value')\n",
    "        empty = self.saved(p, 'empty')\n",
    "        c = self.kern.leftgrad(d, dd)\n",
    "        g = torch.zeros(p.natoms, 3)\n",
    "        _i = 0\n",
    "        for i, loc in enumerate(p):\n",
    "            if not empty[i]:\n",
    "                grad = self.saved(loc, 'grad')\n",
    "                j = self.saved(loc, 'j')\n",
    "                t = (c[:, _i][..., None, None]*grad[:, None]).sum(dim=(0, 1))\n",
    "                g = g.index_add(0, j, t)\n",
    "                _i += 1\n",
    "        return g.view(-1, 1)\n",
    "\n",
    "    def get_rightgrad(self, p, q):\n",
    "        d = self.saved(p, 'value')\n",
    "        dd = self.saved(q, 'value')\n",
    "        empty = self.saved(q, 'empty')\n",
    "        c = self.kern.rightgrad(d, dd)\n",
    "        g = torch.zeros(q.natoms, 3)\n",
    "        _i = 0\n",
    "        for i, loc in enumerate(q):\n",
    "            if not empty[i]:\n",
    "                grad = self.saved(loc, 'grad')\n",
    "                j = self.saved(loc, 'j')\n",
    "                t = (c[..., _i][..., None, None]*grad[:, None]).sum(dim=(0, 1))\n",
    "                g = g.index_add(0, j, t)\n",
    "                _i += 1\n",
    "        return g.view(1, -1)\n",
    "\n",
    "    def graddata(self, p):\n",
    "        empty = self.saved(p, 'empty')\n",
    "        i = []\n",
    "        j = []\n",
    "        grad = []\n",
    "        _i = 0\n",
    "        for k, loc in enumerate(p):\n",
    "            if not empty[k]:\n",
    "                grad += [self.saved(loc, 'grad')]\n",
    "                _j = self.saved(loc, 'j')\n",
    "                j += [_j]\n",
    "                i += [torch.full_like(_j, _i)]\n",
    "                _i += 1\n",
    "        i = torch.cat(i)\n",
    "        j = torch.cat(j)\n",
    "        grad = torch.cat(grad, dim=1)\n",
    "        return i, j, grad\n",
    "\n",
    "    def get_gradgrad(self, p, q):\n",
    "        d1 = self.saved(p, 'value')\n",
    "        d2 = self.saved(q, 'value')\n",
    "        c = self.kern.gradgrad(d1, d2)\n",
    "        i1, j1, grad1 = self.graddata(p)\n",
    "        i2, j2, grad2 = self.graddata(q)\n",
    "        a = ((grad1[:, None, :, None, :, None]*grad2[None, :, None, :, None]))\n",
    "        b = c.index_select(2, i1).index_select(3, i2)[..., None, None]\n",
    "        g = (a*b).sum(dim=(0, 1)).permute(0, 2, 1, 3)\n",
    "        f = torch.zeros(p.natoms, 3, j2.size(0), 3).index_add(0, j1, g)\n",
    "        h = torch.zeros(p.natoms, 3, q.natoms, 3).index_add(2, j2, f)\n",
    "        return h.view(p.natoms*3, q.natoms*3)\n",
    "\n",
    "    def get_gradgraddiag(self, p):\n",
    "        d = self.saved(p, 'value')\n",
    "        empty = self.saved(p, 'empty')\n",
    "        c = self.kern.gradgrad(d, d)\n",
    "        i, j, grad = self.graddata(p)\n",
    "        h = torch.zeros(p.natoms, 3)\n",
    "        for jj in torch.unique(j):\n",
    "            m = j == jj\n",
    "            ii = i[m]\n",
    "            a = grad[:, m][:, None, :, None]*grad[:, m][None, :, None]\n",
    "            b = c.index_select(2, ii).index_select(3, ii).unsqueeze(-1)\n",
    "            f = (a*b).sum(dim=(0, 1, 2, 3)).view(1, 3)\n",
    "            h[jj] = f\n",
    "        return h.view(-1)\n",
    "\n",
    "\n",
    "class NormedSoapKernel(SoapKernel):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.descriptor = NormalizedSoap(self.descriptor)\n",
    "\n",
    "\n",
    "def test_grad():\n",
    "    from theforce.descriptor.atoms import namethem\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "    from theforce.regression.kernel import Positive, DotProd, Normed\n",
    "    from theforce.regression.stationary import RBF\n",
    "    from theforce.descriptor.atoms import TorchAtoms, AtomsData\n",
    "    import numpy as np\n",
    "    torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "    # create kernel\n",
    "    kern = Positive(1.0) * Normed(DotProd())**4\n",
    "    #kern = RBF()\n",
    "    #soap = SoapKernel(kern, 10, (18, 10), 2, 2, PolyCut(3.0))\n",
    "    soap = NormedSoapKernel(DotProd()**4, 10, (18, 10), 2, 2, PolyCut(3.0))\n",
    "    namethem([soap])\n",
    "\n",
    "    # create atomic systems\n",
    "    # Note that when one of the displacement vectors becomes is exactly along the z-axis\n",
    "    # because of singularity some inconsistensies exist with autograd.\n",
    "    # For this reason we add a small random number to positions, until that bug is fixed.\n",
    "    cell = np.ones(3)*10\n",
    "    positions = np.array([(-1., 0., 0.), (1., 0., 0.),\n",
    "                          (0., -1., 0.), (0., 1., 0.),\n",
    "                          (0., 0., -1.), (0., 0., 1.0),\n",
    "                          (0., 0., 0.)]) + cell/2 + np.random.uniform(-0.1, 0.1, size=(7, 3))\n",
    "\n",
    "    b = TorchAtoms(positions=positions, numbers=3*[10]+3*[18]+[10], cell=cell,\n",
    "                   pbc=True, cutoff=3.0, descriptors=[soap])\n",
    "    # make natoms different in a, b. P.S. add an isolated atom.\n",
    "    _pos = np.concatenate([positions, [[0., 0., 0.], [3., 5., 5.]]])\n",
    "    a = TorchAtoms(positions=_pos, numbers=2*[10, 18, 10]+[18, 10, 18], cell=cell,\n",
    "                   pbc=True, cutoff=3.0, descriptors=[soap])\n",
    "\n",
    "    # left/right-grad\n",
    "    a.update(posgrad=True, forced=True)\n",
    "    b.update(posgrad=True, forced=True)\n",
    "    soap([a], [b]).backward()\n",
    "    test_left = a.xyz.grad.allclose(soap.leftgrad(a, b).view(-1, 3))\n",
    "    max_left = (a.xyz.grad - soap.leftgrad(a, b).view(-1, 3)).max()\n",
    "    print(\"leftgrad: {}  \\t max diff: {}\".format(test_left, max_left))\n",
    "    test_right = b.xyz.grad.allclose(soap.rightgrad(a, b).view(-1, 3))\n",
    "    max_right = (b.xyz.grad - soap.rightgrad(a, b).view(-1, 3)).max()\n",
    "    print(\"rightgrad: {} \\t max diff: {}\".format(test_right, max_right))\n",
    "\n",
    "    # gradgrad-left\n",
    "    a.update(posgrad=True, forced=True)\n",
    "    b.update(posgrad=True, forced=True)\n",
    "    (soap.leftgrad(a, b).view(-1, 3)*a.xyz).sum().backward()\n",
    "    v1 = a.xyz.grad.data\n",
    "    a.update(posgrad=True, forced=True)\n",
    "    b.update(posgrad=True, forced=True)\n",
    "    (soap.gradgrad(a, b)*a.xyz.view(-1)[:, None]).sum().backward()\n",
    "    v2 = a.xyz.grad.data\n",
    "    print('gradgrad-left: {}'.format(v1.allclose(v2)))\n",
    "\n",
    "    # gradgrad-right\n",
    "    a.update(posgrad=True, forced=True)\n",
    "    b.update(posgrad=True, forced=True)\n",
    "    (soap.rightgrad(a, b).view(-1, 3)*b.xyz).sum().backward()\n",
    "    v1 = b.xyz.grad.data\n",
    "    a.update(posgrad=True, forced=True)\n",
    "    b.update(posgrad=True, forced=True)\n",
    "    (soap.gradgrad(a, b)*b.xyz.view(-1)[None]).sum().backward()\n",
    "    v2 = b.xyz.grad.data\n",
    "    print('gradgradright: {}'.format(v1.allclose(v2)))\n",
    "\n",
    "    # gradgraddiag\n",
    "    test_diag = soap.gradgrad(a, a).diag().allclose(soap.gradgraddiag(a))\n",
    "    print('gradgraddiag: {}'.format(test_diag))\n",
    "\n",
    "\n",
    "def example():\n",
    "    from theforce.regression.kernel import Positive, DotProd, Mul, Add, Pow\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "    kern = (Positive(1.0, requires_grad=True) *\n",
    "            (DotProd() + Positive(0.01, requires_grad=True))**0.1)\n",
    "    soap = SoapKernel(kern, 10, (18, 10), 2, 2, PolyCut(3.0))\n",
    "    assert eval(soap.state).state == soap.state\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    example()\n",
    "    test_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
