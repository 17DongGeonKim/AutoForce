{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theforce.similarity.similarity import SimilarityKernel\n",
    "from theforce.math.soap import SeriesSoap\n",
    "from theforce.util.util import iterable\n",
    "import torch\n",
    "\n",
    "\n",
    "class SoapKernel(SimilarityKernel):\n",
    "    \"\"\"\n",
    "    Notes:\n",
    "    1. the descriptor vectors for all the species will be concatenated, \n",
    "       and then will be normalized (if \"normalize=True\" in init).  \n",
    "       Thus always \"normalize=False\" when constructing the descriptor \n",
    "       for a single species.\n",
    "    2. \"unit\" and \"modify\" keywords in the SeriesSoap control the resolution.\n",
    "    3. Each species can have its own SeriesSoap object.\n",
    "    4. Normalization leads to discontinuities in gradients when separation \n",
    "       of a pair is equal to cutoff.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel, a, b, lmax, nmax, radial, normalize=True):\n",
    "        super().__init__(kernel)\n",
    "        self.a = a\n",
    "        self.b = sorted(iterable(b))\n",
    "        self.descriptor = SeriesSoap(lmax, nmax, radial, unit=None, modify=None, normalize=False,\n",
    "                                     cutcorners=0, symm=False)\n",
    "        self.normalize = normalize\n",
    "        self.soapdim = self.descriptor.mask.sum()*(lmax+1)\n",
    "        self.dim = len(self.b)*self.soapdim\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return super().state_args + ', {}, {}, {}, {}, {}, normalize={}'.format(\n",
    "            self.a, self.b, self.descriptor.abs.ylm.lmax, self.descriptor.abs.nmax,\n",
    "            self.descriptor.abs.radial.state, self.normalize)\n",
    "\n",
    "    def precalculate(self, loc):\n",
    "        if (self.a == loc._a.unique()).all():\n",
    "            idx = torch.arange(loc._j.size(0)).long()\n",
    "            zero = torch.zeros(self.soapdim, loc._j.size(0), 3)\n",
    "            dat = []\n",
    "            for b in self.b:\n",
    "                loc.select(self.a, b, bothways=True, in_place=True)\n",
    "                d, _grad = self.descriptor(loc.r)\n",
    "                grad = zero.index_add(1, idx[loc._m], _grad)\n",
    "                dat += [(d, grad)]\n",
    "            d, grad = (torch.cat(a) for a in zip(*dat))\n",
    "            if self.normalize:\n",
    "                norm = d.norm()\n",
    "                if norm > 0.0:\n",
    "                    d = d/norm\n",
    "                    grad = grad/norm\n",
    "                    grad = (grad - d[..., None, None] *\n",
    "                            (d[..., None, None] * grad).sum(dim=0))\n",
    "            grad = torch.cat([grad, -grad.sum(dim=1, keepdim=True)], dim=1)\n",
    "            j = torch.cat([loc._j, loc._i.unique()])\n",
    "            a = torch.ones(1)\n",
    "        else:\n",
    "            d = torch.zeros(self.dim)\n",
    "            grad = torch.zeros(self.dim, 0, 3)\n",
    "            j = torch.empty(0).long()\n",
    "            a = torch.zeros(1)\n",
    "        # save\n",
    "        data = {'value': d[None], 'grad': grad, 'j': j, 'a': a}\n",
    "        self.save_for_later(loc, data)\n",
    "\n",
    "    def func(self, p, q):\n",
    "        d = self.saved(p, 'value')\n",
    "        dd = self.saved(q, 'value')\n",
    "        a = self.saved(p, 'a')\n",
    "        aa = self.saved(q, 'a')\n",
    "        zo = a[:, None]*aa[None]\n",
    "        c = self.kern(d, dd) * zo\n",
    "        return c.sum().view(1, 1)\n",
    "\n",
    "    def leftgrad(self, p, q):\n",
    "        d = self.saved(p, 'value')\n",
    "        dd = self.saved(q, 'value')\n",
    "        a = self.saved(p, 'a')\n",
    "        aa = self.saved(q, 'a')\n",
    "        zo = a[:, None]*aa[None]\n",
    "        c = self.kern.leftgrad(d, dd) * zo\n",
    "        g = torch.zeros(p.natoms, 3)\n",
    "        for i, loc in enumerate(p):\n",
    "            grad = self.saved(loc, 'grad')\n",
    "            j = self.saved(loc, 'j')\n",
    "            t = (c[:, i][..., None, None]*grad[:, None]).sum(dim=(0, 1))\n",
    "            g = g.index_add(0, j, t)\n",
    "        return g.view(-1, 1)\n",
    "\n",
    "    def rightgrad(self, p, q):\n",
    "        d = self.saved(p, 'value')\n",
    "        dd = self.saved(q, 'value')\n",
    "        a = self.saved(p, 'a')\n",
    "        aa = self.saved(q, 'a')\n",
    "        zo = a[:, None]*aa[None]\n",
    "        c = self.kern.rightgrad(d, dd) * zo\n",
    "        g = torch.zeros(p.natoms, 3)\n",
    "        for i, loc in enumerate(q):\n",
    "            grad = self.saved(loc, 'grad')\n",
    "            j = self.saved(loc, 'j')\n",
    "            t = (c[..., i][..., None, None]*grad[:, None]).sum(dim=(0, 1))\n",
    "            g = g.index_add(0, j, t)\n",
    "        return g.view(1, -1)\n",
    "\n",
    "    def gradgrad(self, p, q):\n",
    "        raise NotImplementedError('Not defined yet')\n",
    "\n",
    "    def gradgraddiag(self, p):\n",
    "        raise NotImplementedError('Not defined yet')\n",
    "\n",
    "\n",
    "def test_grad():\n",
    "    from theforce.descriptor.atoms import namethem\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "    from theforce.regression.kernel import Positive, DotProd\n",
    "    from theforce.regression.stationary import RBF\n",
    "    from theforce.descriptor.atoms import TorchAtoms, AtomsData\n",
    "    import numpy as np\n",
    "    torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "    # create kernel\n",
    "    kern = Positive(1.0) * (DotProd()+Positive(0.01))**0.1\n",
    "    #kern = RBF()\n",
    "    soap = SoapKernel(kern, 10, (18, 10), 2, 2, PolyCut(3.0), normalize=True)\n",
    "    namethem([soap])\n",
    "\n",
    "    # create atomic systems\n",
    "    # Note that when one of the displacement vectors becomes is exactly along the z-axis\n",
    "    # because of singularity some inconsistensies exist with autograd.\n",
    "    # For this reason we add a small random number to positions, until that bug is fixed.\n",
    "    cell = np.ones(3)*10\n",
    "    positions = np.array([(-1., 0., 0.), (1., 0., 0.),\n",
    "                          (0., -1., 0.), (0., 1., 0.),\n",
    "                          (0., 0., -1.), (0., 0., 1.),\n",
    "                          (0., 0., 0.)]) + cell/2 + np.random.uniform(-0.1, 0.1, size=(7, 3))\n",
    "\n",
    "    b = TorchAtoms(positions=positions, numbers=3*[10]+3*[18]+[10], cell=cell,\n",
    "                   pbc=True, cutoff=3.0, descriptors=[soap])\n",
    "    a = TorchAtoms(positions=positions, numbers=2*[10, 18, 10]+[18], cell=cell,\n",
    "                   pbc=True, cutoff=3.0, descriptors=[soap])\n",
    "    a.update(posgrad=True, forced=True)\n",
    "    b.update(posgrad=True, forced=True)\n",
    "    soap([a], [b]).backward()\n",
    "\n",
    "    test_left = a.xyz.grad.allclose(soap.leftgrad(a, b).view(-1, 3))\n",
    "    max_left = (a.xyz.grad - soap.leftgrad(a, b).view(-1, 3)).max()\n",
    "    print(\"leftgrad: {}  \\t max diff: {}\".format(test_left, max_left))\n",
    "    test_right = b.xyz.grad.allclose(soap.rightgrad(a, b).view(-1, 3))\n",
    "    max_right = (b.xyz.grad - soap.rightgrad(a, b).view(-1, 3)).max()\n",
    "    print(\"rightgrad: {} \\t max diff: {}\".format(test_right, max_right))\n",
    "\n",
    "\n",
    "def example():\n",
    "    from theforce.regression.kernel import Positive, DotProd\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "    kern = (Positive(1.0, requires_grad=True) *\n",
    "            (DotProd() + Positive(0.01, requires_grad=True))**0.1)\n",
    "    soap = SoapKernel(kern, 10, (18, 10), 2, 2, PolyCut(3.0))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    example()\n",
    "    test_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
