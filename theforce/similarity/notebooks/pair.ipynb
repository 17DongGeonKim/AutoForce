{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theforce.similarity.similarity import SimilarityKernel\n",
    "from theforce.regression.algebra import positive, free_form\n",
    "from torch import zeros, cat, stack\n",
    "from theforce.util.util import iterable\n",
    "import torch\n",
    "from torch.nn import Module, Parameter\n",
    "\n",
    "\n",
    "class PairSimilarityKernel(SimilarityKernel):\n",
    "\n",
    "    def __init__(self, kernel, a, b):\n",
    "        super().__init__(kernel)\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return super().state_args + ', {}, {}'.format(self.a, self.b)\n",
    "\n",
    "    def descriptor(self, r):\n",
    "        raise NotImplementedError(\n",
    "            'descriptor shoud be implemented in a child class')\n",
    "\n",
    "    def save_for_later(self, loc, keyvals):\n",
    "        for key, val in keyvals.items():\n",
    "            setattr(loc, self.name+'_'+key, val)\n",
    "\n",
    "    def saved(self, atoms_or_loc, key):\n",
    "        return getattr(atoms_or_loc, self.name+'_'+key)\n",
    "\n",
    "    def precalculate(self, loc):\n",
    "        loc.select(self.a, self.b, bothways=True)\n",
    "        d, grad = self.descriptor(loc.r)\n",
    "        data = {'diag_value': d, 'diag_grad': grad}\n",
    "        self.save_for_later(loc, data)\n",
    "        m = (loc.j > loc.i)\n",
    "        if self.a == self.b:\n",
    "            m = m | ((loc.j == loc.i) & loc.lex)\n",
    "        data = {'value': d[m], 'grad': grad[m], 'i': loc.i[m], 'j': loc.j[m]}\n",
    "        self.save_for_later(loc, data)\n",
    "        if hasattr(self, 'factor'):\n",
    "            fac, facgrad = self.factor(d)\n",
    "            self.save_for_later(loc, {'diag_fac': fac, 'diag_facgrad': facgrad,\n",
    "                                      'fac': fac[m], 'facgrad': facgrad[m]})\n",
    "            self.has_factor = True\n",
    "            if hasattr(self.factor, 'state'):\n",
    "                self.has_state = True\n",
    "                self.save_for_later(loc, {'m': m, 'state': self.factor.state})\n",
    "            else:\n",
    "                self.has_state = False\n",
    "        else:\n",
    "            self.has_factor = False\n",
    "\n",
    "    def recalculate(self, atoms_or_loc):\n",
    "        if self.has_factor:\n",
    "            if self.has_state:\n",
    "                state = self.factor.state\n",
    "                for loc in iterable(atoms_or_loc):\n",
    "                    if state != self.saved(loc, 'state'):\n",
    "                        d = self.saved(loc, 'diag_value')\n",
    "                        m = self.saved(loc, 'm')\n",
    "                        fac, facgrad = self.factor(d)\n",
    "                        self.save_for_later(loc, {'diag_fac': fac, 'diag_facgrad': facgrad,\n",
    "                                                  'fac': fac[m], 'facgrad': facgrad[m],\n",
    "                                                  'state': self.factor.state})\n",
    "\n",
    "    def func(self, p, q):\n",
    "        d = self.saved(p, 'value')\n",
    "        dd = self.saved(q, 'value')\n",
    "        c = self.kern(d, dd)\n",
    "        if self.has_factor:\n",
    "            self.recalculate(p)\n",
    "            self.recalculate(q)\n",
    "            f = self.saved(p, 'fac')\n",
    "            ff = self.saved(q, 'fac')\n",
    "            c = c * (f*ff.t())\n",
    "        return c.sum().view(1, 1)\n",
    "\n",
    "    def leftgrad(self, p, q):\n",
    "        d = self.saved(p, 'value')\n",
    "        grad = self.saved(p, 'grad')\n",
    "        i = self.saved(p, 'i')\n",
    "        j = self.saved(p, 'j')\n",
    "        dd = self.saved(q, 'value')\n",
    "        c = self.kern.leftgrad(d, dd).squeeze(0)\n",
    "        if self.has_factor:\n",
    "            self.recalculate(p)\n",
    "            self.recalculate(q)\n",
    "            f = self.saved(p, 'fac')\n",
    "            fg = self.saved(p, 'facgrad')\n",
    "            ff = self.saved(q, 'fac')\n",
    "            c = c*(f*ff.t()) + (fg*ff.t())*self.kern(d, dd)\n",
    "        c = c[:, None] * grad[..., None]\n",
    "        c = c.sum(dim=-1).view(-1, 3)\n",
    "        g = zeros(p.natoms, 3).index_add(0, i, -c).index_add(0, j, c)\n",
    "        return g.view(-1, 1)\n",
    "\n",
    "    def rightgrad(self, p, q):\n",
    "        d = self.saved(p, 'value')\n",
    "        dd = self.saved(q, 'value')\n",
    "        grad = self.saved(q, 'grad')\n",
    "        i = self.saved(q, 'i')\n",
    "        j = self.saved(q, 'j')\n",
    "        c = self.kern.rightgrad(d, dd).squeeze(0)\n",
    "        if self.has_factor:\n",
    "            self.recalculate(p)\n",
    "            self.recalculate(q)\n",
    "            f = self.saved(p, 'fac')\n",
    "            ff = self.saved(q, 'fac')\n",
    "            ffg = self.saved(q, 'facgrad')\n",
    "            c = c*(f*ff.t()) + (f*ffg.t())*self.kern(d, dd)\n",
    "        c = c[..., None] * grad\n",
    "        c = c.sum(dim=0).view(-1, 3)\n",
    "        g = zeros(q.natoms, 3).index_add(0, i, -c).index_add(0, j, c)\n",
    "        return g.view(1, -1)\n",
    "\n",
    "    def gradgrad(self, p, q):\n",
    "        d1 = self.saved(p, 'value')\n",
    "        g1 = self.saved(p, 'grad')\n",
    "        i1 = self.saved(p, 'i')\n",
    "        j1 = self.saved(p, 'j')\n",
    "        d2 = self.saved(q, 'value')\n",
    "        g2 = self.saved(q, 'grad')\n",
    "        i2 = self.saved(q, 'i')\n",
    "        j2 = self.saved(q, 'j')\n",
    "        c = self.kern.gradgrad(d1, d2)\n",
    "        if self.has_factor:\n",
    "            self.recalculate(p)\n",
    "            self.recalculate(q)\n",
    "            f1 = self.saved(p, 'fac')\n",
    "            h1 = self.saved(p, 'facgrad')\n",
    "            f2 = self.saved(q, 'fac')\n",
    "            h2 = self.saved(q, 'facgrad')\n",
    "            c = (c*(f1*f2.t()) + h1*h2.t()*self.kern(d1, d2) +\n",
    "                 (h1*f2.t()*self.kern.rightgrad(d1, d2)) +\n",
    "                 (f1*h2.t()*self.kern.leftgrad(d1, d2)))\n",
    "        c = c.squeeze()[:, None, :, None] * g1[..., None, None] * g2\n",
    "        cc = torch.zeros(p.natoms, 3, j2.size(0), 3).index_add(\n",
    "            0, j1, c).index_add(0, i1, -c)\n",
    "        ccc = torch.zeros(p.natoms, 3, q.natoms, 3).index_add(\n",
    "            2, j2, cc).index_add(2, i2, -cc)\n",
    "        return ccc.view(p.natoms*3, q.natoms*3)\n",
    "\n",
    "    def gradgraddiag(self, p):\n",
    "        forces = []\n",
    "        for i, loc in enumerate(iterable(p.loc)):\n",
    "            d = self.saved(loc, 'diag_value')\n",
    "            grad = self.saved(loc, 'diag_grad')\n",
    "            c = self.kern.gradgrad(d, d).squeeze()\n",
    "            if self.has_factor:\n",
    "                self.recalculate(loc)\n",
    "                f = self.saved(loc, 'diag_fac')\n",
    "                fg = self.saved(loc, 'diag_facgrad')\n",
    "                c = (c*(f*f.t()) + fg*fg.t()*self.kern(d, d) +\n",
    "                     (fg*f.t()*self.kern.rightgrad(d, d)) +\n",
    "                     (f*fg.t()*self.kern.leftgrad(d, d))).squeeze()\n",
    "            c = c[..., None] * grad[None, ] * grad[:, None]\n",
    "            c = c.sum(dim=(0, 1))\n",
    "            forces += [c]\n",
    "        forces = cat(forces)\n",
    "        return forces.view(-1)\n",
    "\n",
    "\n",
    "class DistanceKernel(PairSimilarityKernel):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "\n",
    "    def descriptor(self, r):\n",
    "        d = (r**2).sum(dim=-1).sqrt().view(-1, 1)\n",
    "        grad = r/d\n",
    "        return d, grad\n",
    "\n",
    "\n",
    "class LogDistanceKernel(PairSimilarityKernel):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "\n",
    "    def descriptor(self, r):\n",
    "        d = (r**2).sum(dim=-1).sqrt().view(-1, 1)\n",
    "        grad = r/d**2\n",
    "        return d.log(), grad\n",
    "\n",
    "\n",
    "class RepulsiveCoreKernel(DistanceKernel):\n",
    "    def __init__(self, *args, eta=1):\n",
    "        super().__init__(*args)\n",
    "        self.eta = eta\n",
    "\n",
    "    def factor(self, d):\n",
    "        return 1./d**self.eta, -self.eta/d**(self.eta+1)\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return super().state_args + ', eta={}'.format(self.eta)\n",
    "\n",
    "\n",
    "class PairKernel(DistanceKernel):\n",
    "    def __init__(self, *args, factor=None):\n",
    "        super().__init__(*args)\n",
    "        if factor is not None:\n",
    "            self.factor = factor\n",
    "            if hasattr(factor, 'params'):\n",
    "                self.params += factor.params\n",
    "\n",
    "    @property\n",
    "    def state_args(self):\n",
    "        return super().state_args + ', factor={}'.format(\n",
    "            self.factor.state if hasattr(self, 'factor')\n",
    "            and hasattr(self.factor, 'state') else None)\n",
    "\n",
    "\n",
    "def test():\n",
    "    from theforce.descriptor.atoms import namethem\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "    from theforce.regression.kernel import Positive, DotProd, Normed\n",
    "    from theforce.regression.stationary import RBF\n",
    "    from theforce.descriptor.atoms import TorchAtoms, AtomsData\n",
    "    import numpy as np\n",
    "    torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "    # create kernel\n",
    "    kern = PairKernel(RBF(), 18, 10, factor=PolyCut(3.0))\n",
    "    kerns = [kern]\n",
    "    namethem(kerns)\n",
    "\n",
    "    cell = np.ones(3)*10\n",
    "    positions = np.array([(-1., 0., 0.), (1., 0., 0.),\n",
    "                          (0., -1., 0.), (0., 1., 0.),\n",
    "                          (0., 0., -1.1), (0., 0., 1.1),\n",
    "                          (0., 0., 0.)]) + cell/2\n",
    "\n",
    "    b = TorchAtoms(positions=positions, numbers=3*[10]+3*[18]+[10], cell=cell,\n",
    "                   pbc=True, cutoff=3.0, descriptors=kerns)\n",
    "\n",
    "    # make natoms different in a, b. P.S. add an isolated atom.\n",
    "    _pos = np.concatenate([positions, [[0., 0., 0.], [3., 5., 5.]]])\n",
    "    a = TorchAtoms(positions=_pos, numbers=2*[10, 18, 10]+[18, 10, 18], cell=cell,\n",
    "                   pbc=True, cutoff=3.0, descriptors=kerns)\n",
    "\n",
    "    a.update(posgrad=True, forced=True)\n",
    "    b.update(posgrad=True, forced=True)\n",
    "\n",
    "    # left/right-grad\n",
    "    kern([a], [b]).backward()\n",
    "    test_left = a.xyz.grad.allclose(kern.leftgrad(a, b).view(-1, 3))\n",
    "    max_left = (a.xyz.grad - kern.leftgrad(a, b).view(-1, 3)).max()\n",
    "    print(\"leftgrad: {}  \\t max diff: {}\".format(test_left, max_left))\n",
    "    test_right = b.xyz.grad.allclose(kern.rightgrad(a, b).view(-1, 3))\n",
    "    max_right = (b.xyz.grad - kern.rightgrad(a, b).view(-1, 3)).max()\n",
    "    print(\"rightgrad: {} \\t max diff: {}\".format(test_right, max_right))\n",
    "\n",
    "    # gradgrad-left\n",
    "    a.update(posgrad=True, forced=True)\n",
    "    b.update(posgrad=True, forced=True)\n",
    "    (kern.leftgrad(a, b).view(-1, 3)*a.xyz).sum().backward()\n",
    "    v1 = a.xyz.grad.data\n",
    "    a.update(posgrad=True, forced=True)\n",
    "    b.update(posgrad=True, forced=True)\n",
    "    (kern.gradgrad(a, b)*a.xyz.view(-1)[:, None]).sum().backward()\n",
    "    v2 = a.xyz.grad.data\n",
    "    print('gradgrad-left: {}'.format(v1.allclose(v2)))\n",
    "\n",
    "    # gradgrad-right\n",
    "    a.update(posgrad=True, forced=True)\n",
    "    b.update(posgrad=True, forced=True)\n",
    "    (kern.rightgrad(a, b).view(-1, 3)*b.xyz).sum().backward()\n",
    "    v1 = b.xyz.grad.data\n",
    "    a.update(posgrad=True, forced=True)\n",
    "    b.update(posgrad=True, forced=True)\n",
    "    (kern.gradgrad(a, b)*b.xyz.view(-1)[None]).sum().backward()\n",
    "    v2 = b.xyz.grad.data\n",
    "    print('gradgrad-right: {}'.format(v1.allclose(v2)))\n",
    "\n",
    "    # gradgraddiag\n",
    "    test_diag = kern.gradgrad(a, a).diag().allclose(kern.gradgraddiag(a))\n",
    "    print('gradgraddiag: {}'.format(test_diag))\n",
    "\n",
    "\n",
    "def example():\n",
    "    from torch import tensor\n",
    "    from theforce.regression.stationary import RBF\n",
    "    from theforce.math.cutoff import PolyCut\n",
    "    from theforce.math.radial import Product, ParamedRepulsiveCore\n",
    "\n",
    "    factor = Product(PolyCut(1.0), ParamedRepulsiveCore())\n",
    "    kern = PairKernel(RBF(), 1, 1, factor=factor)\n",
    "    print(kern.state == eval(kern.state).state)\n",
    "\n",
    "    d = torch.arange(0.1, 1.5, 0.1).view(-1)\n",
    "    d.requires_grad = True\n",
    "    f = eval(factor.state)\n",
    "    a, b = f(d)\n",
    "    a.sum().backward()\n",
    "    print(d.grad.allclose(b))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()\n",
    "    example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
